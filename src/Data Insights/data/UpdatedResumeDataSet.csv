Category,Resume
Data Science,"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details 

Data Science Assurance Associate 

Data Science Assurance Associate - Ernst & Young LLP
Skill Details 
JAVASCRIPT- Exprience - 24 months
jQuery- Exprience - 24 months
Python- Exprience - 24 monthsCompany Details 
company - Ernst & Young LLP
description - Fraud Investigations and Dispute Services   Assurance
TECHNOLOGY ASSISTED REVIEW
TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.
* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.
* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.
* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify ""red flags"" and fraud-related issues.

Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.

MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)
TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.
* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.
* Created customized tableau dashboards for effective reporting and visualizations.
CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.
* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.
* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.

Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer

INFORMATION GOVERNANCE
Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.
* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.
* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.
* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.
Tools & Technologies: Python, Flask, Elastic Search, Kibana

FRAUD ANALYTIC PLATFORM
Fraud Analytics and investigative platform to review all red flag cases.
â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.
* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics
Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js"
Data Science,"Education Details 
May 2013 to May 2017 B.E   UIT-RGPV
Data Scientist 

Data Scientist - Matelabs
Skill Details 
Python- Exprience - Less than 1 year months
Statsmodels- Exprience - 12 months
AWS- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Sklearn- Exprience - Less than 1 year months
Scipy- Exprience - Less than 1 year months
Keras- Exprience - Less than 1 year monthsCompany Details 
company - Matelabs
description - ML Platform for business professionals, dummies and enthusiasts.
60/A Koramangala 5th block,
Achievements/Tasks behind sukh sagar, Bengaluru,
India                               Developed and deployed auto preprocessing steps of machine learning mainly missing value
treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.
Deployed automated classification and regression model.
linkedin.com/in/aditya-rathore-
b4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and
Prophet.
Worked on meta-feature extracting problem.
github.com/rathorology
Implemented a state of the art research paper on outlier detection for mixed attributes.
company - Matelabs
description - "
Data Science,"Areas of Interest Deep Learning, Control System Design, Programming in-Python, Electric Machinery, Web Development, Analytics Technical Activities q Hindustan Aeronautics Limited, Bangalore - For 4 weeks under the guidance of Mr. Satish, Senior Engineer in the hangar of Mirage 2000 fighter aircraft Technical Skills Programming Matlab, Python and Java, LabView, Python WebFrameWork-Django, Flask, LTSPICE-intermediate Languages and and MIPOWER-intermediate, Github (GitBash), Jupyter Notebook, Xampp, MySQL-Basics, Python Software Packages Interpreters-Anaconda, Python2, Python3, Pycharm, Java IDE-Eclipse Operating Systems Windows, Ubuntu, Debian-Kali Linux Education Details 
January 2019 B.Tech. Electrical and Electronics Engineering  Manipal Institute of Technology
January 2015    DEEKSHA CENTER
January 2013    Little Flower Public School
August 2000    Manipal Academy of Higher
DATA SCIENCE 

DATA SCIENCE AND ELECTRICAL ENTHUSIAST
Skill Details 
Data Analysis- Exprience - Less than 1 year months
excel- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
mathematics- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year months
Electrical Engineering- Exprience - Less than 1 year months
Sql- Exprience - Less than 1 year monthsCompany Details 
company - THEMATHCOMPANY
description - I am currently working with a Casino based operator(name not to be disclosed) in Macau.I need to segment the customers who visit their property based on the value the patrons bring into the company.Basically prove that the segmentation can be done in much better way than the current system which they have with proper numbers to back it up.Henceforth they can implement target marketing strategy to attract their customers who add value to the business."
Data Science,"Skills â¢ R â¢ Python â¢ SAP HANA â¢ Tableau â¢ SAP HANA SQL â¢ SAP HANA PAL â¢ MS SQL â¢ SAP Lumira â¢ C# â¢ Linear Programming â¢ Data Modelling â¢ Advance Analytics â¢ SCM Analytics â¢ Retail Analytics â¢Social Media Analytics â¢ NLP Education Details 
January 2017 to January 2018 PGDM Business Analytics  Great Lakes Institute of Management & Illinois Institute of Technology
January 2013 Bachelor of Engineering Electronics and Communication Bengaluru, Karnataka New Horizon College of Engineering, Bangalore Visvesvaraya Technological University
Data Science Consultant 

Consultant - Deloitte USI
Skill Details 
LINEAR PROGRAMMING- Exprience - 6 months
RETAIL- Exprience - 6 months
RETAIL MARKETING- Exprience - 6 months
SCM- Exprience - 6 months
SQL- Exprience - Less than 1 year months
Deep Learning- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
R- Exprience - Less than 1 year monthsCompany Details 
company - Deloitte USI
description - The project involved analysing historic deals and coming with insights to optimize future deals.
Role: Was given raw data, carried out end to end analysis and presented insights to client.
Key Responsibilities:
â¢ Extract data from client systems across geographies.
â¢ Understand and build reports in tableau. Infer meaningful insights to optimize prices and find out process blockades.
Technical Environment: R, Tableau.

Industry: Cross Industry
Service Area: Cross Industry - Products
Project Name: Handwriting recognition
Consultant: 3 months.
The project involved taking handwritten images and converting them to digital text images by object detection and sentence creation.
Role: I was developing sentence correction functionality.
Key Responsibilities:
â¢ Gather data large enough to capture all English words
â¢ Train LSTM models on words.
Technical Environment: Python.

Industry: Finance
Service Area: Financial Services - BI development Project Name: SWIFT
Consultant: 8 months.
The project was to develop an analytics infrastructure on top of SAP S/4, it would user to view
financial reports to respective departments. Reporting also included forecasting expenses.
Role: I was leading the offshore team.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop ETL for data flow
â¢ Validate various reports.
Technical Environment: SAP HANA, Tableau, SAP AO.

Industry: Healthcare Analytics
Service Area: Life Sciences - Product development Project Name: Clinical Healthcare System
Consultant: 2 months.
The project was to develop an analytics infrastructure on top of Argus, it would allow users to query faster and provide advance analytics capabilities.
Role: I was involved from design to deploy phase, performed a lot of data restructuring and built
models for insights.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop and deploy analytical models.
â¢ Validate various reports.
Technical Environment: Data Modelling, SAP HANA, Tableau, NLP.

Industry: FMCG
Service Area: Trade & Promotion
Project Name: Consumption Based Planning for Flowers Foods Consultant; 8 months.
The project involved setting up of CRM and CBP modules.
Role: I was involved in key data decomposition activities and setting up the base for future year
forecast. Over the course of the project I developed various models and carried out key
performance improvements.
Key Responsibilities:
â¢ Design & Develop HANA models for decomposition.
â¢ Develop data flow for forecast.
â¢ Developed various views for reporting of Customer/Sales/Funds.
â¢ Validate various reports in BOBJ.
Technical Environment: Data Modelling, SAP HANA, BOBJ, Time Series Forecasting.

Internal Initiative Industry: FMCG
Customer Segmentation and RFM analysis Consultant; 3 months.
The initiative involved setting up of HANA-Python interface and advance analytics on Python. Over the course I had successfully segmented data into five core segments using K-means and carried out RFM analysis in Python. Also developed algorithm to categorize any new customer under the defined buckets.
Technical Environment: Anaconda3, Python3.6, HANA SPS12

Industry: Telecom Invoice state detection Consultant; 1 months.
The initiative was to reduce the manual effort in verifying closed and open invoices manually, it
involved development to a decision tree to classify open/closed invoices. This enabled effort
reduction by 60%.
Technical Environment: R, SAP PAL, SAP HANA SPS12

Accenture Experience
Industry: Analytics - Cross Industry
In Process Analytics for SAP Senior Developer; 19 months.
Accenture Solutions Pvt. Ltd., India
The project involved development of SAP analytics tool - In Process Analytics (IPA) . My role was to develop database objects and data models to provide operational insights to clients.
Role: I have developed various Finance related KPIs and spearheaded various deployments.
Introduced SAP Predictive analytics to reduce development time and reuse functionalities for KPIs and prepared production planning reports.
Key Responsibilities:
â¢ Involved in information gather phase.
â¢ Designed and implemented SAP HANA data modelling using Attribute View, Analytic View, and
Calculation View.
â¢ Developed various KPI's individually using complex SQL scripts in Calculation views.
â¢ Created procedures in HANA Database.
â¢ Took ownership and developed Dashboard functionality.
â¢ Involved in building data processing algorithms to be executed in R server for cluster analysis.
Technical Environment: R, SAP HANA, T-SQL.
Industry: Cross Industry
Accenture Testing Accelerator for SAP Database Developer; 21 months.
Accenture Solutions Pvt. Ltd., India
Role: I have taken care of all development activities for the ATAS tool and have also completed
various deployments of the product.
Apart from these activities I was also actively involved in maintenance of the database servers
(Production & Quality)
Key Responsibilities:
â¢ Analyzing business requirements, understanding the scope, getting requirements clarified
interacting with business and further transform all requirements to generate attribute
mapping documents and reviewing mapping specification documentation
â¢ Create / Update database objects like tables, views, stored procedures, function, and packages
â¢ Monitored SQL Server Error Logs and Application Logs through SQL Server Agent
â¢ Prepared Data Flow Diagrams, Entity Relationship Diagrams using UML
â¢ Responsible for Designing, developing and Normalization of database tables
â¢ Experience in performance tuning using SQL profiler.
â¢ Involved in QA, UAT, knowledge transfer and support activities
Technical Environment: SQL Server 2008/2014, Visual Studio 2010, Windows Server, Performance
Monitor, SQL Server Profiler, C#, PL-SQL, T-SQL."
Data Science,"Education Details 
 MCA   YMCAUST,  Faridabad,  Haryana
Data Science internship 


Skill Details 
Data Structure- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
Data Analysis- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Core Java- Exprience - Less than 1 year months
Database Management- Exprience - Less than 1 year monthsCompany Details 
company - Itechpower
description - "
Data Science,"SKILLS C Basics, IOT, Python, MATLAB, Data Science, Machine Learning, HTML, Microsoft Word, Microsoft Excel, Microsoft Powerpoint. RECOGNITION Academic Secured First place in B.Tech.Education Details 
August 2014 to May 2018 B.Tech.  Ghatkesar, Andhra Pradesh Aurora's Scientific and Technological Institute
June 2012 to May 2014  Secondary Education Warangal, Telangana SR Junior College
Data Science 


Skill Details 
MS OFFICE- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
machine learning- Exprience - Less than 1 year months
data science- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year monthsCompany Details 
company - 
description - "
Data Science,"Skills â¢ Python â¢ Tableau â¢ Data Visualization â¢ R Studio â¢ Machine Learning â¢ Statistics IABAC Certified Data Scientist with versatile experience over 1+ years in managing business, data science consulting and leading innovation projects, bringing business ideas to working real world solutions. Being a strong advocator of augmented era, where human capabilities are enhanced by machines, Fahed is passionate about bringing business concepts in area of machine learning, AI, robotics etc., to real life solutions.Education Details 
January 2017 B. Tech Computer Science & Engineering Mohali, Punjab Indo Global College of Engineering
Data Science Consultant 

Data Science Consultant - Datamites
Skill Details 
MACHINE LEARNING- Exprience - 13 months
PYTHON- Exprience - 24 months
SOLUTIONS- Exprience - 24 months
DATA SCIENCE- Exprience - 24 months
DATA VISUALIZATION- Exprience - 24 months
Tableau- Exprience - 24 monthsCompany Details 
company - Datamites
description - â¢ Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.
â¢ Responsible for loading, extracting and validation of client data.
â¢ Worked on manipulating, cleaning & processing data using python.
â¢ Used Tableau for data visualization.
company - Heretic Solutions Pvt Ltd
description - â¢ Worked closely with business to identify issues and used data to propose solutions for effective decision making.
â¢ Manipulating, cleansing & processing data using Python, Excel and R.
â¢ Analyzed raw data, drawing conclusions & developing recommendations.
â¢ Used machine learning tools and statistical techniques to produce solutions to problems."
Data Science,"Education Details 
 B.Tech   Rayat and Bahra Institute of Engineering and Biotechnology
Data Science 

Data Science
Skill Details 
Numpy- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
Tensorflow- Exprience - Less than 1 year months
Scikit- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
GCP- Exprience - Less than 1 year months
Pandas- Exprience - Less than 1 year months
Neural Network- Exprience - Less than 1 year monthsCompany Details 
company - Wipro
description - Bhawana Aggarwal
E-Mail:bhawana.chd@gmail.com
Phone: 09876971076
VVersatile, high-energy professional targeting challenging assignments in Machine
PROFILE SUMMARY
âª An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability.
TECHNICAL SKILLS
Programming Languages Python, C
Libraries Seaborn, Numpy, Pandas, Cufflinks, Matplotlib
Algorithms
KNN, Decision Tree, Linear regression, Logistic Regression, Neural Networks, K means clustering,
Tensorflow, SVM
Databases SQL, Oracle
Operating Systems Linux, Window
Development Environments NetBeans, Notebooks, Sublime
Ticketing tools Service Now, Remedy
Education
UG Education:
B.Tech (Computer Science) from Rayat and Bahra Institute of Engineering and Biotechnology passed with 78.4%in
2016.
Schooling:
XII in 2012 from Moti Ram Arya Sr. Secondary School(Passed with 78.4%)
X in 2010 from Valley Public School (Passed with 9.4 CGPA)
WORK EXPERINCE
Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
OTHER PROJECTS
Title : Diabetes Detection
Brief : Developed the software which can detect whether the person is suffering from Diabetes or not and got the third
prize in it.
TRAINING AND CERTIFICATIONS
Title: Python Training, Machine Learning, Data Science, Deep Learning
Organization: Udemy, Coursera (Machine Learning, Deep Learning)
Personal Profile
Fatherâs Name :Mr. Tirlok Aggarwal
Language Known : English & Hindi
Marital Status :Single
Date of Birth(Gender):1993-12-20(YYYY-MM-DD) (F)
company - Wipro
description - Developing programs in Python.
company - Wipro
description - Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
company - Wipro Technologies
description - An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability."
Data Science,"Personal Skills â¢ Ability to quickly grasp technical aspects and willingness to learn â¢ High energy levels & Result oriented. Education Details 
January 2018 Master of Engineering Computer Technology & Application Bhopal, Madhya Pradesh Truba Institute of Engineering & Information Technology
January 2010 B.E. computer science Bhopal, Madhya Pradesh RKDF Institute of Science and Technology College of Engineering
January 2006 Polytechnic Information Technology Vidisha, Madhya Pradesh SATI Engineering College in Vidisha
January 2003 M.tech Thesis Detail  BMCH School in Ganj basoda
Data science 

I have six month experience in Data Science. Key Skills: - Experience in Machine Learning, Deep Leaning, NLP, Python, SQL, Web Scraping Good knowledge in computer subjects and ability to update
Skill Details 
Experience in Machine Learning, Deep Learning, NLP, Python, SQL, Web Crawling, HTML,CSS.- Exprience - Less than 1 year monthsCompany Details 
company - RNT.AI Technology Solution
description - Text classification using Machine learning Algorithms with python.
Practical knowledge of Deep learning algorithms such as Â Recurrent Neural Networks(RNN).
Develop custom data models and algorithms to apply to dataset
Experience with Python packages like Pandas, Scikit-learn, Tensor Flow, Numpy, Matplotliv, NLTK.
Comfort with SQL, Â MYSQL
Sentiment analysis.
Â Apply leave Dataset using classification technique like Tf--idf , LSA with cosine similarity using Machine learning Algorithms.
Web crawling using Selenium web driver and Beautiful Soup with python.
company - Life Insurance Corporation of India Bhopal
description - Ã¼Â Explaining policy features and the benefits
Ã¼ Updated knowledge of life insurance products and shared with customers"
Data Science,"Expertise â Data and Quantitative Analysis â Decision Analytics â Predictive Modeling â Data-Driven Personalization â KPI Dashboards â Big Data Queries and Interpretation â Data Mining and Visualization Tools â Machine Learning Algorithms â Business Intelligence (BI) â Research, Reports and Forecasts Education Details 
 PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business
 B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy
Data Scientist 

Data Scientist with PR Canada
Skill Details 
Algorithms- Exprience - 6 months
BI- Exprience - 6 months
Business Intelligence- Exprience - 6 months
Machine Learning- Exprience - 24 months
Visualization- Exprience - 24 months
spark- Exprience - 24 months
python- Exprience - 36 months
tableau- Exprience - 36 months
Data Analysis- Exprience - 24 monthsCompany Details 
company - Aegis school of Data Science & Business
description - Mostly working on industry project for providing solution along with Teaching Appointments: Teach undergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis School of Data Science, Mumbai (2017 to Present)
company - Aegis school of Data & Business
description - Data Science Intern, Nov 2015 to Jan 2016

Furnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategic planning across all business units, distribution channels and product lines.

â Chat Bot using AWS LEX and Tensor flow  Python
The goal of project creates a chat bot for an academic institution or university to handle queries related courses offered by that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this companies handle their client 24x7. In this case companies are academic institutions and clients are participants or students.
â Web scraping using Selenium web driver   Python
The task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it.
â Data Visualization and Data insights   Hadoop Eco System, Hive, PySpark, QlikSense
The goal of this project is to build a Business Solutions to a Internet Service Provider Company, like handling data which is generated per day basis, for that we have to visualize that data and find the usage pattern form it and have a generate a reports.
â Image Based Fraud Detection   Microsoft Face API, PySpark, Open CV
The main goal of project is Recognize similarity for a face to given Database images. Face recognition is the recognizing a special face from set of different faces. Face is extracted and then compared with the database Image if that Image recognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how we are going to prevent the frauds in the initial stage itself.
â Churn Analysis for Internet Service Provider   R, Python, Machine Learning, Hadoop
The objective is to identify the customer who is likely to churn in a given period of time; we have to pretend the customer giving incentive offers.
â Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.
This project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.

Quantifiable Results:
â Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.
â Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.
â Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.
â Working for some social cause with the help of Data Science for Social Goods Committee, where our team developed a product called ""Let's find a missing Child"" for helping society.
company - IBM India pvt ltd
description - Mostly worked on blumix and IBM Watson for Data science."
Data Science,"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details 

Data Science Assurance Associate 

Data Science Assurance Associate - Ernst & Young LLP
Skill Details 
JAVASCRIPT- Exprience - 24 months
jQuery- Exprience - 24 months
Python- Exprience - 24 monthsCompany Details 
company - Ernst & Young LLP
description - Fraud Investigations and Dispute Services   Assurance
TECHNOLOGY ASSISTED REVIEW
TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.
* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.
* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.
* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify ""red flags"" and fraud-related issues.

Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.

MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)
TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.
* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.
* Created customized tableau dashboards for effective reporting and visualizations.
CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.
* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.
* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.

Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer

INFORMATION GOVERNANCE
Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.
* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.
* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.
* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.
Tools & Technologies: Python, Flask, Elastic Search, Kibana

FRAUD ANALYTIC PLATFORM
Fraud Analytics and investigative platform to review all red flag cases.
â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.
* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics
Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js"
Data Science,"Education Details 
May 2013 to May 2017 B.E   UIT-RGPV
Data Scientist 

Data Scientist - Matelabs
Skill Details 
Python- Exprience - Less than 1 year months
Statsmodels- Exprience - 12 months
AWS- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Sklearn- Exprience - Less than 1 year months
Scipy- Exprience - Less than 1 year months
Keras- Exprience - Less than 1 year monthsCompany Details 
company - Matelabs
description - ML Platform for business professionals, dummies and enthusiasts.
60/A Koramangala 5th block,
Achievements/Tasks behind sukh sagar, Bengaluru,
India                               Developed and deployed auto preprocessing steps of machine learning mainly missing value
treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.
Deployed automated classification and regression model.
linkedin.com/in/aditya-rathore-
b4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and
Prophet.
Worked on meta-feature extracting problem.
github.com/rathorology
Implemented a state of the art research paper on outlier detection for mixed attributes.
company - Matelabs
description - "
Data Science,"Areas of Interest Deep Learning, Control System Design, Programming in-Python, Electric Machinery, Web Development, Analytics Technical Activities q Hindustan Aeronautics Limited, Bangalore - For 4 weeks under the guidance of Mr. Satish, Senior Engineer in the hangar of Mirage 2000 fighter aircraft Technical Skills Programming Matlab, Python and Java, LabView, Python WebFrameWork-Django, Flask, LTSPICE-intermediate Languages and and MIPOWER-intermediate, Github (GitBash), Jupyter Notebook, Xampp, MySQL-Basics, Python Software Packages Interpreters-Anaconda, Python2, Python3, Pycharm, Java IDE-Eclipse Operating Systems Windows, Ubuntu, Debian-Kali Linux Education Details 
January 2019 B.Tech. Electrical and Electronics Engineering  Manipal Institute of Technology
January 2015    DEEKSHA CENTER
January 2013    Little Flower Public School
August 2000    Manipal Academy of Higher
DATA SCIENCE 

DATA SCIENCE AND ELECTRICAL ENTHUSIAST
Skill Details 
Data Analysis- Exprience - Less than 1 year months
excel- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
mathematics- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year months
Electrical Engineering- Exprience - Less than 1 year months
Sql- Exprience - Less than 1 year monthsCompany Details 
company - THEMATHCOMPANY
description - I am currently working with a Casino based operator(name not to be disclosed) in Macau.I need to segment the customers who visit their property based on the value the patrons bring into the company.Basically prove that the segmentation can be done in much better way than the current system which they have with proper numbers to back it up.Henceforth they can implement target marketing strategy to attract their customers who add value to the business."
Data Science,"Skills â¢ R â¢ Python â¢ SAP HANA â¢ Tableau â¢ SAP HANA SQL â¢ SAP HANA PAL â¢ MS SQL â¢ SAP Lumira â¢ C# â¢ Linear Programming â¢ Data Modelling â¢ Advance Analytics â¢ SCM Analytics â¢ Retail Analytics â¢Social Media Analytics â¢ NLP Education Details 
January 2017 to January 2018 PGDM Business Analytics  Great Lakes Institute of Management & Illinois Institute of Technology
January 2013 Bachelor of Engineering Electronics and Communication Bengaluru, Karnataka New Horizon College of Engineering, Bangalore Visvesvaraya Technological University
Data Science Consultant 

Consultant - Deloitte USI
Skill Details 
LINEAR PROGRAMMING- Exprience - 6 months
RETAIL- Exprience - 6 months
RETAIL MARKETING- Exprience - 6 months
SCM- Exprience - 6 months
SQL- Exprience - Less than 1 year months
Deep Learning- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
R- Exprience - Less than 1 year monthsCompany Details 
company - Deloitte USI
description - The project involved analysing historic deals and coming with insights to optimize future deals.
Role: Was given raw data, carried out end to end analysis and presented insights to client.
Key Responsibilities:
â¢ Extract data from client systems across geographies.
â¢ Understand and build reports in tableau. Infer meaningful insights to optimize prices and find out process blockades.
Technical Environment: R, Tableau.

Industry: Cross Industry
Service Area: Cross Industry - Products
Project Name: Handwriting recognition
Consultant: 3 months.
The project involved taking handwritten images and converting them to digital text images by object detection and sentence creation.
Role: I was developing sentence correction functionality.
Key Responsibilities:
â¢ Gather data large enough to capture all English words
â¢ Train LSTM models on words.
Technical Environment: Python.

Industry: Finance
Service Area: Financial Services - BI development Project Name: SWIFT
Consultant: 8 months.
The project was to develop an analytics infrastructure on top of SAP S/4, it would user to view
financial reports to respective departments. Reporting also included forecasting expenses.
Role: I was leading the offshore team.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop ETL for data flow
â¢ Validate various reports.
Technical Environment: SAP HANA, Tableau, SAP AO.

Industry: Healthcare Analytics
Service Area: Life Sciences - Product development Project Name: Clinical Healthcare System
Consultant: 2 months.
The project was to develop an analytics infrastructure on top of Argus, it would allow users to query faster and provide advance analytics capabilities.
Role: I was involved from design to deploy phase, performed a lot of data restructuring and built
models for insights.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop and deploy analytical models.
â¢ Validate various reports.
Technical Environment: Data Modelling, SAP HANA, Tableau, NLP.

Industry: FMCG
Service Area: Trade & Promotion
Project Name: Consumption Based Planning for Flowers Foods Consultant; 8 months.
The project involved setting up of CRM and CBP modules.
Role: I was involved in key data decomposition activities and setting up the base for future year
forecast. Over the course of the project I developed various models and carried out key
performance improvements.
Key Responsibilities:
â¢ Design & Develop HANA models for decomposition.
â¢ Develop data flow for forecast.
â¢ Developed various views for reporting of Customer/Sales/Funds.
â¢ Validate various reports in BOBJ.
Technical Environment: Data Modelling, SAP HANA, BOBJ, Time Series Forecasting.

Internal Initiative Industry: FMCG
Customer Segmentation and RFM analysis Consultant; 3 months.
The initiative involved setting up of HANA-Python interface and advance analytics on Python. Over the course I had successfully segmented data into five core segments using K-means and carried out RFM analysis in Python. Also developed algorithm to categorize any new customer under the defined buckets.
Technical Environment: Anaconda3, Python3.6, HANA SPS12

Industry: Telecom Invoice state detection Consultant; 1 months.
The initiative was to reduce the manual effort in verifying closed and open invoices manually, it
involved development to a decision tree to classify open/closed invoices. This enabled effort
reduction by 60%.
Technical Environment: R, SAP PAL, SAP HANA SPS12

Accenture Experience
Industry: Analytics - Cross Industry
In Process Analytics for SAP Senior Developer; 19 months.
Accenture Solutions Pvt. Ltd., India
The project involved development of SAP analytics tool - In Process Analytics (IPA) . My role was to develop database objects and data models to provide operational insights to clients.
Role: I have developed various Finance related KPIs and spearheaded various deployments.
Introduced SAP Predictive analytics to reduce development time and reuse functionalities for KPIs and prepared production planning reports.
Key Responsibilities:
â¢ Involved in information gather phase.
â¢ Designed and implemented SAP HANA data modelling using Attribute View, Analytic View, and
Calculation View.
â¢ Developed various KPI's individually using complex SQL scripts in Calculation views.
â¢ Created procedures in HANA Database.
â¢ Took ownership and developed Dashboard functionality.
â¢ Involved in building data processing algorithms to be executed in R server for cluster analysis.
Technical Environment: R, SAP HANA, T-SQL.
Industry: Cross Industry
Accenture Testing Accelerator for SAP Database Developer; 21 months.
Accenture Solutions Pvt. Ltd., India
Role: I have taken care of all development activities for the ATAS tool and have also completed
various deployments of the product.
Apart from these activities I was also actively involved in maintenance of the database servers
(Production & Quality)
Key Responsibilities:
â¢ Analyzing business requirements, understanding the scope, getting requirements clarified
interacting with business and further transform all requirements to generate attribute
mapping documents and reviewing mapping specification documentation
â¢ Create / Update database objects like tables, views, stored procedures, function, and packages
â¢ Monitored SQL Server Error Logs and Application Logs through SQL Server Agent
â¢ Prepared Data Flow Diagrams, Entity Relationship Diagrams using UML
â¢ Responsible for Designing, developing and Normalization of database tables
â¢ Experience in performance tuning using SQL profiler.
â¢ Involved in QA, UAT, knowledge transfer and support activities
Technical Environment: SQL Server 2008/2014, Visual Studio 2010, Windows Server, Performance
Monitor, SQL Server Profiler, C#, PL-SQL, T-SQL."
Data Science,"Education Details 
 MCA   YMCAUST,  Faridabad,  Haryana
Data Science internship 


Skill Details 
Data Structure- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
Data Analysis- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Core Java- Exprience - Less than 1 year months
Database Management- Exprience - Less than 1 year monthsCompany Details 
company - Itechpower
description - "
Data Science,"SKILLS C Basics, IOT, Python, MATLAB, Data Science, Machine Learning, HTML, Microsoft Word, Microsoft Excel, Microsoft Powerpoint. RECOGNITION Academic Secured First place in B.Tech.Education Details 
August 2014 to May 2018 B.Tech.  Ghatkesar, Andhra Pradesh Aurora's Scientific and Technological Institute
June 2012 to May 2014  Secondary Education Warangal, Telangana SR Junior College
Data Science 


Skill Details 
MS OFFICE- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
machine learning- Exprience - Less than 1 year months
data science- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year monthsCompany Details 
company - 
description - "
Data Science,"Skills â¢ Python â¢ Tableau â¢ Data Visualization â¢ R Studio â¢ Machine Learning â¢ Statistics IABAC Certified Data Scientist with versatile experience over 1+ years in managing business, data science consulting and leading innovation projects, bringing business ideas to working real world solutions. Being a strong advocator of augmented era, where human capabilities are enhanced by machines, Fahed is passionate about bringing business concepts in area of machine learning, AI, robotics etc., to real life solutions.Education Details 
January 2017 B. Tech Computer Science & Engineering Mohali, Punjab Indo Global College of Engineering
Data Science Consultant 

Data Science Consultant - Datamites
Skill Details 
MACHINE LEARNING- Exprience - 13 months
PYTHON- Exprience - 24 months
SOLUTIONS- Exprience - 24 months
DATA SCIENCE- Exprience - 24 months
DATA VISUALIZATION- Exprience - 24 months
Tableau- Exprience - 24 monthsCompany Details 
company - Datamites
description - â¢ Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.
â¢ Responsible for loading, extracting and validation of client data.
â¢ Worked on manipulating, cleaning & processing data using python.
â¢ Used Tableau for data visualization.
company - Heretic Solutions Pvt Ltd
description - â¢ Worked closely with business to identify issues and used data to propose solutions for effective decision making.
â¢ Manipulating, cleansing & processing data using Python, Excel and R.
â¢ Analyzed raw data, drawing conclusions & developing recommendations.
â¢ Used machine learning tools and statistical techniques to produce solutions to problems."
Data Science,"Education Details 
 B.Tech   Rayat and Bahra Institute of Engineering and Biotechnology
Data Science 

Data Science
Skill Details 
Numpy- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
Tensorflow- Exprience - Less than 1 year months
Scikit- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
GCP- Exprience - Less than 1 year months
Pandas- Exprience - Less than 1 year months
Neural Network- Exprience - Less than 1 year monthsCompany Details 
company - Wipro
description - Bhawana Aggarwal
E-Mail:bhawana.chd@gmail.com
Phone: 09876971076
VVersatile, high-energy professional targeting challenging assignments in Machine
PROFILE SUMMARY
âª An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability.
TECHNICAL SKILLS
Programming Languages Python, C
Libraries Seaborn, Numpy, Pandas, Cufflinks, Matplotlib
Algorithms
KNN, Decision Tree, Linear regression, Logistic Regression, Neural Networks, K means clustering,
Tensorflow, SVM
Databases SQL, Oracle
Operating Systems Linux, Window
Development Environments NetBeans, Notebooks, Sublime
Ticketing tools Service Now, Remedy
Education
UG Education:
B.Tech (Computer Science) from Rayat and Bahra Institute of Engineering and Biotechnology passed with 78.4%in
2016.
Schooling:
XII in 2012 from Moti Ram Arya Sr. Secondary School(Passed with 78.4%)
X in 2010 from Valley Public School (Passed with 9.4 CGPA)
WORK EXPERINCE
Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
OTHER PROJECTS
Title : Diabetes Detection
Brief : Developed the software which can detect whether the person is suffering from Diabetes or not and got the third
prize in it.
TRAINING AND CERTIFICATIONS
Title: Python Training, Machine Learning, Data Science, Deep Learning
Organization: Udemy, Coursera (Machine Learning, Deep Learning)
Personal Profile
Fatherâs Name :Mr. Tirlok Aggarwal
Language Known : English & Hindi
Marital Status :Single
Date of Birth(Gender):1993-12-20(YYYY-MM-DD) (F)
company - Wipro
description - Developing programs in Python.
company - Wipro
description - Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
company - Wipro Technologies
description - An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability."
Data Science,"Personal Skills â¢ Ability to quickly grasp technical aspects and willingness to learn â¢ High energy levels & Result oriented. Education Details 
January 2018 Master of Engineering Computer Technology & Application Bhopal, Madhya Pradesh Truba Institute of Engineering & Information Technology
January 2010 B.E. computer science Bhopal, Madhya Pradesh RKDF Institute of Science and Technology College of Engineering
January 2006 Polytechnic Information Technology Vidisha, Madhya Pradesh SATI Engineering College in Vidisha
January 2003 M.tech Thesis Detail  BMCH School in Ganj basoda
Data science 

I have six month experience in Data Science. Key Skills: - Experience in Machine Learning, Deep Leaning, NLP, Python, SQL, Web Scraping Good knowledge in computer subjects and ability to update
Skill Details 
Experience in Machine Learning, Deep Learning, NLP, Python, SQL, Web Crawling, HTML,CSS.- Exprience - Less than 1 year monthsCompany Details 
company - RNT.AI Technology Solution
description - Text classification using Machine learning Algorithms with python.
Practical knowledge of Deep learning algorithms such as Â Recurrent Neural Networks(RNN).
Develop custom data models and algorithms to apply to dataset
Experience with Python packages like Pandas, Scikit-learn, Tensor Flow, Numpy, Matplotliv, NLTK.
Comfort with SQL, Â MYSQL
Sentiment analysis.
Â Apply leave Dataset using classification technique like Tf--idf , LSA with cosine similarity using Machine learning Algorithms.
Web crawling using Selenium web driver and Beautiful Soup with python.
company - Life Insurance Corporation of India Bhopal
description - Ã¼Â Explaining policy features and the benefits
Ã¼ Updated knowledge of life insurance products and shared with customers"
Data Science,"Expertise â Data and Quantitative Analysis â Decision Analytics â Predictive Modeling â Data-Driven Personalization â KPI Dashboards â Big Data Queries and Interpretation â Data Mining and Visualization Tools â Machine Learning Algorithms â Business Intelligence (BI) â Research, Reports and Forecasts Education Details 
 PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business
 B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy
Data Scientist 

Data Scientist with PR Canada
Skill Details 
Algorithms- Exprience - 6 months
BI- Exprience - 6 months
Business Intelligence- Exprience - 6 months
Machine Learning- Exprience - 24 months
Visualization- Exprience - 24 months
spark- Exprience - 24 months
python- Exprience - 36 months
tableau- Exprience - 36 months
Data Analysis- Exprience - 24 monthsCompany Details 
company - Aegis school of Data Science & Business
description - Mostly working on industry project for providing solution along with Teaching Appointments: Teach undergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis School of Data Science, Mumbai (2017 to Present)
company - Aegis school of Data & Business
description - Data Science Intern, Nov 2015 to Jan 2016

Furnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategic planning across all business units, distribution channels and product lines.

â Chat Bot using AWS LEX and Tensor flow  Python
The goal of project creates a chat bot for an academic institution or university to handle queries related courses offered by that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this companies handle their client 24x7. In this case companies are academic institutions and clients are participants or students.
â Web scraping using Selenium web driver   Python
The task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it.
â Data Visualization and Data insights   Hadoop Eco System, Hive, PySpark, QlikSense
The goal of this project is to build a Business Solutions to a Internet Service Provider Company, like handling data which is generated per day basis, for that we have to visualize that data and find the usage pattern form it and have a generate a reports.
â Image Based Fraud Detection   Microsoft Face API, PySpark, Open CV
The main goal of project is Recognize similarity for a face to given Database images. Face recognition is the recognizing a special face from set of different faces. Face is extracted and then compared with the database Image if that Image recognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how we are going to prevent the frauds in the initial stage itself.
â Churn Analysis for Internet Service Provider   R, Python, Machine Learning, Hadoop
The objective is to identify the customer who is likely to churn in a given period of time; we have to pretend the customer giving incentive offers.
â Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.
This project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.

Quantifiable Results:
â Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.
â Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.
â Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.
â Working for some social cause with the help of Data Science for Social Goods Committee, where our team developed a product called ""Let's find a missing Child"" for helping society.
company - IBM India pvt ltd
description - Mostly worked on blumix and IBM Watson for Data science."
Data Science,"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details 

Data Science Assurance Associate 

Data Science Assurance Associate - Ernst & Young LLP
Skill Details 
JAVASCRIPT- Exprience - 24 months
jQuery- Exprience - 24 months
Python- Exprience - 24 monthsCompany Details 
company - Ernst & Young LLP
description - Fraud Investigations and Dispute Services   Assurance
TECHNOLOGY ASSISTED REVIEW
TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.
* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.
* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.
* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify ""red flags"" and fraud-related issues.

Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.

MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)
TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.
* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.
* Created customized tableau dashboards for effective reporting and visualizations.
CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.
* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.
* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.

Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer

INFORMATION GOVERNANCE
Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.
* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.
* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.
* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.
Tools & Technologies: Python, Flask, Elastic Search, Kibana

FRAUD ANALYTIC PLATFORM
Fraud Analytics and investigative platform to review all red flag cases.
â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.
* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics
Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js"
Data Science,"Education Details 
May 2013 to May 2017 B.E   UIT-RGPV
Data Scientist 

Data Scientist - Matelabs
Skill Details 
Python- Exprience - Less than 1 year months
Statsmodels- Exprience - 12 months
AWS- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Sklearn- Exprience - Less than 1 year months
Scipy- Exprience - Less than 1 year months
Keras- Exprience - Less than 1 year monthsCompany Details 
company - Matelabs
description - ML Platform for business professionals, dummies and enthusiasts.
60/A Koramangala 5th block,
Achievements/Tasks behind sukh sagar, Bengaluru,
India                               Developed and deployed auto preprocessing steps of machine learning mainly missing value
treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.
Deployed automated classification and regression model.
linkedin.com/in/aditya-rathore-
b4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and
Prophet.
Worked on meta-feature extracting problem.
github.com/rathorology
Implemented a state of the art research paper on outlier detection for mixed attributes.
company - Matelabs
description - "
Data Science,"Areas of Interest Deep Learning, Control System Design, Programming in-Python, Electric Machinery, Web Development, Analytics Technical Activities q Hindustan Aeronautics Limited, Bangalore - For 4 weeks under the guidance of Mr. Satish, Senior Engineer in the hangar of Mirage 2000 fighter aircraft Technical Skills Programming Matlab, Python and Java, LabView, Python WebFrameWork-Django, Flask, LTSPICE-intermediate Languages and and MIPOWER-intermediate, Github (GitBash), Jupyter Notebook, Xampp, MySQL-Basics, Python Software Packages Interpreters-Anaconda, Python2, Python3, Pycharm, Java IDE-Eclipse Operating Systems Windows, Ubuntu, Debian-Kali Linux Education Details 
January 2019 B.Tech. Electrical and Electronics Engineering  Manipal Institute of Technology
January 2015    DEEKSHA CENTER
January 2013    Little Flower Public School
August 2000    Manipal Academy of Higher
DATA SCIENCE 

DATA SCIENCE AND ELECTRICAL ENTHUSIAST
Skill Details 
Data Analysis- Exprience - Less than 1 year months
excel- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
mathematics- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year months
Electrical Engineering- Exprience - Less than 1 year months
Sql- Exprience - Less than 1 year monthsCompany Details 
company - THEMATHCOMPANY
description - I am currently working with a Casino based operator(name not to be disclosed) in Macau.I need to segment the customers who visit their property based on the value the patrons bring into the company.Basically prove that the segmentation can be done in much better way than the current system which they have with proper numbers to back it up.Henceforth they can implement target marketing strategy to attract their customers who add value to the business."
Data Science,"Skills â¢ R â¢ Python â¢ SAP HANA â¢ Tableau â¢ SAP HANA SQL â¢ SAP HANA PAL â¢ MS SQL â¢ SAP Lumira â¢ C# â¢ Linear Programming â¢ Data Modelling â¢ Advance Analytics â¢ SCM Analytics â¢ Retail Analytics â¢Social Media Analytics â¢ NLP Education Details 
January 2017 to January 2018 PGDM Business Analytics  Great Lakes Institute of Management & Illinois Institute of Technology
January 2013 Bachelor of Engineering Electronics and Communication Bengaluru, Karnataka New Horizon College of Engineering, Bangalore Visvesvaraya Technological University
Data Science Consultant 

Consultant - Deloitte USI
Skill Details 
LINEAR PROGRAMMING- Exprience - 6 months
RETAIL- Exprience - 6 months
RETAIL MARKETING- Exprience - 6 months
SCM- Exprience - 6 months
SQL- Exprience - Less than 1 year months
Deep Learning- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
R- Exprience - Less than 1 year monthsCompany Details 
company - Deloitte USI
description - The project involved analysing historic deals and coming with insights to optimize future deals.
Role: Was given raw data, carried out end to end analysis and presented insights to client.
Key Responsibilities:
â¢ Extract data from client systems across geographies.
â¢ Understand and build reports in tableau. Infer meaningful insights to optimize prices and find out process blockades.
Technical Environment: R, Tableau.

Industry: Cross Industry
Service Area: Cross Industry - Products
Project Name: Handwriting recognition
Consultant: 3 months.
The project involved taking handwritten images and converting them to digital text images by object detection and sentence creation.
Role: I was developing sentence correction functionality.
Key Responsibilities:
â¢ Gather data large enough to capture all English words
â¢ Train LSTM models on words.
Technical Environment: Python.

Industry: Finance
Service Area: Financial Services - BI development Project Name: SWIFT
Consultant: 8 months.
The project was to develop an analytics infrastructure on top of SAP S/4, it would user to view
financial reports to respective departments. Reporting also included forecasting expenses.
Role: I was leading the offshore team.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop ETL for data flow
â¢ Validate various reports.
Technical Environment: SAP HANA, Tableau, SAP AO.

Industry: Healthcare Analytics
Service Area: Life Sciences - Product development Project Name: Clinical Healthcare System
Consultant: 2 months.
The project was to develop an analytics infrastructure on top of Argus, it would allow users to query faster and provide advance analytics capabilities.
Role: I was involved from design to deploy phase, performed a lot of data restructuring and built
models for insights.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop and deploy analytical models.
â¢ Validate various reports.
Technical Environment: Data Modelling, SAP HANA, Tableau, NLP.

Industry: FMCG
Service Area: Trade & Promotion
Project Name: Consumption Based Planning for Flowers Foods Consultant; 8 months.
The project involved setting up of CRM and CBP modules.
Role: I was involved in key data decomposition activities and setting up the base for future year
forecast. Over the course of the project I developed various models and carried out key
performance improvements.
Key Responsibilities:
â¢ Design & Develop HANA models for decomposition.
â¢ Develop data flow for forecast.
â¢ Developed various views for reporting of Customer/Sales/Funds.
â¢ Validate various reports in BOBJ.
Technical Environment: Data Modelling, SAP HANA, BOBJ, Time Series Forecasting.

Internal Initiative Industry: FMCG
Customer Segmentation and RFM analysis Consultant; 3 months.
The initiative involved setting up of HANA-Python interface and advance analytics on Python. Over the course I had successfully segmented data into five core segments using K-means and carried out RFM analysis in Python. Also developed algorithm to categorize any new customer under the defined buckets.
Technical Environment: Anaconda3, Python3.6, HANA SPS12

Industry: Telecom Invoice state detection Consultant; 1 months.
The initiative was to reduce the manual effort in verifying closed and open invoices manually, it
involved development to a decision tree to classify open/closed invoices. This enabled effort
reduction by 60%.
Technical Environment: R, SAP PAL, SAP HANA SPS12

Accenture Experience
Industry: Analytics - Cross Industry
In Process Analytics for SAP Senior Developer; 19 months.
Accenture Solutions Pvt. Ltd., India
The project involved development of SAP analytics tool - In Process Analytics (IPA) . My role was to develop database objects and data models to provide operational insights to clients.
Role: I have developed various Finance related KPIs and spearheaded various deployments.
Introduced SAP Predictive analytics to reduce development time and reuse functionalities for KPIs and prepared production planning reports.
Key Responsibilities:
â¢ Involved in information gather phase.
â¢ Designed and implemented SAP HANA data modelling using Attribute View, Analytic View, and
Calculation View.
â¢ Developed various KPI's individually using complex SQL scripts in Calculation views.
â¢ Created procedures in HANA Database.
â¢ Took ownership and developed Dashboard functionality.
â¢ Involved in building data processing algorithms to be executed in R server for cluster analysis.
Technical Environment: R, SAP HANA, T-SQL.
Industry: Cross Industry
Accenture Testing Accelerator for SAP Database Developer; 21 months.
Accenture Solutions Pvt. Ltd., India
Role: I have taken care of all development activities for the ATAS tool and have also completed
various deployments of the product.
Apart from these activities I was also actively involved in maintenance of the database servers
(Production & Quality)
Key Responsibilities:
â¢ Analyzing business requirements, understanding the scope, getting requirements clarified
interacting with business and further transform all requirements to generate attribute
mapping documents and reviewing mapping specification documentation
â¢ Create / Update database objects like tables, views, stored procedures, function, and packages
â¢ Monitored SQL Server Error Logs and Application Logs through SQL Server Agent
â¢ Prepared Data Flow Diagrams, Entity Relationship Diagrams using UML
â¢ Responsible for Designing, developing and Normalization of database tables
â¢ Experience in performance tuning using SQL profiler.
â¢ Involved in QA, UAT, knowledge transfer and support activities
Technical Environment: SQL Server 2008/2014, Visual Studio 2010, Windows Server, Performance
Monitor, SQL Server Profiler, C#, PL-SQL, T-SQL."
Data Science,"Education Details 
 MCA   YMCAUST,  Faridabad,  Haryana
Data Science internship 


Skill Details 
Data Structure- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
Data Analysis- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Core Java- Exprience - Less than 1 year months
Database Management- Exprience - Less than 1 year monthsCompany Details 
company - Itechpower
description - "
Data Science,"SKILLS C Basics, IOT, Python, MATLAB, Data Science, Machine Learning, HTML, Microsoft Word, Microsoft Excel, Microsoft Powerpoint. RECOGNITION Academic Secured First place in B.Tech.Education Details 
August 2014 to May 2018 B.Tech.  Ghatkesar, Andhra Pradesh Aurora's Scientific and Technological Institute
June 2012 to May 2014  Secondary Education Warangal, Telangana SR Junior College
Data Science 


Skill Details 
MS OFFICE- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
machine learning- Exprience - Less than 1 year months
data science- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year monthsCompany Details 
company - 
description - "
Data Science,"Skills â¢ Python â¢ Tableau â¢ Data Visualization â¢ R Studio â¢ Machine Learning â¢ Statistics IABAC Certified Data Scientist with versatile experience over 1+ years in managing business, data science consulting and leading innovation projects, bringing business ideas to working real world solutions. Being a strong advocator of augmented era, where human capabilities are enhanced by machines, Fahed is passionate about bringing business concepts in area of machine learning, AI, robotics etc., to real life solutions.Education Details 
January 2017 B. Tech Computer Science & Engineering Mohali, Punjab Indo Global College of Engineering
Data Science Consultant 

Data Science Consultant - Datamites
Skill Details 
MACHINE LEARNING- Exprience - 13 months
PYTHON- Exprience - 24 months
SOLUTIONS- Exprience - 24 months
DATA SCIENCE- Exprience - 24 months
DATA VISUALIZATION- Exprience - 24 months
Tableau- Exprience - 24 monthsCompany Details 
company - Datamites
description - â¢ Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.
â¢ Responsible for loading, extracting and validation of client data.
â¢ Worked on manipulating, cleaning & processing data using python.
â¢ Used Tableau for data visualization.
company - Heretic Solutions Pvt Ltd
description - â¢ Worked closely with business to identify issues and used data to propose solutions for effective decision making.
â¢ Manipulating, cleansing & processing data using Python, Excel and R.
â¢ Analyzed raw data, drawing conclusions & developing recommendations.
â¢ Used machine learning tools and statistical techniques to produce solutions to problems."
Data Science,"Education Details 
 B.Tech   Rayat and Bahra Institute of Engineering and Biotechnology
Data Science 

Data Science
Skill Details 
Numpy- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
Tensorflow- Exprience - Less than 1 year months
Scikit- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
GCP- Exprience - Less than 1 year months
Pandas- Exprience - Less than 1 year months
Neural Network- Exprience - Less than 1 year monthsCompany Details 
company - Wipro
description - Bhawana Aggarwal
E-Mail:bhawana.chd@gmail.com
Phone: 09876971076
VVersatile, high-energy professional targeting challenging assignments in Machine
PROFILE SUMMARY
âª An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability.
TECHNICAL SKILLS
Programming Languages Python, C
Libraries Seaborn, Numpy, Pandas, Cufflinks, Matplotlib
Algorithms
KNN, Decision Tree, Linear regression, Logistic Regression, Neural Networks, K means clustering,
Tensorflow, SVM
Databases SQL, Oracle
Operating Systems Linux, Window
Development Environments NetBeans, Notebooks, Sublime
Ticketing tools Service Now, Remedy
Education
UG Education:
B.Tech (Computer Science) from Rayat and Bahra Institute of Engineering and Biotechnology passed with 78.4%in
2016.
Schooling:
XII in 2012 from Moti Ram Arya Sr. Secondary School(Passed with 78.4%)
X in 2010 from Valley Public School (Passed with 9.4 CGPA)
WORK EXPERINCE
Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
OTHER PROJECTS
Title : Diabetes Detection
Brief : Developed the software which can detect whether the person is suffering from Diabetes or not and got the third
prize in it.
TRAINING AND CERTIFICATIONS
Title: Python Training, Machine Learning, Data Science, Deep Learning
Organization: Udemy, Coursera (Machine Learning, Deep Learning)
Personal Profile
Fatherâs Name :Mr. Tirlok Aggarwal
Language Known : English & Hindi
Marital Status :Single
Date of Birth(Gender):1993-12-20(YYYY-MM-DD) (F)
company - Wipro
description - Developing programs in Python.
company - Wipro
description - Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
company - Wipro Technologies
description - An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability."
Data Science,"Personal Skills â¢ Ability to quickly grasp technical aspects and willingness to learn â¢ High energy levels & Result oriented. Education Details 
January 2018 Master of Engineering Computer Technology & Application Bhopal, Madhya Pradesh Truba Institute of Engineering & Information Technology
January 2010 B.E. computer science Bhopal, Madhya Pradesh RKDF Institute of Science and Technology College of Engineering
January 2006 Polytechnic Information Technology Vidisha, Madhya Pradesh SATI Engineering College in Vidisha
January 2003 M.tech Thesis Detail  BMCH School in Ganj basoda
Data science 

I have six month experience in Data Science. Key Skills: - Experience in Machine Learning, Deep Leaning, NLP, Python, SQL, Web Scraping Good knowledge in computer subjects and ability to update
Skill Details 
Experience in Machine Learning, Deep Learning, NLP, Python, SQL, Web Crawling, HTML,CSS.- Exprience - Less than 1 year monthsCompany Details 
company - RNT.AI Technology Solution
description - Text classification using Machine learning Algorithms with python.
Practical knowledge of Deep learning algorithms such as Â Recurrent Neural Networks(RNN).
Develop custom data models and algorithms to apply to dataset
Experience with Python packages like Pandas, Scikit-learn, Tensor Flow, Numpy, Matplotliv, NLTK.
Comfort with SQL, Â MYSQL
Sentiment analysis.
Â Apply leave Dataset using classification technique like Tf--idf , LSA with cosine similarity using Machine learning Algorithms.
Web crawling using Selenium web driver and Beautiful Soup with python.
company - Life Insurance Corporation of India Bhopal
description - Ã¼Â Explaining policy features and the benefits
Ã¼ Updated knowledge of life insurance products and shared with customers"
Data Science,"Expertise â Data and Quantitative Analysis â Decision Analytics â Predictive Modeling â Data-Driven Personalization â KPI Dashboards â Big Data Queries and Interpretation â Data Mining and Visualization Tools â Machine Learning Algorithms â Business Intelligence (BI) â Research, Reports and Forecasts Education Details 
 PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business
 B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy
Data Scientist 

Data Scientist with PR Canada
Skill Details 
Algorithms- Exprience - 6 months
BI- Exprience - 6 months
Business Intelligence- Exprience - 6 months
Machine Learning- Exprience - 24 months
Visualization- Exprience - 24 months
spark- Exprience - 24 months
python- Exprience - 36 months
tableau- Exprience - 36 months
Data Analysis- Exprience - 24 monthsCompany Details 
company - Aegis school of Data Science & Business
description - Mostly working on industry project for providing solution along with Teaching Appointments: Teach undergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis School of Data Science, Mumbai (2017 to Present)
company - Aegis school of Data & Business
description - Data Science Intern, Nov 2015 to Jan 2016

Furnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategic planning across all business units, distribution channels and product lines.

â Chat Bot using AWS LEX and Tensor flow  Python
The goal of project creates a chat bot for an academic institution or university to handle queries related courses offered by that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this companies handle their client 24x7. In this case companies are academic institutions and clients are participants or students.
â Web scraping using Selenium web driver   Python
The task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it.
â Data Visualization and Data insights   Hadoop Eco System, Hive, PySpark, QlikSense
The goal of this project is to build a Business Solutions to a Internet Service Provider Company, like handling data which is generated per day basis, for that we have to visualize that data and find the usage pattern form it and have a generate a reports.
â Image Based Fraud Detection   Microsoft Face API, PySpark, Open CV
The main goal of project is Recognize similarity for a face to given Database images. Face recognition is the recognizing a special face from set of different faces. Face is extracted and then compared with the database Image if that Image recognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how we are going to prevent the frauds in the initial stage itself.
â Churn Analysis for Internet Service Provider   R, Python, Machine Learning, Hadoop
The objective is to identify the customer who is likely to churn in a given period of time; we have to pretend the customer giving incentive offers.
â Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.
This project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.

Quantifiable Results:
â Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.
â Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.
â Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.
â Working for some social cause with the help of Data Science for Social Goods Committee, where our team developed a product called ""Let's find a missing Child"" for helping society.
company - IBM India pvt ltd
description - Mostly worked on blumix and IBM Watson for Data science."
Data Science,"Skills * Programming Languages: Python (pandas, numpy, scipy, scikit-learn, matplotlib), Sql, Java, JavaScript/JQuery. * Machine learning: Regression, SVM, NaÃ¯ve Bayes, KNN, Random Forest, Decision Trees, Boosting techniques, Cluster Analysis, Word Embedding, Sentiment Analysis, Natural Language processing, Dimensionality reduction, Topic Modelling (LDA, NMF), PCA & Neural Nets. * Database Visualizations: Mysql, SqlServer, Cassandra, Hbase, ElasticSearch D3.js, DC.js, Plotly, kibana, matplotlib, ggplot, Tableau. * Others: Regular Expression, HTML, CSS, Angular 6, Logstash, Kafka, Python Flask, Git, Docker, computer vision - Open CV and understanding of Deep learning.Education Details 

Data Science Assurance Associate 

Data Science Assurance Associate - Ernst & Young LLP
Skill Details 
JAVASCRIPT- Exprience - 24 months
jQuery- Exprience - 24 months
Python- Exprience - 24 monthsCompany Details 
company - Ernst & Young LLP
description - Fraud Investigations and Dispute Services   Assurance
TECHNOLOGY ASSISTED REVIEW
TAR (Technology Assisted Review) assists in accelerating the review process and run analytics and generate reports.
* Core member of a team helped in developing automated review platform tool from scratch for assisting E discovery domain, this tool implements predictive coding and topic modelling by automating reviews, resulting in reduced labor costs and time spent during the lawyers review.
* Understand the end to end flow of the solution, doing research and development for classification models, predictive analysis and mining of the information present in text data. Worked on analyzing the outputs and precision monitoring for the entire tool.
* TAR assists in predictive coding, topic modelling from the evidence by following EY standards. Developed the classifier models in order to identify ""red flags"" and fraud-related issues.

Tools & Technologies: Python, scikit-learn, tfidf, word2vec, doc2vec, cosine similarity, NaÃ¯ve Bayes, LDA, NMF for topic modelling, Vader and text blob for sentiment analysis. Matplot lib, Tableau dashboard for reporting.

MULTIPLE DATA SCIENCE AND ANALYTIC PROJECTS (USA CLIENTS)
TEXT ANALYTICS - MOTOR VEHICLE CUSTOMER REVIEW DATA * Received customer feedback survey data for past one year. Performed sentiment (Positive, Negative & Neutral) and time series analysis on customer comments across all 4 categories.
* Created heat map of terms by survey category based on frequency of words * Extracted Positive and Negative words across all the Survey categories and plotted Word cloud.
* Created customized tableau dashboards for effective reporting and visualizations.
CHATBOT * Developed a user friendly chatbot for one of our Products which handle simple questions about hours of operation, reservation options and so on.
* This chat bot serves entire product related questions. Giving overview of tool via QA platform and also give recommendation responses so that user question to build chain of relevant answer.
* This too has intelligence to build the pipeline of questions as per user requirement and asks the relevant /recommended questions.

Tools & Technologies: Python, Natural language processing, NLTK, spacy, topic modelling, Sentiment analysis, Word Embedding, scikit-learn, JavaScript/JQuery, SqlServer

INFORMATION GOVERNANCE
Organizations to make informed decisions about all of the information they store. The integrated Information Governance portfolio synthesizes intelligence across unstructured data sources and facilitates action to ensure organizations are best positioned to counter information risk.
* Scan data from multiple sources of formats and parse different file formats, extract Meta data information, push results for indexing elastic search and created customized, interactive dashboards using kibana.
* Preforming ROT Analysis on the data which give information of data which helps identify content that is either Redundant, Outdated, or Trivial.
* Preforming full-text search analysis on elastic search with predefined methods which can tag as (PII) personally identifiable information (social security numbers, addresses, names, etc.) which frequently targeted during cyber-attacks.
Tools & Technologies: Python, Flask, Elastic Search, Kibana

FRAUD ANALYTIC PLATFORM
Fraud Analytics and investigative platform to review all red flag cases.
â¢ FAP is a Fraud Analytics and investigative platform with inbuilt case manager and suite of Analytics for various ERP systems.
* It can be used by clients to interrogate their Accounting systems for identifying the anomalies which can be indicators of fraud by running advanced analytics
Tools & Technologies: HTML, JavaScript, SqlServer, JQuery, CSS, Bootstrap, Node.js, D3.js, DC.js"
Data Science,"Education Details 
May 2013 to May 2017 B.E   UIT-RGPV
Data Scientist 

Data Scientist - Matelabs
Skill Details 
Python- Exprience - Less than 1 year months
Statsmodels- Exprience - 12 months
AWS- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Sklearn- Exprience - Less than 1 year months
Scipy- Exprience - Less than 1 year months
Keras- Exprience - Less than 1 year monthsCompany Details 
company - Matelabs
description - ML Platform for business professionals, dummies and enthusiasts.
60/A Koramangala 5th block,
Achievements/Tasks behind sukh sagar, Bengaluru,
India                               Developed and deployed auto preprocessing steps of machine learning mainly missing value
treatment, outlier detection, encoding, scaling, feature selection and dimensionality reduction.
Deployed automated classification and regression model.
linkedin.com/in/aditya-rathore-
b4600b146                           Reasearch and deployed the time series forecasting model ARIMA, SARIMAX, Holt-winter and
Prophet.
Worked on meta-feature extracting problem.
github.com/rathorology
Implemented a state of the art research paper on outlier detection for mixed attributes.
company - Matelabs
description - "
Data Science,"Areas of Interest Deep Learning, Control System Design, Programming in-Python, Electric Machinery, Web Development, Analytics Technical Activities q Hindustan Aeronautics Limited, Bangalore - For 4 weeks under the guidance of Mr. Satish, Senior Engineer in the hangar of Mirage 2000 fighter aircraft Technical Skills Programming Matlab, Python and Java, LabView, Python WebFrameWork-Django, Flask, LTSPICE-intermediate Languages and and MIPOWER-intermediate, Github (GitBash), Jupyter Notebook, Xampp, MySQL-Basics, Python Software Packages Interpreters-Anaconda, Python2, Python3, Pycharm, Java IDE-Eclipse Operating Systems Windows, Ubuntu, Debian-Kali Linux Education Details 
January 2019 B.Tech. Electrical and Electronics Engineering  Manipal Institute of Technology
January 2015    DEEKSHA CENTER
January 2013    Little Flower Public School
August 2000    Manipal Academy of Higher
DATA SCIENCE 

DATA SCIENCE AND ELECTRICAL ENTHUSIAST
Skill Details 
Data Analysis- Exprience - Less than 1 year months
excel- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
mathematics- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year months
Electrical Engineering- Exprience - Less than 1 year months
Sql- Exprience - Less than 1 year monthsCompany Details 
company - THEMATHCOMPANY
description - I am currently working with a Casino based operator(name not to be disclosed) in Macau.I need to segment the customers who visit their property based on the value the patrons bring into the company.Basically prove that the segmentation can be done in much better way than the current system which they have with proper numbers to back it up.Henceforth they can implement target marketing strategy to attract their customers who add value to the business."
Data Science,"Skills â¢ R â¢ Python â¢ SAP HANA â¢ Tableau â¢ SAP HANA SQL â¢ SAP HANA PAL â¢ MS SQL â¢ SAP Lumira â¢ C# â¢ Linear Programming â¢ Data Modelling â¢ Advance Analytics â¢ SCM Analytics â¢ Retail Analytics â¢Social Media Analytics â¢ NLP Education Details 
January 2017 to January 2018 PGDM Business Analytics  Great Lakes Institute of Management & Illinois Institute of Technology
January 2013 Bachelor of Engineering Electronics and Communication Bengaluru, Karnataka New Horizon College of Engineering, Bangalore Visvesvaraya Technological University
Data Science Consultant 

Consultant - Deloitte USI
Skill Details 
LINEAR PROGRAMMING- Exprience - 6 months
RETAIL- Exprience - 6 months
RETAIL MARKETING- Exprience - 6 months
SCM- Exprience - 6 months
SQL- Exprience - Less than 1 year months
Deep Learning- Exprience - Less than 1 year months
Machine learning- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
R- Exprience - Less than 1 year monthsCompany Details 
company - Deloitte USI
description - The project involved analysing historic deals and coming with insights to optimize future deals.
Role: Was given raw data, carried out end to end analysis and presented insights to client.
Key Responsibilities:
â¢ Extract data from client systems across geographies.
â¢ Understand and build reports in tableau. Infer meaningful insights to optimize prices and find out process blockades.
Technical Environment: R, Tableau.

Industry: Cross Industry
Service Area: Cross Industry - Products
Project Name: Handwriting recognition
Consultant: 3 months.
The project involved taking handwritten images and converting them to digital text images by object detection and sentence creation.
Role: I was developing sentence correction functionality.
Key Responsibilities:
â¢ Gather data large enough to capture all English words
â¢ Train LSTM models on words.
Technical Environment: Python.

Industry: Finance
Service Area: Financial Services - BI development Project Name: SWIFT
Consultant: 8 months.
The project was to develop an analytics infrastructure on top of SAP S/4, it would user to view
financial reports to respective departments. Reporting also included forecasting expenses.
Role: I was leading the offshore team.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop ETL for data flow
â¢ Validate various reports.
Technical Environment: SAP HANA, Tableau, SAP AO.

Industry: Healthcare Analytics
Service Area: Life Sciences - Product development Project Name: Clinical Healthcare System
Consultant: 2 months.
The project was to develop an analytics infrastructure on top of Argus, it would allow users to query faster and provide advance analytics capabilities.
Role: I was involved from design to deploy phase, performed a lot of data restructuring and built
models for insights.
Key Responsibilities:
â¢ Design & Develop data models for reporting.
â¢ Develop and deploy analytical models.
â¢ Validate various reports.
Technical Environment: Data Modelling, SAP HANA, Tableau, NLP.

Industry: FMCG
Service Area: Trade & Promotion
Project Name: Consumption Based Planning for Flowers Foods Consultant; 8 months.
The project involved setting up of CRM and CBP modules.
Role: I was involved in key data decomposition activities and setting up the base for future year
forecast. Over the course of the project I developed various models and carried out key
performance improvements.
Key Responsibilities:
â¢ Design & Develop HANA models for decomposition.
â¢ Develop data flow for forecast.
â¢ Developed various views for reporting of Customer/Sales/Funds.
â¢ Validate various reports in BOBJ.
Technical Environment: Data Modelling, SAP HANA, BOBJ, Time Series Forecasting.

Internal Initiative Industry: FMCG
Customer Segmentation and RFM analysis Consultant; 3 months.
The initiative involved setting up of HANA-Python interface and advance analytics on Python. Over the course I had successfully segmented data into five core segments using K-means and carried out RFM analysis in Python. Also developed algorithm to categorize any new customer under the defined buckets.
Technical Environment: Anaconda3, Python3.6, HANA SPS12

Industry: Telecom Invoice state detection Consultant; 1 months.
The initiative was to reduce the manual effort in verifying closed and open invoices manually, it
involved development to a decision tree to classify open/closed invoices. This enabled effort
reduction by 60%.
Technical Environment: R, SAP PAL, SAP HANA SPS12

Accenture Experience
Industry: Analytics - Cross Industry
In Process Analytics for SAP Senior Developer; 19 months.
Accenture Solutions Pvt. Ltd., India
The project involved development of SAP analytics tool - In Process Analytics (IPA) . My role was to develop database objects and data models to provide operational insights to clients.
Role: I have developed various Finance related KPIs and spearheaded various deployments.
Introduced SAP Predictive analytics to reduce development time and reuse functionalities for KPIs and prepared production planning reports.
Key Responsibilities:
â¢ Involved in information gather phase.
â¢ Designed and implemented SAP HANA data modelling using Attribute View, Analytic View, and
Calculation View.
â¢ Developed various KPI's individually using complex SQL scripts in Calculation views.
â¢ Created procedures in HANA Database.
â¢ Took ownership and developed Dashboard functionality.
â¢ Involved in building data processing algorithms to be executed in R server for cluster analysis.
Technical Environment: R, SAP HANA, T-SQL.
Industry: Cross Industry
Accenture Testing Accelerator for SAP Database Developer; 21 months.
Accenture Solutions Pvt. Ltd., India
Role: I have taken care of all development activities for the ATAS tool and have also completed
various deployments of the product.
Apart from these activities I was also actively involved in maintenance of the database servers
(Production & Quality)
Key Responsibilities:
â¢ Analyzing business requirements, understanding the scope, getting requirements clarified
interacting with business and further transform all requirements to generate attribute
mapping documents and reviewing mapping specification documentation
â¢ Create / Update database objects like tables, views, stored procedures, function, and packages
â¢ Monitored SQL Server Error Logs and Application Logs through SQL Server Agent
â¢ Prepared Data Flow Diagrams, Entity Relationship Diagrams using UML
â¢ Responsible for Designing, developing and Normalization of database tables
â¢ Experience in performance tuning using SQL profiler.
â¢ Involved in QA, UAT, knowledge transfer and support activities
Technical Environment: SQL Server 2008/2014, Visual Studio 2010, Windows Server, Performance
Monitor, SQL Server Profiler, C#, PL-SQL, T-SQL."
Data Science,"Education Details 
 MCA   YMCAUST,  Faridabad,  Haryana
Data Science internship 


Skill Details 
Data Structure- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
Data Analysis- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Core Java- Exprience - Less than 1 year months
Database Management- Exprience - Less than 1 year monthsCompany Details 
company - Itechpower
description - "
Data Science,"SKILLS C Basics, IOT, Python, MATLAB, Data Science, Machine Learning, HTML, Microsoft Word, Microsoft Excel, Microsoft Powerpoint. RECOGNITION Academic Secured First place in B.Tech.Education Details 
August 2014 to May 2018 B.Tech.  Ghatkesar, Andhra Pradesh Aurora's Scientific and Technological Institute
June 2012 to May 2014  Secondary Education Warangal, Telangana SR Junior College
Data Science 


Skill Details 
MS OFFICE- Exprience - Less than 1 year months
C- Exprience - Less than 1 year months
machine learning- Exprience - Less than 1 year months
data science- Exprience - Less than 1 year months
Matlab- Exprience - Less than 1 year monthsCompany Details 
company - 
description - "
Data Science,"Skills â¢ Python â¢ Tableau â¢ Data Visualization â¢ R Studio â¢ Machine Learning â¢ Statistics IABAC Certified Data Scientist with versatile experience over 1+ years in managing business, data science consulting and leading innovation projects, bringing business ideas to working real world solutions. Being a strong advocator of augmented era, where human capabilities are enhanced by machines, Fahed is passionate about bringing business concepts in area of machine learning, AI, robotics etc., to real life solutions.Education Details 
January 2017 B. Tech Computer Science & Engineering Mohali, Punjab Indo Global College of Engineering
Data Science Consultant 

Data Science Consultant - Datamites
Skill Details 
MACHINE LEARNING- Exprience - 13 months
PYTHON- Exprience - 24 months
SOLUTIONS- Exprience - 24 months
DATA SCIENCE- Exprience - 24 months
DATA VISUALIZATION- Exprience - 24 months
Tableau- Exprience - 24 monthsCompany Details 
company - Datamites
description - â¢ Analyzed and processed complex data sets using advanced querying, visualization and analytics tools.
â¢ Responsible for loading, extracting and validation of client data.
â¢ Worked on manipulating, cleaning & processing data using python.
â¢ Used Tableau for data visualization.
company - Heretic Solutions Pvt Ltd
description - â¢ Worked closely with business to identify issues and used data to propose solutions for effective decision making.
â¢ Manipulating, cleansing & processing data using Python, Excel and R.
â¢ Analyzed raw data, drawing conclusions & developing recommendations.
â¢ Used machine learning tools and statistical techniques to produce solutions to problems."
Data Science,"Education Details 
 B.Tech   Rayat and Bahra Institute of Engineering and Biotechnology
Data Science 

Data Science
Skill Details 
Numpy- Exprience - Less than 1 year months
Machine Learning- Exprience - Less than 1 year months
Tensorflow- Exprience - Less than 1 year months
Scikit- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
GCP- Exprience - Less than 1 year months
Pandas- Exprience - Less than 1 year months
Neural Network- Exprience - Less than 1 year monthsCompany Details 
company - Wipro
description - Bhawana Aggarwal
E-Mail:bhawana.chd@gmail.com
Phone: 09876971076
VVersatile, high-energy professional targeting challenging assignments in Machine
PROFILE SUMMARY
âª An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability.
TECHNICAL SKILLS
Programming Languages Python, C
Libraries Seaborn, Numpy, Pandas, Cufflinks, Matplotlib
Algorithms
KNN, Decision Tree, Linear regression, Logistic Regression, Neural Networks, K means clustering,
Tensorflow, SVM
Databases SQL, Oracle
Operating Systems Linux, Window
Development Environments NetBeans, Notebooks, Sublime
Ticketing tools Service Now, Remedy
Education
UG Education:
B.Tech (Computer Science) from Rayat and Bahra Institute of Engineering and Biotechnology passed with 78.4%in
2016.
Schooling:
XII in 2012 from Moti Ram Arya Sr. Secondary School(Passed with 78.4%)
X in 2010 from Valley Public School (Passed with 9.4 CGPA)
WORK EXPERINCE
Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
OTHER PROJECTS
Title : Diabetes Detection
Brief : Developed the software which can detect whether the person is suffering from Diabetes or not and got the third
prize in it.
TRAINING AND CERTIFICATIONS
Title: Python Training, Machine Learning, Data Science, Deep Learning
Organization: Udemy, Coursera (Machine Learning, Deep Learning)
Personal Profile
Fatherâs Name :Mr. Tirlok Aggarwal
Language Known : English & Hindi
Marital Status :Single
Date of Birth(Gender):1993-12-20(YYYY-MM-DD) (F)
company - Wipro
description - Developing programs in Python.
company - Wipro
description - Title : Wipro Neural Intelligence Platform
Team Size : 5
Brief: Wiproâs Neural Intelligence Platform harnesses the power of automation and artificial intelligence
technologiesânatural language processing (NLP), cognitive, machine learning, and analytics. The platform
comprises three layers: a data engagement platform that can easily access and manage multiple structured and
unstructured data sources; an âintent assessment and reasoningâ engine that includes sentiment and predictive
analytics; and a deep machine learning engine that can sense, act, and learn over time. The project entailed
automating responses to user queries at the earliest. The Monster Bot using the power of Deep Machine Learning,
NLP to handle such queries. User can see the how their queries can be answered quickly like allL1 activities can be
eliminated.
Entity Extractor -> This involves text extraction and NLP for fetching out important information from the text like
dates, names, places, contact numbers etc. This involves Regex, Bluemix NLU apiâs and machine learning using
Tensor flow for further learning of new entities.
Classifier ->This involves the classifications of classes, training of dataset and predicting the output using the SKLearn
classifier (MNB, SVM, SGD as Classifier) and SGD for the optimization to map the user queries with the best
suited response and make the system efficient.
NER: A Deep Learning NER Model is trained to extract the entities from the text. Entities like Roles, Skills,
Organizations can be extracted from raw text. RNN(LSTM) Bidirectional model is trained for extracting such entities
using Keras TensorFlow framework.
company - Wipro Technologies
description - An IT professional with knowledge and experience of 2 years in Wipro Technologies in Machine
Learning, Deep Learning, Data Science, Python, Software Development.
âª Skilled in managing end-to-end development and software products / projects from inception, requirement
specs, planning, designing, implementation, configuration and documentation.
âª Knowledge on Python , Machine Learning, Deep Learning, data Science, Algorithms, Neural Network,
NLP, GCP.
âª Knowledge on Python Libraries like Numpy, Pandas, Seaborn , Matplotlib, Cufflinks.
âª Knowledge on different algorithms in Machine learning like KNN, Decision Tree, Bias variance Trade off,
Support vector Machine(SVM),Logistic Regression, Neural networks.
âª Have knowledge on unsupervised, Supervised and reinforcement data.
âª Programming experience in relational platforms like MySQL,Oracle.
âª Have knowledge on Some programming language like C++,Java.
âª Experience in cloud based environment like Google Cloud.
âª Working on different Operating System like Linux, Ubuntu, Windows.
âª Good interpersonal and communication skills.
âª Problem solving skills with the ability to think laterally, and to think with a medium term and long term
perspective
âª Flexibility and an open attitude to change.
âª Ability to create, define and own frameworks with a strong emphasis on code reusability."
Data Science,"Personal Skills â¢ Ability to quickly grasp technical aspects and willingness to learn â¢ High energy levels & Result oriented. Education Details 
January 2018 Master of Engineering Computer Technology & Application Bhopal, Madhya Pradesh Truba Institute of Engineering & Information Technology
January 2010 B.E. computer science Bhopal, Madhya Pradesh RKDF Institute of Science and Technology College of Engineering
January 2006 Polytechnic Information Technology Vidisha, Madhya Pradesh SATI Engineering College in Vidisha
January 2003 M.tech Thesis Detail  BMCH School in Ganj basoda
Data science 

I have six month experience in Data Science. Key Skills: - Experience in Machine Learning, Deep Leaning, NLP, Python, SQL, Web Scraping Good knowledge in computer subjects and ability to update
Skill Details 
Experience in Machine Learning, Deep Learning, NLP, Python, SQL, Web Crawling, HTML,CSS.- Exprience - Less than 1 year monthsCompany Details 
company - RNT.AI Technology Solution
description - Text classification using Machine learning Algorithms with python.
Practical knowledge of Deep learning algorithms such as Â Recurrent Neural Networks(RNN).
Develop custom data models and algorithms to apply to dataset
Experience with Python packages like Pandas, Scikit-learn, Tensor Flow, Numpy, Matplotliv, NLTK.
Comfort with SQL, Â MYSQL
Sentiment analysis.
Â Apply leave Dataset using classification technique like Tf--idf , LSA with cosine similarity using Machine learning Algorithms.
Web crawling using Selenium web driver and Beautiful Soup with python.
company - Life Insurance Corporation of India Bhopal
description - Ã¼Â Explaining policy features and the benefits
Ã¼ Updated knowledge of life insurance products and shared with customers"
Data Science,"Expertise â Data and Quantitative Analysis â Decision Analytics â Predictive Modeling â Data-Driven Personalization â KPI Dashboards â Big Data Queries and Interpretation â Data Mining and Visualization Tools â Machine Learning Algorithms â Business Intelligence (BI) â Research, Reports and Forecasts Education Details 
 PGP in Data Science  Mumbai, Maharashtra Aegis School of data science & Business
 B.E. in Electronics & Communication Electronics & Communication Indore, Madhya Pradesh IES IPS Academy
Data Scientist 

Data Scientist with PR Canada
Skill Details 
Algorithms- Exprience - 6 months
BI- Exprience - 6 months
Business Intelligence- Exprience - 6 months
Machine Learning- Exprience - 24 months
Visualization- Exprience - 24 months
spark- Exprience - 24 months
python- Exprience - 36 months
tableau- Exprience - 36 months
Data Analysis- Exprience - 24 monthsCompany Details 
company - Aegis school of Data Science & Business
description - Mostly working on industry project for providing solution along with Teaching Appointments: Teach undergraduate and graduate-level courses in Spark and Machine Learning as an adjunct faculty member at Aegis School of Data Science, Mumbai (2017 to Present)
company - Aegis school of Data & Business
description - Data Science Intern, Nov 2015 to Jan 2016

Furnish executive leadership team with insights, analytics, reports and recommendations enabling effective strategic planning across all business units, distribution channels and product lines.

â Chat Bot using AWS LEX and Tensor flow  Python
The goal of project creates a chat bot for an academic institution or university to handle queries related courses offered by that institute. The objective of this task is to reduce human efforts as well as reduce man made errors. Even by this companies handle their client 24x7. In this case companies are academic institutions and clients are participants or students.
â Web scraping using Selenium web driver   Python
The task is to scrap the data from the online messaging portal in a text format and have to find the pattern form it.
â Data Visualization and Data insights   Hadoop Eco System, Hive, PySpark, QlikSense
The goal of this project is to build a Business Solutions to a Internet Service Provider Company, like handling data which is generated per day basis, for that we have to visualize that data and find the usage pattern form it and have a generate a reports.
â Image Based Fraud Detection   Microsoft Face API, PySpark, Open CV
The main goal of project is Recognize similarity for a face to given Database images. Face recognition is the recognizing a special face from set of different faces. Face is extracted and then compared with the database Image if that Image recognized then the person already applied for loan from somewhere else and now hiding his or her identity, this is how we are going to prevent the frauds in the initial stage itself.
â Churn Analysis for Internet Service Provider   R, Python, Machine Learning, Hadoop
The objective is to identify the customer who is likely to churn in a given period of time; we have to pretend the customer giving incentive offers.
â Sentiment Analysis   Python, NLP, Apache Spark service in IBM Bluemix.
This project is highly emphasis on tweets from Twitter data were taken for mobile networks service provider to do a sentiment analysis and analyze whether the expressed opinion was positive, negative or neutral, capture the emotions of the tweets and comparative analysis.

Quantifiable Results:
â Mentored 7-12 Data Science Enthusiast each year that have all since gone on to graduate school in Data Science and Business Analytics.
â Reviewed and evaluated 20-40 Research Papers on Data Science for one of the largest Data Science Conference called Data Science Congress by Aegis School of Business Mumbai.
â Heading a solution providing organization called Data Science Delivered into Aegis school of Data Science Mumbai and managed 4-5 live projects using Data Science techniques.
â Working for some social cause with the help of Data Science for Social Goods Committee, where our team developed a product called ""Let's find a missing Child"" for helping society.
company - IBM India pvt ltd
description - Mostly worked on blumix and IBM Watson for Data science."
Business Analyst,"Education Details 
 BE Computer Science Mumbai, Maharashtra Mumbai University
 HSC  Mumbai, Maharashtra Maharashtra State Board
 SSC  Mumbai, Maharashtra Maharashtra State Board
Business Analyst 

Business Analyst - Fino Payments Bank
Skill Details 
Company Details 
company - Fino Payments Bank
description - Key Role   In-depth requirement and input gathering
Responsibilities and Achievements:
â¦ Conducted in-depth requirement and input gathering from all concerned stakeholders [Business SMEs, Technical Architect and Business Architect] to create artifacts like Business Requirement Document (BRD) to arrive at functional requirements for development team
â¦ Created Functional Specification Document (FSD) highlighting the technical implementation and the use cases
â¦ Led the Merchant Commission Module project from end-to-end and co-ordinated for CUG and Live
â¦ Designed the Account Opening Process flow end-to-end during the time when bank was going Live.
â¦ SPOC for all the configurations (both account level and customer level) in production.
â¦ Led the Cash Controlling Processes for the field users as per the requirement from the business team.
â¦ Design and build proof of concepts to validate the viability of alternate approaches and determine optimum choice
â¦ Involved in Process Design for development of the products
â¦ Performed Functional Testing of the entire system and provided support during UAT by preparing UAT test cases, performing UAT tests to onboard new processes as BAU
â¦ Worked with the development teams in arriving at detailed techno-functional specifications, participate in Feasibility
Analysis
â¦ Conducting twice a week meetings with the vendor to discuss the status of CRs and to resolve technical queries
company - Fino Paytech Pvt. Ltd
description - Key Role   Requirement gathering, Development, Testing
Responsibilities and Achievements:
â¦ Requirement gathering, preparation of traceability matrix, preparation and execution of use cases, developing of test plans based on requirements for Airtel Zambia National Partner Project
â¦ Led the employee profile creation, maintenance of employee details in the database: Preparation of work flow, end-to-end development and testing of the module
â¦ Designed the work flow process of the CAPA (Corrective Action Preventive Analysis) module to maintain the audit findings raised by the internal audit team
â¦ Designed the Expense Management module and automated it for end-to-end in-house expense flow
â¦ Designed the PMO tool Parivartan used for tracking the projects end-to-end"
Business Analyst,"Technical Skills Application Servers: IIS 6.0, Jboss 7.1. Database: SQL, Oracle and DB2. Report Tool: iReport, Crystal report. Career GraphEducation Details 

Business Analyst 

Business Analyst - Zensar Technologies Ltd
Skill Details 
CRYSTAL REPORT- Exprience - 15 months
DATABASE- Exprience - 6 months
DB2- Exprience - 6 months
IIS- Exprience - 6 months
IIS 6- Exprience - 6 monthsCompany Details 
company - Zensar Technologies Ltd
description - Location: Goregoan, Mumbai (Client -SUN Pharmaceutical)
Designation: Business Analyst.
Role: Requirement gathering, gap analysis, support, end user training, documentation.
company - Proteus Technologies Pvt Ltd
description - Base Information Management Pvt. Ltd. Is a Mumbai base software service provider with core competency and proven track record of installations of Enterprise Wide Solutions. Base customers come from industries as wide as Pharmaceuticals, Life Sciences, Plastics, Engineering, Chemicals
company - Wings Infonet Pvt Ltd
description - It is IT solutions Provider Company. Company provides comprehensive technology solutions to Accounting, trade, payroll and Asset Management firms across the world.
company - Hiral Tektronix Pvt Ltd
description - Software relates to recruitment (HRMS), accounting, and Payroll.

Job Responsibilities: â¢ ERP Implementation and after go live support.
â¢ Documenting user requirements and developing specifications for customization.
â¢ Integrating with other modules, integration testing & extending Post Go-live support, including training support to end-users.
â¢ Drafting functional requirements for ERP systems to represent the processes and functions involved.
â¢ Guiding the users in using various modules and the management for various functional issues.
â¢ Delivering awareness about various reports available in ERP system for day to day transactions and for MIS reporting of departments
System Audit for better results and follow ups for Observation.
â¢ Developing and designing report using iReport and crystal report"
Business Analyst,"Key Skills - Requirement Gathering - Requirement Analysis -Design Specifications - Client Communication - System Documentation - Problem solving - SDLC Operating Systems: Windows OS, UNIX (Linux/Ubuntu) Languages: Java, C++ Web Languages: JavaScript, HTML Tools: Citrix Software, System Architect, Quality Center v9.0 & v10.0, Tortoise SVN, DOORS, Artifact Viewer, JformDesigner, JIRA, Microsoft D365 Other Skills: Microsoft Office, MS Excel, MS PowerPoint, MS Visio, AutoCAD, VLSI, MS-CIT Certified. Education Details 
January 2012 BE Electronics Mumbai, Maharashtra Mumbai University
January 2006    Maharashtra State Board
Business Analyst 

Business Analyst - Intertek India Pvt Ltd
Skill Details 
SDLC- Exprience - 75 months
VISIO- Exprience - 60 months
REQUIREMENT GATHERING- Exprience - 15 months
Documentation- Exprience - Less than 1 year months
Functional Testing- Exprience - Less than 1 year months
Business Analysis- Exprience - Less than 1 year months
Jira- Exprience - Less than 1 year monthsCompany Details 
company - Intertek India Pvt Ltd
description - Business Analyst. Key responsibilities include Requirements Gathering, Requirements Analysis. Documentation like FRD creation. Providing KT sessions to the team. Having Client Communication. Gap Analysis.
company - Intertek India Pvt Ltd
description - Requirement Gathering from Businesses. Creating FRDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Walkthrough to team regarding the requirement â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Creating UAT Test cases. Executing the same.
â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	Inlight (Feb 2018 till date)
Platform	.Net

Description:
Inlight - (Supplier Risk Assessment Application)
Inlight is an Application designed to assess the Suppliers within the Supply chain. The Application on boards the Importers, Exporters and Suppliers. Based on the role they perform a Questionnaire is assigned to them and they fill out the Questionnaire. Basis the answer a scoring methodology is defined and the Suppliers are assessed to be Critical, High, Medium and Low. This helps in assessing the risk involved in working with certain Suppliers in the Supply chain.

Beyond Curriculum â Completed Internship in L&T â Attended Logistics Business School Training in Germany.
â A1 Certified in German Language.
â Travelled Onsite for Business Meetings and Discussions with Clients.

Personal Dossier .
company - AllCargo India Pvt Ltd
description - FRD creation
Client communication
Vendor Management
Having product Walk through with the team.
company - AllCargo India Pvt Ltd
description - Requirement Gathering from Businesses. Creating BRDs and FSDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	CRM (Nov 2017 to Feb 2018)
Platform	Microsoft D365

Description:
CRM - (Sales Management System)
CRM is a Software solution specially designed for handling Sales Management. This is a product provided by Microsoft which helps in tracking the sales of company, the activities of the salesperson, 360-degree view of customer accounts. This basically helps to get the overall status and view of various businesses the company is achieving from different Customers. A platform where the salesperson provides the details of Lead, Opportunity, Accounts and Businesses. Available on Cloud.

Project 2	Credit Risk (Nov 2017 to Feb 2018)
Platform	.Net

Description:
Credit Risk - (Customer credit check Management System)
Credit Risk is a Software solution specially designed for checking the credit status of the customer from which businesses are gained. The software basically is designed to take the KYC and the consent from the customer. For those customers who provide the consent, the credit report and monitoring report are obtained from the Credit Bureau. Based on the reports the customer health can be determined and business with them can either be  or discontinued.

Work Experience 3:
company - Capgemini India Pvt Ltd
description - Client: DB Schenker â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Impart business process knowledge transfer to the team members. Prepare business / functional process workflow using Visio, UML etc â Working knowledge of OOAD - Object Oriented Analysis & Design concept.
â Helping the Junior BAs in their work. Supervising their work.
â Tools & Applications: System Architect, DOORS, UML designs & concepts, HP Quality Center, MWB, Jformdesigner â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
company - Capgemini India Pvt Ltd
description - Platform	Java

Description:
TANGO - (Sea & Air Cargo Management System)
TANGO is a Software solution specially designed for handling sea and air cargo (Import & Export) Management. TANGO manages the creation of Shipment, Tracking the shipment via multiple service legs i.e. Pick-up, Delivery leg etc. It helps in managing the end to end shipment with respect to the entire department involvement (globally)

Work Experience 2:
company - Capgemini India Pvt Ltd
description - "
Business Analyst,"IT Skills: Area Exposure Modeling Tool: Bizagi, MS Visio Prototyping Tool: Indigo Studio. Documentation: MS Office (MS Word, MS Excel, MS Power Point) Testing Proficiency: Smoke, Sanity, Integration, Functional, Acceptance and UI Methodology implemented: Waterfall, Agile (Scrum) Database: SQL Testing Tool: HPQC Business Exposure Education Details 
 Bachelor Of Computer Engineering Computer Engineering Mumbai, Maharashtra Thadomal Shahani Engineering college
 Diploma Computer Engineering Ulhasnagar, Maharashtra Institute of Technology
 Secondary School Certificate  Ulhasnagar, Maharashtra New English High School
Senior Business Analyst - RPA 

Senior Business Analyst - RPA - Hexaware Technologies
Skill Details 
DOCUMENTATION- Exprience - 47 months
TESTING- Exprience - 29 months
INTEGRATION- Exprience - 25 months
INTEGRATOR- Exprience - 25 months
PROTOTYPE- Exprience - 13 monthsCompany Details 
company - Hexaware Technologies
description - Working as a RPA Business Analyst
company - BBH- Brown Brothers Harriman & Co
description - is a private bank that provides commercial banking, investment management, brokerage, and trust services to private companies and individuals. It also performs merger advisory, foreign exchange, custody services, commercial banking, and corporate financing services.

Responsibilities: â¢ Performed Automation Assessment of various Processes and identified processes which can be candidates of RPA.
â¢ Conducting Assessment that involves an initial Understanding of the Existing System, their technology, processes, Usage of the tools, Feasibility of tool with automation tool along with automation ROI analysis.
â¢ Preparing the Automation Potential Sheet which describes the steps in the process, the volume and frequency of the transaction, the AHT taken by SME to perform the process and depending on the steps that could be automated, Automation potential and the manual efforts that will be saved are calculated.
Calculating the complexity of the Process which is considered for automation and depending on all these factors Number of Bots and Number of Automation tool Licenses are determined.
â¢ Implementing a Proof of Concept (POC) to Validate Feasibility by executing the selected critical use cases for conducting a POC which will helps to identify financial and operational benefits and provide recommendations regarding the actual need for complete automation.
â¢ Gathering business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME's) â¢ Preparing Business Requirement Document and then converted Business requirements into Functional Requirements Specification.
 â¢ Constructing prototype early toward a design acceptable to the customer and feasible.
â¢ Assisting in designing test plans, test scenarios and test cases for integration, regression, and user acceptance testing (UAT) to improve the overall quality of the Automation.
â¢ Participating regularly in Walkthroughs and Review meetings with Project Manager, QA Engineers, and Development team.
â¢ Regularly interacting with offshore and onshore development teams.
company - FADV - First Advantage
description - is a criminal background check company that delivers global solutions ranging from employment screenings to background checks.
The following are the processes which were covered:
Email Process, Research Process, Review Process.

Responsibilities: â¢ Requirement Gathering through conducting Interviews & Brainstorming sessions with stakeholders â¢ To develop decision models and execute those rules as per the use case specifications.
â¢ To Test/validate the decision models against document test data.
â¢ To maintain and enhance the decision models for changes in regulations as per use case specifications.
â¢ Responsible for performing the business research that will make a business growth.
â¢ Developing a clear understanding of existing business functions and processes.
â¢ Effectively communicate with the onsite clients for the queries, suggestions, and update.
â¢ Giving suggestions to enhance the current processes.
â¢ Identifying areas for process improvement.
â¢ Flagging up potential problems at an early stage.
â¢ Preparing PowerPoint presentations and documents for business meetings.
â¢ Using any information gathered to write up detailed reports.
â¢ Highlighting risks and issues that could impact project delivery.
â¢ Able to work accurately.
â¢ To develop and maintain documentation for internal team training and client end user operations.
â¢ To work efficiently with team members and across teams.
â¢ To mentor and train junior team members.
company - Clinical Testing, Lab Work and Diagnostic Testing
description - IQVIA provides services to its customers this includes: Clinical Testing, Lab Work and Diagnostic Testing under clinical trial. These customers need to pay to IQVIA and aging details and invoices are generated for the same.
The following are the processes which were covered:

Tracking Payments, Automated Real Time Metrics Reporting (Dashboard), Past Due Notifications, AR Statements, Credit/Rebill.
Responsibilities: â¢ Conducting meetings with clients and key stakeholders to gather requirements, analyze, finalize and have formal sign-offs from approvers Gather and perform analysis of the business requirements â¢ Translating the business requirements into the Business Requirement Document [BRD], Functional Requirement Document [FRD].
â¢ Facilitating meetings with the appropriate subject matter experts in both business and technology teams â¢ Coordinating with business user community for the execution of user acceptance test as well as tracking issues â¢ Working, collaborating and coordinating with Offshore and Onsite team members to fulfill the BA responsibilities from project initiation to Post-Implementation â¢ Reviewing the test scripts with business users as well as technology team. Execute test scripts with expected results for the System Integration Test (SIT) and User Acceptance Test (UAT) â¢ Coordinating and conducting the Production Acceptance Testing (PAT) with the business users â¢ Creating flow diagrams, structure charts, and other types of system or process representations â¢ Managing changes to requirements and baseline through a change control process â¢ Utilizing standard methods, design and testing tools throughout project development life cycle â¢ Work closely with the operational functional teams, operations management, and personnel, and various technology teams to facilitate a shared understanding of requirements and priorities across all areas
company - Eduavenir IT Solution
description - Project: M.B.M.S

M.B.M.S. - is an Inventory management application that allows user to manage inventory details of different warehouses, having different products located at various locations and help extract what goods have been procured, sold or returned by customers. It generates automated invoicesalong withcustomized reports. It also managescustomer complaint and resolution system implementation along with automated MIS on monthly basis.Sales and forecastingis also developed on MIS System and the streamlining of process of warehousing and dispatch along with online proof of delivery management system (POD documentation) is generated.

Responsibilities: â¢ Participate in requirement gathering discussion with client to understand the flow of business processes â¢ Analyze the requirements and determine the core processes, develop Process Documentation and ensure to stay up-to-date in conjunction with on-going changes â¢ Participate in process flow analysis and preparing BRD, SRS.
â¢ Coordinating with developers, designers & operations teams for various nuances of the project, communicate the stakeholder requirements from requirement /enhancement to implementation and finally deliver the same within estimated timeframe.
â¢ Support UAT by reviewing test cases, manage version control of documents, software builds.
â¢ Coordinate with the stakeholders for UAT sign off and coordinate internally for production movement till Golive stage of the application.
â¢ Provide demo and training to internal and end user using PowerPoint presentation.
â¢ Resolving project functional &technical issues during UAT.
â¢ Prioritizing the Production bugs and resolving the same within the estimated timeframe.
â¢ Preparing Project Status Report and Production Bugs Status to all the stakeholders.
â¢ Promoting and Networking for online trading platform.
â¢ Designing query sheet for obtaining and comparison of quotes from various vendors.
â¢ Development of product codes / material codes for inventory management (Master Data Management)
company - CAPGEMINI Head Office
description - Type: Mobile and Device Testing.       Duration: January 2014 - August 2014

Follet - An application which takes an electronic request from the user for the books he requires from a particular follet store. This detailed information about books that will include the name of the book, its price, the date of the transaction and the parties involved which will then be sent to follet stores. User then create request for one or more books for a given date. This request is then processed further and user gets a mail of the date when he will be provided with that book.

Responsibilities: â¢ Understanding the needs and business requirements.
â¢ Preparing BRD, SRS by eliciting all the requirements from the client and SMEs â¢ Understanding the dependency of the modules in the system â¢ Preparation of test plan for Unit level and Integration level.
â¢ Preparation and execution of test cases.
â¢ Defect tracking, Issue Resolution, Risk Monitoring, Status Tracking, Reporting and Follow-up.
â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - 
company - CAPGEMINI Head Office
description - Humana is a health care insurance project of U.S. which deals with supplying various medicines to citizens as per the doctor's reference and patient's insurance policy. This application keeps track of all the medicines user has consumed in the past and generates a patient history. A citizen is given a drug only after the doctor's reference so the doctor's information is also linked with the patient's history.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Involved in writing test cases based on test scenarios and execute them.
â¢ Ensuring Test Coverage using Requirement Traceability Matrix (RTM) â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - Testing Trends WQR (World Quality Report) is an application which allows the users to take a survey on different methods and technologies used for testing. Users can choose to answer any type of questions under three different categories. Users have a facility to search, view and export the data to excel. Also, users get daily and weekly reports through email about the new trends in testing implemented around the globe. Testing Trends WQR app is available on Android and IOS platforms.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Writing test cases based on test scenarios and executed them.
â¢ Performing different types of testing such as Functional, Integration, System, and UAT.
â¢ Defect resolution and maintenance of the application."
Business Analyst,"TECHNOLOGICAL SKILLS â¦ Knowledge of Computers on the Windows platform. â¦ Fluency in MS-Office Applications such as Excel, Word, PowerPoint, etc. â¦ HTML, JAVA, PHP ATTRIBUTES â¦ Hardworking towards achieving the Goal â¦ Good communication skills â¦ Quick learner â¦ Good interpersonal relationEducation Details 
January 2016 to January 2018 MMS  Mumbai, Maharashtra University of Mumbai
January 2016 to January 2018 MMS Management Mumbai, Maharashtra University of Mumbai
January 2014 B.Sc.  Bandra, MAHARASHTRA, IN Rivzi College
January 2011 HSC  Bandra, Maharashtra, IN St. Andrews College
January 2011 HSC   Allana Junior College
January 2009 SSC   Canossa High School
January 2008 SSC   Maharashtra State Board
Business Analyst 

Business Analyst - Mass Group of Companies
Skill Details 
EXCEL- Exprience - 23 months
HTML- Exprience - 6 months
JAVA- Exprience - 6 months
PHP- Exprience - 6 months
POWERPOINT- Exprience - 6 monthsCompany Details 
company - Mass Group of Companies
description - Key Role   Analyst
Responsibilities:
â¦ Manage risk and provide risk management process.
â¦ Liaise with other project areas to coordinate with interdependencies and resolve issues.
â¦ Analyse and map business process.
â¦ Guide stakeholders on devising effective and efficient approaches to achieving project objectives.
â¦ Preparation of various Derivable i.e Business requirement, functional requirement and report specification.
company - Commversion Pvt.LTD
description - Responsibilities:
â¦ Employee Satisfaction reports
â¦ Evaluate overall work
â¦ Internal Audit
â¦ Maintain timesheet validation.
â¦ Set Simple and performance driven compensation strategies and polices."
Business Analyst,"Education Details 
February 2006 to February 2006 TYBCOM Commerce  mumbai
Business Analyst 

Business Analyst
Skill Details 
Company Details 
company - Motilal Oswal
description - Business Analyst
Handling IT Operation for Institutional Equities 
Maintain Daily MIS in Excel for CAG, Research, Derivative, Sales team Preparing Auto Dashboard For Research, Sales, Trading team Working on Excel Macro to Create Innovative Report 
Working on Block Related Data Working on BD Fund from different GEO Working on Investors Corporate Meeting to track Corporate Block & Fund Interest in Sector
company - FSS
description - Project Description:
Maintain and prepare cash indent, cash report, cash position, and cash planning
Responsibilities:

â¢ Maintain Daily MIS in excel.
â¢ Provide complete information about MIS & ATM.
â¢ Maintain and prepare cash indent, cash report, cash position., cash planning
â¢ Co-ordinate with BANK CASH DEPARTMENT.
â¢ Co-ordinate with custodians

.
DEGREE/ COURSE          YEAR of PASSING   GRADE          INSTITUTE                              UNIVERSITY/ BOARD   PERCENTAGE
company - ANGEL BROKING
description - Reporting: Assistant Manager /   SR. Manger
Responsibilities:

â¢ Handling team of 14 Quality assurance team members
â¢ Maintain Daily MIS in excel of team productivity
â¢ Maintain and prepare repots, adding comments on remark
â¢ mailing client  for modification of given number
â¢ Mailing reports to different branches
â¢ Coordinating with RM
â¢ Provide complete information about script to client"
Business Analyst,"Key Skills - Requirement Gathering - Requirement Analysis -Design Specifications - Client Communication - System Documentation - Problem solving - SDLC Operating Systems: Windows OS, UNIX (Linux/Ubuntu) Languages: Java, C++ Web Languages: JavaScript, HTML Tools: Citrix Software, System Architect, Quality Center v9.0 & v10.0, Tortoise SVN, DOORS, Artifact Viewer, JformDesigner, JIRA, Microsoft D365 Other Skills: Microsoft Office, MS Excel, MS PowerPoint, MS Visio, AutoCAD, VLSI, MS-CIT Certified. Education Details 
January 2012 BE Electronics Mumbai, Maharashtra Mumbai University
January 2006    Maharashtra State Board
Business Analyst 

Business Analyst - Intertek India Pvt Ltd
Skill Details 
SDLC- Exprience - 75 months
VISIO- Exprience - 60 months
REQUIREMENT GATHERING- Exprience - 15 months
Documentation- Exprience - Less than 1 year months
Functional Testing- Exprience - Less than 1 year months
Business Analysis- Exprience - Less than 1 year months
Jira- Exprience - Less than 1 year monthsCompany Details 
company - Intertek India Pvt Ltd
description - Business Analyst. Key responsibilities include Requirements Gathering, Requirements Analysis. Documentation like FRD creation. Providing KT sessions to the team. Having Client Communication. Gap Analysis.
company - Intertek India Pvt Ltd
description - Requirement Gathering from Businesses. Creating FRDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Walkthrough to team regarding the requirement â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Creating UAT Test cases. Executing the same.
â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	Inlight (Feb 2018 till date)
Platform	.Net

Description:
Inlight - (Supplier Risk Assessment Application)
Inlight is an Application designed to assess the Suppliers within the Supply chain. The Application on boards the Importers, Exporters and Suppliers. Based on the role they perform a Questionnaire is assigned to them and they fill out the Questionnaire. Basis the answer a scoring methodology is defined and the Suppliers are assessed to be Critical, High, Medium and Low. This helps in assessing the risk involved in working with certain Suppliers in the Supply chain.

Beyond Curriculum â Completed Internship in L&T â Attended Logistics Business School Training in Germany.
â A1 Certified in German Language.
â Travelled Onsite for Business Meetings and Discussions with Clients.

Personal Dossier .
company - AllCargo India Pvt Ltd
description - FRD creation
Client communication
Vendor Management
Having product Walk through with the team.
company - AllCargo India Pvt Ltd
description - Requirement Gathering from Businesses. Creating BRDs and FSDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	CRM (Nov 2017 to Feb 2018)
Platform	Microsoft D365

Description:
CRM - (Sales Management System)
CRM is a Software solution specially designed for handling Sales Management. This is a product provided by Microsoft which helps in tracking the sales of company, the activities of the salesperson, 360-degree view of customer accounts. This basically helps to get the overall status and view of various businesses the company is achieving from different Customers. A platform where the salesperson provides the details of Lead, Opportunity, Accounts and Businesses. Available on Cloud.

Project 2	Credit Risk (Nov 2017 to Feb 2018)
Platform	.Net

Description:
Credit Risk - (Customer credit check Management System)
Credit Risk is a Software solution specially designed for checking the credit status of the customer from which businesses are gained. The software basically is designed to take the KYC and the consent from the customer. For those customers who provide the consent, the credit report and monitoring report are obtained from the Credit Bureau. Based on the reports the customer health can be determined and business with them can either be  or discontinued.

Work Experience 3:
company - Capgemini India Pvt Ltd
description - Client: DB Schenker â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Impart business process knowledge transfer to the team members. Prepare business / functional process workflow using Visio, UML etc â Working knowledge of OOAD - Object Oriented Analysis & Design concept.
â Helping the Junior BAs in their work. Supervising their work.
â Tools & Applications: System Architect, DOORS, UML designs & concepts, HP Quality Center, MWB, Jformdesigner â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
company - Capgemini India Pvt Ltd
description - Platform	Java

Description:
TANGO - (Sea & Air Cargo Management System)
TANGO is a Software solution specially designed for handling sea and air cargo (Import & Export) Management. TANGO manages the creation of Shipment, Tracking the shipment via multiple service legs i.e. Pick-up, Delivery leg etc. It helps in managing the end to end shipment with respect to the entire department involvement (globally)

Work Experience 2:
company - Capgemini India Pvt Ltd
description - "
Business Analyst,"IT Skills: Area Exposure Modeling Tool: Bizagi, MS Visio Prototyping Tool: Indigo Studio. Documentation: MS Office (MS Word, MS Excel, MS Power Point) Testing Proficiency: Smoke, Sanity, Integration, Functional, Acceptance and UI Methodology implemented: Waterfall, Agile (Scrum) Database: SQL Testing Tool: HPQC Business Exposure Education Details 
 Bachelor Of Computer Engineering Computer Engineering Mumbai, Maharashtra Thadomal Shahani Engineering college
 Diploma Computer Engineering Ulhasnagar, Maharashtra Institute of Technology
 Secondary School Certificate  Ulhasnagar, Maharashtra New English High School
Senior Business Analyst - RPA 

Senior Business Analyst - RPA - Hexaware Technologies
Skill Details 
DOCUMENTATION- Exprience - 47 months
TESTING- Exprience - 29 months
INTEGRATION- Exprience - 25 months
INTEGRATOR- Exprience - 25 months
PROTOTYPE- Exprience - 13 monthsCompany Details 
company - Hexaware Technologies
description - Working as a RPA Business Analyst
company - BBH- Brown Brothers Harriman & Co
description - is a private bank that provides commercial banking, investment management, brokerage, and trust services to private companies and individuals. It also performs merger advisory, foreign exchange, custody services, commercial banking, and corporate financing services.

Responsibilities: â¢ Performed Automation Assessment of various Processes and identified processes which can be candidates of RPA.
â¢ Conducting Assessment that involves an initial Understanding of the Existing System, their technology, processes, Usage of the tools, Feasibility of tool with automation tool along with automation ROI analysis.
â¢ Preparing the Automation Potential Sheet which describes the steps in the process, the volume and frequency of the transaction, the AHT taken by SME to perform the process and depending on the steps that could be automated, Automation potential and the manual efforts that will be saved are calculated.
Calculating the complexity of the Process which is considered for automation and depending on all these factors Number of Bots and Number of Automation tool Licenses are determined.
â¢ Implementing a Proof of Concept (POC) to Validate Feasibility by executing the selected critical use cases for conducting a POC which will helps to identify financial and operational benefits and provide recommendations regarding the actual need for complete automation.
â¢ Gathering business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME's) â¢ Preparing Business Requirement Document and then converted Business requirements into Functional Requirements Specification.
 â¢ Constructing prototype early toward a design acceptable to the customer and feasible.
â¢ Assisting in designing test plans, test scenarios and test cases for integration, regression, and user acceptance testing (UAT) to improve the overall quality of the Automation.
â¢ Participating regularly in Walkthroughs and Review meetings with Project Manager, QA Engineers, and Development team.
â¢ Regularly interacting with offshore and onshore development teams.
company - FADV - First Advantage
description - is a criminal background check company that delivers global solutions ranging from employment screenings to background checks.
The following are the processes which were covered:
Email Process, Research Process, Review Process.

Responsibilities: â¢ Requirement Gathering through conducting Interviews & Brainstorming sessions with stakeholders â¢ To develop decision models and execute those rules as per the use case specifications.
â¢ To Test/validate the decision models against document test data.
â¢ To maintain and enhance the decision models for changes in regulations as per use case specifications.
â¢ Responsible for performing the business research that will make a business growth.
â¢ Developing a clear understanding of existing business functions and processes.
â¢ Effectively communicate with the onsite clients for the queries, suggestions, and update.
â¢ Giving suggestions to enhance the current processes.
â¢ Identifying areas for process improvement.
â¢ Flagging up potential problems at an early stage.
â¢ Preparing PowerPoint presentations and documents for business meetings.
â¢ Using any information gathered to write up detailed reports.
â¢ Highlighting risks and issues that could impact project delivery.
â¢ Able to work accurately.
â¢ To develop and maintain documentation for internal team training and client end user operations.
â¢ To work efficiently with team members and across teams.
â¢ To mentor and train junior team members.
company - Clinical Testing, Lab Work and Diagnostic Testing
description - IQVIA provides services to its customers this includes: Clinical Testing, Lab Work and Diagnostic Testing under clinical trial. These customers need to pay to IQVIA and aging details and invoices are generated for the same.
The following are the processes which were covered:

Tracking Payments, Automated Real Time Metrics Reporting (Dashboard), Past Due Notifications, AR Statements, Credit/Rebill.
Responsibilities: â¢ Conducting meetings with clients and key stakeholders to gather requirements, analyze, finalize and have formal sign-offs from approvers Gather and perform analysis of the business requirements â¢ Translating the business requirements into the Business Requirement Document [BRD], Functional Requirement Document [FRD].
â¢ Facilitating meetings with the appropriate subject matter experts in both business and technology teams â¢ Coordinating with business user community for the execution of user acceptance test as well as tracking issues â¢ Working, collaborating and coordinating with Offshore and Onsite team members to fulfill the BA responsibilities from project initiation to Post-Implementation â¢ Reviewing the test scripts with business users as well as technology team. Execute test scripts with expected results for the System Integration Test (SIT) and User Acceptance Test (UAT) â¢ Coordinating and conducting the Production Acceptance Testing (PAT) with the business users â¢ Creating flow diagrams, structure charts, and other types of system or process representations â¢ Managing changes to requirements and baseline through a change control process â¢ Utilizing standard methods, design and testing tools throughout project development life cycle â¢ Work closely with the operational functional teams, operations management, and personnel, and various technology teams to facilitate a shared understanding of requirements and priorities across all areas
company - Eduavenir IT Solution
description - Project: M.B.M.S

M.B.M.S. - is an Inventory management application that allows user to manage inventory details of different warehouses, having different products located at various locations and help extract what goods have been procured, sold or returned by customers. It generates automated invoicesalong withcustomized reports. It also managescustomer complaint and resolution system implementation along with automated MIS on monthly basis.Sales and forecastingis also developed on MIS System and the streamlining of process of warehousing and dispatch along with online proof of delivery management system (POD documentation) is generated.

Responsibilities: â¢ Participate in requirement gathering discussion with client to understand the flow of business processes â¢ Analyze the requirements and determine the core processes, develop Process Documentation and ensure to stay up-to-date in conjunction with on-going changes â¢ Participate in process flow analysis and preparing BRD, SRS.
â¢ Coordinating with developers, designers & operations teams for various nuances of the project, communicate the stakeholder requirements from requirement /enhancement to implementation and finally deliver the same within estimated timeframe.
â¢ Support UAT by reviewing test cases, manage version control of documents, software builds.
â¢ Coordinate with the stakeholders for UAT sign off and coordinate internally for production movement till Golive stage of the application.
â¢ Provide demo and training to internal and end user using PowerPoint presentation.
â¢ Resolving project functional &technical issues during UAT.
â¢ Prioritizing the Production bugs and resolving the same within the estimated timeframe.
â¢ Preparing Project Status Report and Production Bugs Status to all the stakeholders.
â¢ Promoting and Networking for online trading platform.
â¢ Designing query sheet for obtaining and comparison of quotes from various vendors.
â¢ Development of product codes / material codes for inventory management (Master Data Management)
company - CAPGEMINI Head Office
description - Type: Mobile and Device Testing.       Duration: January 2014 - August 2014

Follet - An application which takes an electronic request from the user for the books he requires from a particular follet store. This detailed information about books that will include the name of the book, its price, the date of the transaction and the parties involved which will then be sent to follet stores. User then create request for one or more books for a given date. This request is then processed further and user gets a mail of the date when he will be provided with that book.

Responsibilities: â¢ Understanding the needs and business requirements.
â¢ Preparing BRD, SRS by eliciting all the requirements from the client and SMEs â¢ Understanding the dependency of the modules in the system â¢ Preparation of test plan for Unit level and Integration level.
â¢ Preparation and execution of test cases.
â¢ Defect tracking, Issue Resolution, Risk Monitoring, Status Tracking, Reporting and Follow-up.
â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - 
company - CAPGEMINI Head Office
description - Humana is a health care insurance project of U.S. which deals with supplying various medicines to citizens as per the doctor's reference and patient's insurance policy. This application keeps track of all the medicines user has consumed in the past and generates a patient history. A citizen is given a drug only after the doctor's reference so the doctor's information is also linked with the patient's history.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Involved in writing test cases based on test scenarios and execute them.
â¢ Ensuring Test Coverage using Requirement Traceability Matrix (RTM) â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - Testing Trends WQR (World Quality Report) is an application which allows the users to take a survey on different methods and technologies used for testing. Users can choose to answer any type of questions under three different categories. Users have a facility to search, view and export the data to excel. Also, users get daily and weekly reports through email about the new trends in testing implemented around the globe. Testing Trends WQR app is available on Android and IOS platforms.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Writing test cases based on test scenarios and executed them.
â¢ Performing different types of testing such as Functional, Integration, System, and UAT.
â¢ Defect resolution and maintenance of the application."
Business Analyst,"TECHNOLOGICAL SKILLS â¦ Knowledge of Computers on the Windows platform. â¦ Fluency in MS-Office Applications such as Excel, Word, PowerPoint, etc. â¦ HTML, JAVA, PHP ATTRIBUTES â¦ Hardworking towards achieving the Goal â¦ Good communication skills â¦ Quick learner â¦ Good interpersonal relationEducation Details 
January 2016 to January 2018 MMS  Mumbai, Maharashtra University of Mumbai
January 2016 to January 2018 MMS Management Mumbai, Maharashtra University of Mumbai
January 2014 B.Sc.  Bandra, MAHARASHTRA, IN Rivzi College
January 2011 HSC  Bandra, Maharashtra, IN St. Andrews College
January 2011 HSC   Allana Junior College
January 2009 SSC   Canossa High School
January 2008 SSC   Maharashtra State Board
Business Analyst 

Business Analyst - Mass Group of Companies
Skill Details 
EXCEL- Exprience - 23 months
HTML- Exprience - 6 months
JAVA- Exprience - 6 months
PHP- Exprience - 6 months
POWERPOINT- Exprience - 6 monthsCompany Details 
company - Mass Group of Companies
description - Key Role   Analyst
Responsibilities:
â¦ Manage risk and provide risk management process.
â¦ Liaise with other project areas to coordinate with interdependencies and resolve issues.
â¦ Analyse and map business process.
â¦ Guide stakeholders on devising effective and efficient approaches to achieving project objectives.
â¦ Preparation of various Derivable i.e Business requirement, functional requirement and report specification.
company - Commversion Pvt.LTD
description - Responsibilities:
â¦ Employee Satisfaction reports
â¦ Evaluate overall work
â¦ Internal Audit
â¦ Maintain timesheet validation.
â¦ Set Simple and performance driven compensation strategies and polices."
Business Analyst,"Education Details 
February 2006 to February 2006 TYBCOM Commerce  mumbai
Business Analyst 

Business Analyst
Skill Details 
Company Details 
company - Motilal Oswal
description - Business Analyst
Handling IT Operation for Institutional Equities 
Maintain Daily MIS in Excel for CAG, Research, Derivative, Sales team Preparing Auto Dashboard For Research, Sales, Trading team Working on Excel Macro to Create Innovative Report 
Working on Block Related Data Working on BD Fund from different GEO Working on Investors Corporate Meeting to track Corporate Block & Fund Interest in Sector
company - FSS
description - Project Description:
Maintain and prepare cash indent, cash report, cash position, and cash planning
Responsibilities:

â¢ Maintain Daily MIS in excel.
â¢ Provide complete information about MIS & ATM.
â¢ Maintain and prepare cash indent, cash report, cash position., cash planning
â¢ Co-ordinate with BANK CASH DEPARTMENT.
â¢ Co-ordinate with custodians

.
DEGREE/ COURSE          YEAR of PASSING   GRADE          INSTITUTE                              UNIVERSITY/ BOARD   PERCENTAGE
company - ANGEL BROKING
description - Reporting: Assistant Manager /   SR. Manger
Responsibilities:

â¢ Handling team of 14 Quality assurance team members
â¢ Maintain Daily MIS in excel of team productivity
â¢ Maintain and prepare repots, adding comments on remark
â¢ mailing client  for modification of given number
â¢ Mailing reports to different branches
â¢ Coordinating with RM
â¢ Provide complete information about script to client"
Business Analyst,"Key Skills - Requirement Gathering - Requirement Analysis -Design Specifications - Client Communication - System Documentation - Problem solving - SDLC Operating Systems: Windows OS, UNIX (Linux/Ubuntu) Languages: Java, C++ Web Languages: JavaScript, HTML Tools: Citrix Software, System Architect, Quality Center v9.0 & v10.0, Tortoise SVN, DOORS, Artifact Viewer, JformDesigner, JIRA, Microsoft D365 Other Skills: Microsoft Office, MS Excel, MS PowerPoint, MS Visio, AutoCAD, VLSI, MS-CIT Certified. Education Details 
January 2012 BE Electronics Mumbai, Maharashtra Mumbai University
January 2006    Maharashtra State Board
Business Analyst 

Business Analyst - Intertek India Pvt Ltd
Skill Details 
SDLC- Exprience - 75 months
VISIO- Exprience - 60 months
REQUIREMENT GATHERING- Exprience - 15 months
Documentation- Exprience - Less than 1 year months
Functional Testing- Exprience - Less than 1 year months
Business Analysis- Exprience - Less than 1 year months
Jira- Exprience - Less than 1 year monthsCompany Details 
company - Intertek India Pvt Ltd
description - Business Analyst. Key responsibilities include Requirements Gathering, Requirements Analysis. Documentation like FRD creation. Providing KT sessions to the team. Having Client Communication. Gap Analysis.
company - Intertek India Pvt Ltd
description - Requirement Gathering from Businesses. Creating FRDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Walkthrough to team regarding the requirement â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Creating UAT Test cases. Executing the same.
â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	Inlight (Feb 2018 till date)
Platform	.Net

Description:
Inlight - (Supplier Risk Assessment Application)
Inlight is an Application designed to assess the Suppliers within the Supply chain. The Application on boards the Importers, Exporters and Suppliers. Based on the role they perform a Questionnaire is assigned to them and they fill out the Questionnaire. Basis the answer a scoring methodology is defined and the Suppliers are assessed to be Critical, High, Medium and Low. This helps in assessing the risk involved in working with certain Suppliers in the Supply chain.

Beyond Curriculum â Completed Internship in L&T â Attended Logistics Business School Training in Germany.
â A1 Certified in German Language.
â Travelled Onsite for Business Meetings and Discussions with Clients.

Personal Dossier .
company - AllCargo India Pvt Ltd
description - FRD creation
Client communication
Vendor Management
Having product Walk through with the team.
company - AllCargo India Pvt Ltd
description - Requirement Gathering from Businesses. Creating BRDs and FSDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	CRM (Nov 2017 to Feb 2018)
Platform	Microsoft D365

Description:
CRM - (Sales Management System)
CRM is a Software solution specially designed for handling Sales Management. This is a product provided by Microsoft which helps in tracking the sales of company, the activities of the salesperson, 360-degree view of customer accounts. This basically helps to get the overall status and view of various businesses the company is achieving from different Customers. A platform where the salesperson provides the details of Lead, Opportunity, Accounts and Businesses. Available on Cloud.

Project 2	Credit Risk (Nov 2017 to Feb 2018)
Platform	.Net

Description:
Credit Risk - (Customer credit check Management System)
Credit Risk is a Software solution specially designed for checking the credit status of the customer from which businesses are gained. The software basically is designed to take the KYC and the consent from the customer. For those customers who provide the consent, the credit report and monitoring report are obtained from the Credit Bureau. Based on the reports the customer health can be determined and business with them can either be  or discontinued.

Work Experience 3:
company - Capgemini India Pvt Ltd
description - Client: DB Schenker â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Impart business process knowledge transfer to the team members. Prepare business / functional process workflow using Visio, UML etc â Working knowledge of OOAD - Object Oriented Analysis & Design concept.
â Helping the Junior BAs in their work. Supervising their work.
â Tools & Applications: System Architect, DOORS, UML designs & concepts, HP Quality Center, MWB, Jformdesigner â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
company - Capgemini India Pvt Ltd
description - Platform	Java

Description:
TANGO - (Sea & Air Cargo Management System)
TANGO is a Software solution specially designed for handling sea and air cargo (Import & Export) Management. TANGO manages the creation of Shipment, Tracking the shipment via multiple service legs i.e. Pick-up, Delivery leg etc. It helps in managing the end to end shipment with respect to the entire department involvement (globally)

Work Experience 2:
company - Capgemini India Pvt Ltd
description - "
Business Analyst,"IT Skills: Area Exposure Modeling Tool: Bizagi, MS Visio Prototyping Tool: Indigo Studio. Documentation: MS Office (MS Word, MS Excel, MS Power Point) Testing Proficiency: Smoke, Sanity, Integration, Functional, Acceptance and UI Methodology implemented: Waterfall, Agile (Scrum) Database: SQL Testing Tool: HPQC Business Exposure Education Details 
 Bachelor Of Computer Engineering Computer Engineering Mumbai, Maharashtra Thadomal Shahani Engineering college
 Diploma Computer Engineering Ulhasnagar, Maharashtra Institute of Technology
 Secondary School Certificate  Ulhasnagar, Maharashtra New English High School
Senior Business Analyst - RPA 

Senior Business Analyst - RPA - Hexaware Technologies
Skill Details 
DOCUMENTATION- Exprience - 47 months
TESTING- Exprience - 29 months
INTEGRATION- Exprience - 25 months
INTEGRATOR- Exprience - 25 months
PROTOTYPE- Exprience - 13 monthsCompany Details 
company - Hexaware Technologies
description - Working as a RPA Business Analyst
company - BBH- Brown Brothers Harriman & Co
description - is a private bank that provides commercial banking, investment management, brokerage, and trust services to private companies and individuals. It also performs merger advisory, foreign exchange, custody services, commercial banking, and corporate financing services.

Responsibilities: â¢ Performed Automation Assessment of various Processes and identified processes which can be candidates of RPA.
â¢ Conducting Assessment that involves an initial Understanding of the Existing System, their technology, processes, Usage of the tools, Feasibility of tool with automation tool along with automation ROI analysis.
â¢ Preparing the Automation Potential Sheet which describes the steps in the process, the volume and frequency of the transaction, the AHT taken by SME to perform the process and depending on the steps that could be automated, Automation potential and the manual efforts that will be saved are calculated.
Calculating the complexity of the Process which is considered for automation and depending on all these factors Number of Bots and Number of Automation tool Licenses are determined.
â¢ Implementing a Proof of Concept (POC) to Validate Feasibility by executing the selected critical use cases for conducting a POC which will helps to identify financial and operational benefits and provide recommendations regarding the actual need for complete automation.
â¢ Gathering business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME's) â¢ Preparing Business Requirement Document and then converted Business requirements into Functional Requirements Specification.
 â¢ Constructing prototype early toward a design acceptable to the customer and feasible.
â¢ Assisting in designing test plans, test scenarios and test cases for integration, regression, and user acceptance testing (UAT) to improve the overall quality of the Automation.
â¢ Participating regularly in Walkthroughs and Review meetings with Project Manager, QA Engineers, and Development team.
â¢ Regularly interacting with offshore and onshore development teams.
company - FADV - First Advantage
description - is a criminal background check company that delivers global solutions ranging from employment screenings to background checks.
The following are the processes which were covered:
Email Process, Research Process, Review Process.

Responsibilities: â¢ Requirement Gathering through conducting Interviews & Brainstorming sessions with stakeholders â¢ To develop decision models and execute those rules as per the use case specifications.
â¢ To Test/validate the decision models against document test data.
â¢ To maintain and enhance the decision models for changes in regulations as per use case specifications.
â¢ Responsible for performing the business research that will make a business growth.
â¢ Developing a clear understanding of existing business functions and processes.
â¢ Effectively communicate with the onsite clients for the queries, suggestions, and update.
â¢ Giving suggestions to enhance the current processes.
â¢ Identifying areas for process improvement.
â¢ Flagging up potential problems at an early stage.
â¢ Preparing PowerPoint presentations and documents for business meetings.
â¢ Using any information gathered to write up detailed reports.
â¢ Highlighting risks and issues that could impact project delivery.
â¢ Able to work accurately.
â¢ To develop and maintain documentation for internal team training and client end user operations.
â¢ To work efficiently with team members and across teams.
â¢ To mentor and train junior team members.
company - Clinical Testing, Lab Work and Diagnostic Testing
description - IQVIA provides services to its customers this includes: Clinical Testing, Lab Work and Diagnostic Testing under clinical trial. These customers need to pay to IQVIA and aging details and invoices are generated for the same.
The following are the processes which were covered:

Tracking Payments, Automated Real Time Metrics Reporting (Dashboard), Past Due Notifications, AR Statements, Credit/Rebill.
Responsibilities: â¢ Conducting meetings with clients and key stakeholders to gather requirements, analyze, finalize and have formal sign-offs from approvers Gather and perform analysis of the business requirements â¢ Translating the business requirements into the Business Requirement Document [BRD], Functional Requirement Document [FRD].
â¢ Facilitating meetings with the appropriate subject matter experts in both business and technology teams â¢ Coordinating with business user community for the execution of user acceptance test as well as tracking issues â¢ Working, collaborating and coordinating with Offshore and Onsite team members to fulfill the BA responsibilities from project initiation to Post-Implementation â¢ Reviewing the test scripts with business users as well as technology team. Execute test scripts with expected results for the System Integration Test (SIT) and User Acceptance Test (UAT) â¢ Coordinating and conducting the Production Acceptance Testing (PAT) with the business users â¢ Creating flow diagrams, structure charts, and other types of system or process representations â¢ Managing changes to requirements and baseline through a change control process â¢ Utilizing standard methods, design and testing tools throughout project development life cycle â¢ Work closely with the operational functional teams, operations management, and personnel, and various technology teams to facilitate a shared understanding of requirements and priorities across all areas
company - Eduavenir IT Solution
description - Project: M.B.M.S

M.B.M.S. - is an Inventory management application that allows user to manage inventory details of different warehouses, having different products located at various locations and help extract what goods have been procured, sold or returned by customers. It generates automated invoicesalong withcustomized reports. It also managescustomer complaint and resolution system implementation along with automated MIS on monthly basis.Sales and forecastingis also developed on MIS System and the streamlining of process of warehousing and dispatch along with online proof of delivery management system (POD documentation) is generated.

Responsibilities: â¢ Participate in requirement gathering discussion with client to understand the flow of business processes â¢ Analyze the requirements and determine the core processes, develop Process Documentation and ensure to stay up-to-date in conjunction with on-going changes â¢ Participate in process flow analysis and preparing BRD, SRS.
â¢ Coordinating with developers, designers & operations teams for various nuances of the project, communicate the stakeholder requirements from requirement /enhancement to implementation and finally deliver the same within estimated timeframe.
â¢ Support UAT by reviewing test cases, manage version control of documents, software builds.
â¢ Coordinate with the stakeholders for UAT sign off and coordinate internally for production movement till Golive stage of the application.
â¢ Provide demo and training to internal and end user using PowerPoint presentation.
â¢ Resolving project functional &technical issues during UAT.
â¢ Prioritizing the Production bugs and resolving the same within the estimated timeframe.
â¢ Preparing Project Status Report and Production Bugs Status to all the stakeholders.
â¢ Promoting and Networking for online trading platform.
â¢ Designing query sheet for obtaining and comparison of quotes from various vendors.
â¢ Development of product codes / material codes for inventory management (Master Data Management)
company - CAPGEMINI Head Office
description - Type: Mobile and Device Testing.       Duration: January 2014 - August 2014

Follet - An application which takes an electronic request from the user for the books he requires from a particular follet store. This detailed information about books that will include the name of the book, its price, the date of the transaction and the parties involved which will then be sent to follet stores. User then create request for one or more books for a given date. This request is then processed further and user gets a mail of the date when he will be provided with that book.

Responsibilities: â¢ Understanding the needs and business requirements.
â¢ Preparing BRD, SRS by eliciting all the requirements from the client and SMEs â¢ Understanding the dependency of the modules in the system â¢ Preparation of test plan for Unit level and Integration level.
â¢ Preparation and execution of test cases.
â¢ Defect tracking, Issue Resolution, Risk Monitoring, Status Tracking, Reporting and Follow-up.
â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - 
company - CAPGEMINI Head Office
description - Humana is a health care insurance project of U.S. which deals with supplying various medicines to citizens as per the doctor's reference and patient's insurance policy. This application keeps track of all the medicines user has consumed in the past and generates a patient history. A citizen is given a drug only after the doctor's reference so the doctor's information is also linked with the patient's history.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Involved in writing test cases based on test scenarios and execute them.
â¢ Ensuring Test Coverage using Requirement Traceability Matrix (RTM) â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - Testing Trends WQR (World Quality Report) is an application which allows the users to take a survey on different methods and technologies used for testing. Users can choose to answer any type of questions under three different categories. Users have a facility to search, view and export the data to excel. Also, users get daily and weekly reports through email about the new trends in testing implemented around the globe. Testing Trends WQR app is available on Android and IOS platforms.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Writing test cases based on test scenarios and executed them.
â¢ Performing different types of testing such as Functional, Integration, System, and UAT.
â¢ Defect resolution and maintenance of the application."
Business Analyst,"TECHNOLOGICAL SKILLS â¦ Knowledge of Computers on the Windows platform. â¦ Fluency in MS-Office Applications such as Excel, Word, PowerPoint, etc. â¦ HTML, JAVA, PHP ATTRIBUTES â¦ Hardworking towards achieving the Goal â¦ Good communication skills â¦ Quick learner â¦ Good interpersonal relationEducation Details 
January 2016 to January 2018 MMS  Mumbai, Maharashtra University of Mumbai
January 2016 to January 2018 MMS Management Mumbai, Maharashtra University of Mumbai
January 2014 B.Sc.  Bandra, MAHARASHTRA, IN Rivzi College
January 2011 HSC  Bandra, Maharashtra, IN St. Andrews College
January 2011 HSC   Allana Junior College
January 2009 SSC   Canossa High School
January 2008 SSC   Maharashtra State Board
Business Analyst 

Business Analyst - Mass Group of Companies
Skill Details 
EXCEL- Exprience - 23 months
HTML- Exprience - 6 months
JAVA- Exprience - 6 months
PHP- Exprience - 6 months
POWERPOINT- Exprience - 6 monthsCompany Details 
company - Mass Group of Companies
description - Key Role   Analyst
Responsibilities:
â¦ Manage risk and provide risk management process.
â¦ Liaise with other project areas to coordinate with interdependencies and resolve issues.
â¦ Analyse and map business process.
â¦ Guide stakeholders on devising effective and efficient approaches to achieving project objectives.
â¦ Preparation of various Derivable i.e Business requirement, functional requirement and report specification.
company - Commversion Pvt.LTD
description - Responsibilities:
â¦ Employee Satisfaction reports
â¦ Evaluate overall work
â¦ Internal Audit
â¦ Maintain timesheet validation.
â¦ Set Simple and performance driven compensation strategies and polices."
Business Analyst,"Education Details 
February 2006 to February 2006 TYBCOM Commerce  mumbai
Business Analyst 

Business Analyst
Skill Details 
Company Details 
company - Motilal Oswal
description - Business Analyst
Handling IT Operation for Institutional Equities 
Maintain Daily MIS in Excel for CAG, Research, Derivative, Sales team Preparing Auto Dashboard For Research, Sales, Trading team Working on Excel Macro to Create Innovative Report 
Working on Block Related Data Working on BD Fund from different GEO Working on Investors Corporate Meeting to track Corporate Block & Fund Interest in Sector
company - FSS
description - Project Description:
Maintain and prepare cash indent, cash report, cash position, and cash planning
Responsibilities:

â¢ Maintain Daily MIS in excel.
â¢ Provide complete information about MIS & ATM.
â¢ Maintain and prepare cash indent, cash report, cash position., cash planning
â¢ Co-ordinate with BANK CASH DEPARTMENT.
â¢ Co-ordinate with custodians

.
DEGREE/ COURSE          YEAR of PASSING   GRADE          INSTITUTE                              UNIVERSITY/ BOARD   PERCENTAGE
company - ANGEL BROKING
description - Reporting: Assistant Manager /   SR. Manger
Responsibilities:

â¢ Handling team of 14 Quality assurance team members
â¢ Maintain Daily MIS in excel of team productivity
â¢ Maintain and prepare repots, adding comments on remark
â¢ mailing client  for modification of given number
â¢ Mailing reports to different branches
â¢ Coordinating with RM
â¢ Provide complete information about script to client"
Business Analyst,"Education Details 
 BE Computer Science Mumbai, Maharashtra Mumbai University
 HSC  Mumbai, Maharashtra Maharashtra State Board
 SSC  Mumbai, Maharashtra Maharashtra State Board
Business Analyst 

Business Analyst - Fino Payments Bank
Skill Details 
Company Details 
company - Fino Payments Bank
description - Key Role   In-depth requirement and input gathering
Responsibilities and Achievements:
â¦ Conducted in-depth requirement and input gathering from all concerned stakeholders [Business SMEs, Technical Architect and Business Architect] to create artifacts like Business Requirement Document (BRD) to arrive at functional requirements for development team
â¦ Created Functional Specification Document (FSD) highlighting the technical implementation and the use cases
â¦ Led the Merchant Commission Module project from end-to-end and co-ordinated for CUG and Live
â¦ Designed the Account Opening Process flow end-to-end during the time when bank was going Live.
â¦ SPOC for all the configurations (both account level and customer level) in production.
â¦ Led the Cash Controlling Processes for the field users as per the requirement from the business team.
â¦ Design and build proof of concepts to validate the viability of alternate approaches and determine optimum choice
â¦ Involved in Process Design for development of the products
â¦ Performed Functional Testing of the entire system and provided support during UAT by preparing UAT test cases, performing UAT tests to onboard new processes as BAU
â¦ Worked with the development teams in arriving at detailed techno-functional specifications, participate in Feasibility
Analysis
â¦ Conducting twice a week meetings with the vendor to discuss the status of CRs and to resolve technical queries
company - Fino Paytech Pvt. Ltd
description - Key Role   Requirement gathering, Development, Testing
Responsibilities and Achievements:
â¦ Requirement gathering, preparation of traceability matrix, preparation and execution of use cases, developing of test plans based on requirements for Airtel Zambia National Partner Project
â¦ Led the employee profile creation, maintenance of employee details in the database: Preparation of work flow, end-to-end development and testing of the module
â¦ Designed the work flow process of the CAPA (Corrective Action Preventive Analysis) module to maintain the audit findings raised by the internal audit team
â¦ Designed the Expense Management module and automated it for end-to-end in-house expense flow
â¦ Designed the PMO tool Parivartan used for tracking the projects end-to-end"
Business Analyst,"Technical Skills Application Servers: IIS 6.0, Jboss 7.1. Database: SQL, Oracle and DB2. Report Tool: iReport, Crystal report. Career GraphEducation Details 

Business Analyst 

Business Analyst - Zensar Technologies Ltd
Skill Details 
CRYSTAL REPORT- Exprience - 15 months
DATABASE- Exprience - 6 months
DB2- Exprience - 6 months
IIS- Exprience - 6 months
IIS 6- Exprience - 6 monthsCompany Details 
company - Zensar Technologies Ltd
description - Location: Goregoan, Mumbai (Client -SUN Pharmaceutical)
Designation: Business Analyst.
Role: Requirement gathering, gap analysis, support, end user training, documentation.
company - Proteus Technologies Pvt Ltd
description - Base Information Management Pvt. Ltd. Is a Mumbai base software service provider with core competency and proven track record of installations of Enterprise Wide Solutions. Base customers come from industries as wide as Pharmaceuticals, Life Sciences, Plastics, Engineering, Chemicals
company - Wings Infonet Pvt Ltd
description - It is IT solutions Provider Company. Company provides comprehensive technology solutions to Accounting, trade, payroll and Asset Management firms across the world.
company - Hiral Tektronix Pvt Ltd
description - Software relates to recruitment (HRMS), accounting, and Payroll.

Job Responsibilities: â¢ ERP Implementation and after go live support.
â¢ Documenting user requirements and developing specifications for customization.
â¢ Integrating with other modules, integration testing & extending Post Go-live support, including training support to end-users.
â¢ Drafting functional requirements for ERP systems to represent the processes and functions involved.
â¢ Guiding the users in using various modules and the management for various functional issues.
â¢ Delivering awareness about various reports available in ERP system for day to day transactions and for MIS reporting of departments
System Audit for better results and follow ups for Observation.
â¢ Developing and designing report using iReport and crystal report"
Business Analyst,"Key Skills - Requirement Gathering - Requirement Analysis -Design Specifications - Client Communication - System Documentation - Problem solving - SDLC Operating Systems: Windows OS, UNIX (Linux/Ubuntu) Languages: Java, C++ Web Languages: JavaScript, HTML Tools: Citrix Software, System Architect, Quality Center v9.0 & v10.0, Tortoise SVN, DOORS, Artifact Viewer, JformDesigner, JIRA, Microsoft D365 Other Skills: Microsoft Office, MS Excel, MS PowerPoint, MS Visio, AutoCAD, VLSI, MS-CIT Certified. Education Details 
January 2012 BE Electronics Mumbai, Maharashtra Mumbai University
January 2006    Maharashtra State Board
Business Analyst 

Business Analyst - Intertek India Pvt Ltd
Skill Details 
SDLC- Exprience - 75 months
VISIO- Exprience - 60 months
REQUIREMENT GATHERING- Exprience - 15 months
Documentation- Exprience - Less than 1 year months
Functional Testing- Exprience - Less than 1 year months
Business Analysis- Exprience - Less than 1 year months
Jira- Exprience - Less than 1 year monthsCompany Details 
company - Intertek India Pvt Ltd
description - Business Analyst. Key responsibilities include Requirements Gathering, Requirements Analysis. Documentation like FRD creation. Providing KT sessions to the team. Having Client Communication. Gap Analysis.
company - Intertek India Pvt Ltd
description - Requirement Gathering from Businesses. Creating FRDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Walkthrough to team regarding the requirement â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Creating UAT Test cases. Executing the same.
â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	Inlight (Feb 2018 till date)
Platform	.Net

Description:
Inlight - (Supplier Risk Assessment Application)
Inlight is an Application designed to assess the Suppliers within the Supply chain. The Application on boards the Importers, Exporters and Suppliers. Based on the role they perform a Questionnaire is assigned to them and they fill out the Questionnaire. Basis the answer a scoring methodology is defined and the Suppliers are assessed to be Critical, High, Medium and Low. This helps in assessing the risk involved in working with certain Suppliers in the Supply chain.

Beyond Curriculum â Completed Internship in L&T â Attended Logistics Business School Training in Germany.
â A1 Certified in German Language.
â Travelled Onsite for Business Meetings and Discussions with Clients.

Personal Dossier .
company - AllCargo India Pvt Ltd
description - FRD creation
Client communication
Vendor Management
Having product Walk through with the team.
company - AllCargo India Pvt Ltd
description - Requirement Gathering from Businesses. Creating BRDs and FSDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	CRM (Nov 2017 to Feb 2018)
Platform	Microsoft D365

Description:
CRM - (Sales Management System)
CRM is a Software solution specially designed for handling Sales Management. This is a product provided by Microsoft which helps in tracking the sales of company, the activities of the salesperson, 360-degree view of customer accounts. This basically helps to get the overall status and view of various businesses the company is achieving from different Customers. A platform where the salesperson provides the details of Lead, Opportunity, Accounts and Businesses. Available on Cloud.

Project 2	Credit Risk (Nov 2017 to Feb 2018)
Platform	.Net

Description:
Credit Risk - (Customer credit check Management System)
Credit Risk is a Software solution specially designed for checking the credit status of the customer from which businesses are gained. The software basically is designed to take the KYC and the consent from the customer. For those customers who provide the consent, the credit report and monitoring report are obtained from the Credit Bureau. Based on the reports the customer health can be determined and business with them can either be  or discontinued.

Work Experience 3:
company - Capgemini India Pvt Ltd
description - Client: DB Schenker â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Impart business process knowledge transfer to the team members. Prepare business / functional process workflow using Visio, UML etc â Working knowledge of OOAD - Object Oriented Analysis & Design concept.
â Helping the Junior BAs in their work. Supervising their work.
â Tools & Applications: System Architect, DOORS, UML designs & concepts, HP Quality Center, MWB, Jformdesigner â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
company - Capgemini India Pvt Ltd
description - Platform	Java

Description:
TANGO - (Sea & Air Cargo Management System)
TANGO is a Software solution specially designed for handling sea and air cargo (Import & Export) Management. TANGO manages the creation of Shipment, Tracking the shipment via multiple service legs i.e. Pick-up, Delivery leg etc. It helps in managing the end to end shipment with respect to the entire department involvement (globally)

Work Experience 2:
company - Capgemini India Pvt Ltd
description - "
Business Analyst,"IT Skills: Area Exposure Modeling Tool: Bizagi, MS Visio Prototyping Tool: Indigo Studio. Documentation: MS Office (MS Word, MS Excel, MS Power Point) Testing Proficiency: Smoke, Sanity, Integration, Functional, Acceptance and UI Methodology implemented: Waterfall, Agile (Scrum) Database: SQL Testing Tool: HPQC Business Exposure Education Details 
 Bachelor Of Computer Engineering Computer Engineering Mumbai, Maharashtra Thadomal Shahani Engineering college
 Diploma Computer Engineering Ulhasnagar, Maharashtra Institute of Technology
 Secondary School Certificate  Ulhasnagar, Maharashtra New English High School
Senior Business Analyst - RPA 

Senior Business Analyst - RPA - Hexaware Technologies
Skill Details 
DOCUMENTATION- Exprience - 47 months
TESTING- Exprience - 29 months
INTEGRATION- Exprience - 25 months
INTEGRATOR- Exprience - 25 months
PROTOTYPE- Exprience - 13 monthsCompany Details 
company - Hexaware Technologies
description - Working as a RPA Business Analyst
company - BBH- Brown Brothers Harriman & Co
description - is a private bank that provides commercial banking, investment management, brokerage, and trust services to private companies and individuals. It also performs merger advisory, foreign exchange, custody services, commercial banking, and corporate financing services.

Responsibilities: â¢ Performed Automation Assessment of various Processes and identified processes which can be candidates of RPA.
â¢ Conducting Assessment that involves an initial Understanding of the Existing System, their technology, processes, Usage of the tools, Feasibility of tool with automation tool along with automation ROI analysis.
â¢ Preparing the Automation Potential Sheet which describes the steps in the process, the volume and frequency of the transaction, the AHT taken by SME to perform the process and depending on the steps that could be automated, Automation potential and the manual efforts that will be saved are calculated.
Calculating the complexity of the Process which is considered for automation and depending on all these factors Number of Bots and Number of Automation tool Licenses are determined.
â¢ Implementing a Proof of Concept (POC) to Validate Feasibility by executing the selected critical use cases for conducting a POC which will helps to identify financial and operational benefits and provide recommendations regarding the actual need for complete automation.
â¢ Gathering business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME's) â¢ Preparing Business Requirement Document and then converted Business requirements into Functional Requirements Specification.
 â¢ Constructing prototype early toward a design acceptable to the customer and feasible.
â¢ Assisting in designing test plans, test scenarios and test cases for integration, regression, and user acceptance testing (UAT) to improve the overall quality of the Automation.
â¢ Participating regularly in Walkthroughs and Review meetings with Project Manager, QA Engineers, and Development team.
â¢ Regularly interacting with offshore and onshore development teams.
company - FADV - First Advantage
description - is a criminal background check company that delivers global solutions ranging from employment screenings to background checks.
The following are the processes which were covered:
Email Process, Research Process, Review Process.

Responsibilities: â¢ Requirement Gathering through conducting Interviews & Brainstorming sessions with stakeholders â¢ To develop decision models and execute those rules as per the use case specifications.
â¢ To Test/validate the decision models against document test data.
â¢ To maintain and enhance the decision models for changes in regulations as per use case specifications.
â¢ Responsible for performing the business research that will make a business growth.
â¢ Developing a clear understanding of existing business functions and processes.
â¢ Effectively communicate with the onsite clients for the queries, suggestions, and update.
â¢ Giving suggestions to enhance the current processes.
â¢ Identifying areas for process improvement.
â¢ Flagging up potential problems at an early stage.
â¢ Preparing PowerPoint presentations and documents for business meetings.
â¢ Using any information gathered to write up detailed reports.
â¢ Highlighting risks and issues that could impact project delivery.
â¢ Able to work accurately.
â¢ To develop and maintain documentation for internal team training and client end user operations.
â¢ To work efficiently with team members and across teams.
â¢ To mentor and train junior team members.
company - Clinical Testing, Lab Work and Diagnostic Testing
description - IQVIA provides services to its customers this includes: Clinical Testing, Lab Work and Diagnostic Testing under clinical trial. These customers need to pay to IQVIA and aging details and invoices are generated for the same.
The following are the processes which were covered:

Tracking Payments, Automated Real Time Metrics Reporting (Dashboard), Past Due Notifications, AR Statements, Credit/Rebill.
Responsibilities: â¢ Conducting meetings with clients and key stakeholders to gather requirements, analyze, finalize and have formal sign-offs from approvers Gather and perform analysis of the business requirements â¢ Translating the business requirements into the Business Requirement Document [BRD], Functional Requirement Document [FRD].
â¢ Facilitating meetings with the appropriate subject matter experts in both business and technology teams â¢ Coordinating with business user community for the execution of user acceptance test as well as tracking issues â¢ Working, collaborating and coordinating with Offshore and Onsite team members to fulfill the BA responsibilities from project initiation to Post-Implementation â¢ Reviewing the test scripts with business users as well as technology team. Execute test scripts with expected results for the System Integration Test (SIT) and User Acceptance Test (UAT) â¢ Coordinating and conducting the Production Acceptance Testing (PAT) with the business users â¢ Creating flow diagrams, structure charts, and other types of system or process representations â¢ Managing changes to requirements and baseline through a change control process â¢ Utilizing standard methods, design and testing tools throughout project development life cycle â¢ Work closely with the operational functional teams, operations management, and personnel, and various technology teams to facilitate a shared understanding of requirements and priorities across all areas
company - Eduavenir IT Solution
description - Project: M.B.M.S

M.B.M.S. - is an Inventory management application that allows user to manage inventory details of different warehouses, having different products located at various locations and help extract what goods have been procured, sold or returned by customers. It generates automated invoicesalong withcustomized reports. It also managescustomer complaint and resolution system implementation along with automated MIS on monthly basis.Sales and forecastingis also developed on MIS System and the streamlining of process of warehousing and dispatch along with online proof of delivery management system (POD documentation) is generated.

Responsibilities: â¢ Participate in requirement gathering discussion with client to understand the flow of business processes â¢ Analyze the requirements and determine the core processes, develop Process Documentation and ensure to stay up-to-date in conjunction with on-going changes â¢ Participate in process flow analysis and preparing BRD, SRS.
â¢ Coordinating with developers, designers & operations teams for various nuances of the project, communicate the stakeholder requirements from requirement /enhancement to implementation and finally deliver the same within estimated timeframe.
â¢ Support UAT by reviewing test cases, manage version control of documents, software builds.
â¢ Coordinate with the stakeholders for UAT sign off and coordinate internally for production movement till Golive stage of the application.
â¢ Provide demo and training to internal and end user using PowerPoint presentation.
â¢ Resolving project functional &technical issues during UAT.
â¢ Prioritizing the Production bugs and resolving the same within the estimated timeframe.
â¢ Preparing Project Status Report and Production Bugs Status to all the stakeholders.
â¢ Promoting and Networking for online trading platform.
â¢ Designing query sheet for obtaining and comparison of quotes from various vendors.
â¢ Development of product codes / material codes for inventory management (Master Data Management)
company - CAPGEMINI Head Office
description - Type: Mobile and Device Testing.       Duration: January 2014 - August 2014

Follet - An application which takes an electronic request from the user for the books he requires from a particular follet store. This detailed information about books that will include the name of the book, its price, the date of the transaction and the parties involved which will then be sent to follet stores. User then create request for one or more books for a given date. This request is then processed further and user gets a mail of the date when he will be provided with that book.

Responsibilities: â¢ Understanding the needs and business requirements.
â¢ Preparing BRD, SRS by eliciting all the requirements from the client and SMEs â¢ Understanding the dependency of the modules in the system â¢ Preparation of test plan for Unit level and Integration level.
â¢ Preparation and execution of test cases.
â¢ Defect tracking, Issue Resolution, Risk Monitoring, Status Tracking, Reporting and Follow-up.
â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - 
company - CAPGEMINI Head Office
description - Humana is a health care insurance project of U.S. which deals with supplying various medicines to citizens as per the doctor's reference and patient's insurance policy. This application keeps track of all the medicines user has consumed in the past and generates a patient history. A citizen is given a drug only after the doctor's reference so the doctor's information is also linked with the patient's history.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Involved in writing test cases based on test scenarios and execute them.
â¢ Ensuring Test Coverage using Requirement Traceability Matrix (RTM) â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - Testing Trends WQR (World Quality Report) is an application which allows the users to take a survey on different methods and technologies used for testing. Users can choose to answer any type of questions under three different categories. Users have a facility to search, view and export the data to excel. Also, users get daily and weekly reports through email about the new trends in testing implemented around the globe. Testing Trends WQR app is available on Android and IOS platforms.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Writing test cases based on test scenarios and executed them.
â¢ Performing different types of testing such as Functional, Integration, System, and UAT.
â¢ Defect resolution and maintenance of the application."
Business Analyst,"TECHNOLOGICAL SKILLS â¦ Knowledge of Computers on the Windows platform. â¦ Fluency in MS-Office Applications such as Excel, Word, PowerPoint, etc. â¦ HTML, JAVA, PHP ATTRIBUTES â¦ Hardworking towards achieving the Goal â¦ Good communication skills â¦ Quick learner â¦ Good interpersonal relationEducation Details 
January 2016 to January 2018 MMS  Mumbai, Maharashtra University of Mumbai
January 2016 to January 2018 MMS Management Mumbai, Maharashtra University of Mumbai
January 2014 B.Sc.  Bandra, MAHARASHTRA, IN Rivzi College
January 2011 HSC  Bandra, Maharashtra, IN St. Andrews College
January 2011 HSC   Allana Junior College
January 2009 SSC   Canossa High School
January 2008 SSC   Maharashtra State Board
Business Analyst 

Business Analyst - Mass Group of Companies
Skill Details 
EXCEL- Exprience - 23 months
HTML- Exprience - 6 months
JAVA- Exprience - 6 months
PHP- Exprience - 6 months
POWERPOINT- Exprience - 6 monthsCompany Details 
company - Mass Group of Companies
description - Key Role   Analyst
Responsibilities:
â¦ Manage risk and provide risk management process.
â¦ Liaise with other project areas to coordinate with interdependencies and resolve issues.
â¦ Analyse and map business process.
â¦ Guide stakeholders on devising effective and efficient approaches to achieving project objectives.
â¦ Preparation of various Derivable i.e Business requirement, functional requirement and report specification.
company - Commversion Pvt.LTD
description - Responsibilities:
â¦ Employee Satisfaction reports
â¦ Evaluate overall work
â¦ Internal Audit
â¦ Maintain timesheet validation.
â¦ Set Simple and performance driven compensation strategies and polices."
Business Analyst,"Education Details 
February 2006 to February 2006 TYBCOM Commerce  mumbai
Business Analyst 

Business Analyst
Skill Details 
Company Details 
company - Motilal Oswal
description - Business Analyst
Handling IT Operation for Institutional Equities 
Maintain Daily MIS in Excel for CAG, Research, Derivative, Sales team Preparing Auto Dashboard For Research, Sales, Trading team Working on Excel Macro to Create Innovative Report 
Working on Block Related Data Working on BD Fund from different GEO Working on Investors Corporate Meeting to track Corporate Block & Fund Interest in Sector
company - FSS
description - Project Description:
Maintain and prepare cash indent, cash report, cash position, and cash planning
Responsibilities:

â¢ Maintain Daily MIS in excel.
â¢ Provide complete information about MIS & ATM.
â¢ Maintain and prepare cash indent, cash report, cash position., cash planning
â¢ Co-ordinate with BANK CASH DEPARTMENT.
â¢ Co-ordinate with custodians

.
DEGREE/ COURSE          YEAR of PASSING   GRADE          INSTITUTE                              UNIVERSITY/ BOARD   PERCENTAGE
company - ANGEL BROKING
description - Reporting: Assistant Manager /   SR. Manger
Responsibilities:

â¢ Handling team of 14 Quality assurance team members
â¢ Maintain Daily MIS in excel of team productivity
â¢ Maintain and prepare repots, adding comments on remark
â¢ mailing client  for modification of given number
â¢ Mailing reports to different branches
â¢ Coordinating with RM
â¢ Provide complete information about script to client"
Business Analyst,"Key Skills - Requirement Gathering - Requirement Analysis -Design Specifications - Client Communication - System Documentation - Problem solving - SDLC Operating Systems: Windows OS, UNIX (Linux/Ubuntu) Languages: Java, C++ Web Languages: JavaScript, HTML Tools: Citrix Software, System Architect, Quality Center v9.0 & v10.0, Tortoise SVN, DOORS, Artifact Viewer, JformDesigner, JIRA, Microsoft D365 Other Skills: Microsoft Office, MS Excel, MS PowerPoint, MS Visio, AutoCAD, VLSI, MS-CIT Certified. Education Details 
January 2012 BE Electronics Mumbai, Maharashtra Mumbai University
January 2006    Maharashtra State Board
Business Analyst 

Business Analyst - Intertek India Pvt Ltd
Skill Details 
SDLC- Exprience - 75 months
VISIO- Exprience - 60 months
REQUIREMENT GATHERING- Exprience - 15 months
Documentation- Exprience - Less than 1 year months
Functional Testing- Exprience - Less than 1 year months
Business Analysis- Exprience - Less than 1 year months
Jira- Exprience - Less than 1 year monthsCompany Details 
company - Intertek India Pvt Ltd
description - Business Analyst. Key responsibilities include Requirements Gathering, Requirements Analysis. Documentation like FRD creation. Providing KT sessions to the team. Having Client Communication. Gap Analysis.
company - Intertek India Pvt Ltd
description - Requirement Gathering from Businesses. Creating FRDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Walkthrough to team regarding the requirement â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Creating UAT Test cases. Executing the same.
â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	Inlight (Feb 2018 till date)
Platform	.Net

Description:
Inlight - (Supplier Risk Assessment Application)
Inlight is an Application designed to assess the Suppliers within the Supply chain. The Application on boards the Importers, Exporters and Suppliers. Based on the role they perform a Questionnaire is assigned to them and they fill out the Questionnaire. Basis the answer a scoring methodology is defined and the Suppliers are assessed to be Critical, High, Medium and Low. This helps in assessing the risk involved in working with certain Suppliers in the Supply chain.

Beyond Curriculum â Completed Internship in L&T â Attended Logistics Business School Training in Germany.
â A1 Certified in German Language.
â Travelled Onsite for Business Meetings and Discussions with Clients.

Personal Dossier .
company - AllCargo India Pvt Ltd
description - FRD creation
Client communication
Vendor Management
Having product Walk through with the team.
company - AllCargo India Pvt Ltd
description - Requirement Gathering from Businesses. Creating BRDs and FSDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	CRM (Nov 2017 to Feb 2018)
Platform	Microsoft D365

Description:
CRM - (Sales Management System)
CRM is a Software solution specially designed for handling Sales Management. This is a product provided by Microsoft which helps in tracking the sales of company, the activities of the salesperson, 360-degree view of customer accounts. This basically helps to get the overall status and view of various businesses the company is achieving from different Customers. A platform where the salesperson provides the details of Lead, Opportunity, Accounts and Businesses. Available on Cloud.

Project 2	Credit Risk (Nov 2017 to Feb 2018)
Platform	.Net

Description:
Credit Risk - (Customer credit check Management System)
Credit Risk is a Software solution specially designed for checking the credit status of the customer from which businesses are gained. The software basically is designed to take the KYC and the consent from the customer. For those customers who provide the consent, the credit report and monitoring report are obtained from the Credit Bureau. Based on the reports the customer health can be determined and business with them can either be  or discontinued.

Work Experience 3:
company - Capgemini India Pvt Ltd
description - Client: DB Schenker â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Impart business process knowledge transfer to the team members. Prepare business / functional process workflow using Visio, UML etc â Working knowledge of OOAD - Object Oriented Analysis & Design concept.
â Helping the Junior BAs in their work. Supervising their work.
â Tools & Applications: System Architect, DOORS, UML designs & concepts, HP Quality Center, MWB, Jformdesigner â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
company - Capgemini India Pvt Ltd
description - Platform	Java

Description:
TANGO - (Sea & Air Cargo Management System)
TANGO is a Software solution specially designed for handling sea and air cargo (Import & Export) Management. TANGO manages the creation of Shipment, Tracking the shipment via multiple service legs i.e. Pick-up, Delivery leg etc. It helps in managing the end to end shipment with respect to the entire department involvement (globally)

Work Experience 2:
company - Capgemini India Pvt Ltd
description - "
Business Analyst,"IT Skills: Area Exposure Modeling Tool: Bizagi, MS Visio Prototyping Tool: Indigo Studio. Documentation: MS Office (MS Word, MS Excel, MS Power Point) Testing Proficiency: Smoke, Sanity, Integration, Functional, Acceptance and UI Methodology implemented: Waterfall, Agile (Scrum) Database: SQL Testing Tool: HPQC Business Exposure Education Details 
 Bachelor Of Computer Engineering Computer Engineering Mumbai, Maharashtra Thadomal Shahani Engineering college
 Diploma Computer Engineering Ulhasnagar, Maharashtra Institute of Technology
 Secondary School Certificate  Ulhasnagar, Maharashtra New English High School
Senior Business Analyst - RPA 

Senior Business Analyst - RPA - Hexaware Technologies
Skill Details 
DOCUMENTATION- Exprience - 47 months
TESTING- Exprience - 29 months
INTEGRATION- Exprience - 25 months
INTEGRATOR- Exprience - 25 months
PROTOTYPE- Exprience - 13 monthsCompany Details 
company - Hexaware Technologies
description - Working as a RPA Business Analyst
company - BBH- Brown Brothers Harriman & Co
description - is a private bank that provides commercial banking, investment management, brokerage, and trust services to private companies and individuals. It also performs merger advisory, foreign exchange, custody services, commercial banking, and corporate financing services.

Responsibilities: â¢ Performed Automation Assessment of various Processes and identified processes which can be candidates of RPA.
â¢ Conducting Assessment that involves an initial Understanding of the Existing System, their technology, processes, Usage of the tools, Feasibility of tool with automation tool along with automation ROI analysis.
â¢ Preparing the Automation Potential Sheet which describes the steps in the process, the volume and frequency of the transaction, the AHT taken by SME to perform the process and depending on the steps that could be automated, Automation potential and the manual efforts that will be saved are calculated.
Calculating the complexity of the Process which is considered for automation and depending on all these factors Number of Bots and Number of Automation tool Licenses are determined.
â¢ Implementing a Proof of Concept (POC) to Validate Feasibility by executing the selected critical use cases for conducting a POC which will helps to identify financial and operational benefits and provide recommendations regarding the actual need for complete automation.
â¢ Gathering business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME's) â¢ Preparing Business Requirement Document and then converted Business requirements into Functional Requirements Specification.
 â¢ Constructing prototype early toward a design acceptable to the customer and feasible.
â¢ Assisting in designing test plans, test scenarios and test cases for integration, regression, and user acceptance testing (UAT) to improve the overall quality of the Automation.
â¢ Participating regularly in Walkthroughs and Review meetings with Project Manager, QA Engineers, and Development team.
â¢ Regularly interacting with offshore and onshore development teams.
company - FADV - First Advantage
description - is a criminal background check company that delivers global solutions ranging from employment screenings to background checks.
The following are the processes which were covered:
Email Process, Research Process, Review Process.

Responsibilities: â¢ Requirement Gathering through conducting Interviews & Brainstorming sessions with stakeholders â¢ To develop decision models and execute those rules as per the use case specifications.
â¢ To Test/validate the decision models against document test data.
â¢ To maintain and enhance the decision models for changes in regulations as per use case specifications.
â¢ Responsible for performing the business research that will make a business growth.
â¢ Developing a clear understanding of existing business functions and processes.
â¢ Effectively communicate with the onsite clients for the queries, suggestions, and update.
â¢ Giving suggestions to enhance the current processes.
â¢ Identifying areas for process improvement.
â¢ Flagging up potential problems at an early stage.
â¢ Preparing PowerPoint presentations and documents for business meetings.
â¢ Using any information gathered to write up detailed reports.
â¢ Highlighting risks and issues that could impact project delivery.
â¢ Able to work accurately.
â¢ To develop and maintain documentation for internal team training and client end user operations.
â¢ To work efficiently with team members and across teams.
â¢ To mentor and train junior team members.
company - Clinical Testing, Lab Work and Diagnostic Testing
description - IQVIA provides services to its customers this includes: Clinical Testing, Lab Work and Diagnostic Testing under clinical trial. These customers need to pay to IQVIA and aging details and invoices are generated for the same.
The following are the processes which were covered:

Tracking Payments, Automated Real Time Metrics Reporting (Dashboard), Past Due Notifications, AR Statements, Credit/Rebill.
Responsibilities: â¢ Conducting meetings with clients and key stakeholders to gather requirements, analyze, finalize and have formal sign-offs from approvers Gather and perform analysis of the business requirements â¢ Translating the business requirements into the Business Requirement Document [BRD], Functional Requirement Document [FRD].
â¢ Facilitating meetings with the appropriate subject matter experts in both business and technology teams â¢ Coordinating with business user community for the execution of user acceptance test as well as tracking issues â¢ Working, collaborating and coordinating with Offshore and Onsite team members to fulfill the BA responsibilities from project initiation to Post-Implementation â¢ Reviewing the test scripts with business users as well as technology team. Execute test scripts with expected results for the System Integration Test (SIT) and User Acceptance Test (UAT) â¢ Coordinating and conducting the Production Acceptance Testing (PAT) with the business users â¢ Creating flow diagrams, structure charts, and other types of system or process representations â¢ Managing changes to requirements and baseline through a change control process â¢ Utilizing standard methods, design and testing tools throughout project development life cycle â¢ Work closely with the operational functional teams, operations management, and personnel, and various technology teams to facilitate a shared understanding of requirements and priorities across all areas
company - Eduavenir IT Solution
description - Project: M.B.M.S

M.B.M.S. - is an Inventory management application that allows user to manage inventory details of different warehouses, having different products located at various locations and help extract what goods have been procured, sold or returned by customers. It generates automated invoicesalong withcustomized reports. It also managescustomer complaint and resolution system implementation along with automated MIS on monthly basis.Sales and forecastingis also developed on MIS System and the streamlining of process of warehousing and dispatch along with online proof of delivery management system (POD documentation) is generated.

Responsibilities: â¢ Participate in requirement gathering discussion with client to understand the flow of business processes â¢ Analyze the requirements and determine the core processes, develop Process Documentation and ensure to stay up-to-date in conjunction with on-going changes â¢ Participate in process flow analysis and preparing BRD, SRS.
â¢ Coordinating with developers, designers & operations teams for various nuances of the project, communicate the stakeholder requirements from requirement /enhancement to implementation and finally deliver the same within estimated timeframe.
â¢ Support UAT by reviewing test cases, manage version control of documents, software builds.
â¢ Coordinate with the stakeholders for UAT sign off and coordinate internally for production movement till Golive stage of the application.
â¢ Provide demo and training to internal and end user using PowerPoint presentation.
â¢ Resolving project functional &technical issues during UAT.
â¢ Prioritizing the Production bugs and resolving the same within the estimated timeframe.
â¢ Preparing Project Status Report and Production Bugs Status to all the stakeholders.
â¢ Promoting and Networking for online trading platform.
â¢ Designing query sheet for obtaining and comparison of quotes from various vendors.
â¢ Development of product codes / material codes for inventory management (Master Data Management)
company - CAPGEMINI Head Office
description - Type: Mobile and Device Testing.       Duration: January 2014 - August 2014

Follet - An application which takes an electronic request from the user for the books he requires from a particular follet store. This detailed information about books that will include the name of the book, its price, the date of the transaction and the parties involved which will then be sent to follet stores. User then create request for one or more books for a given date. This request is then processed further and user gets a mail of the date when he will be provided with that book.

Responsibilities: â¢ Understanding the needs and business requirements.
â¢ Preparing BRD, SRS by eliciting all the requirements from the client and SMEs â¢ Understanding the dependency of the modules in the system â¢ Preparation of test plan for Unit level and Integration level.
â¢ Preparation and execution of test cases.
â¢ Defect tracking, Issue Resolution, Risk Monitoring, Status Tracking, Reporting and Follow-up.
â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - 
company - CAPGEMINI Head Office
description - Humana is a health care insurance project of U.S. which deals with supplying various medicines to citizens as per the doctor's reference and patient's insurance policy. This application keeps track of all the medicines user has consumed in the past and generates a patient history. A citizen is given a drug only after the doctor's reference so the doctor's information is also linked with the patient's history.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Involved in writing test cases based on test scenarios and execute them.
â¢ Ensuring Test Coverage using Requirement Traceability Matrix (RTM) â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - Testing Trends WQR (World Quality Report) is an application which allows the users to take a survey on different methods and technologies used for testing. Users can choose to answer any type of questions under three different categories. Users have a facility to search, view and export the data to excel. Also, users get daily and weekly reports through email about the new trends in testing implemented around the globe. Testing Trends WQR app is available on Android and IOS platforms.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Writing test cases based on test scenarios and executed them.
â¢ Performing different types of testing such as Functional, Integration, System, and UAT.
â¢ Defect resolution and maintenance of the application."
Business Analyst,"TECHNOLOGICAL SKILLS â¦ Knowledge of Computers on the Windows platform. â¦ Fluency in MS-Office Applications such as Excel, Word, PowerPoint, etc. â¦ HTML, JAVA, PHP ATTRIBUTES â¦ Hardworking towards achieving the Goal â¦ Good communication skills â¦ Quick learner â¦ Good interpersonal relationEducation Details 
January 2016 to January 2018 MMS  Mumbai, Maharashtra University of Mumbai
January 2016 to January 2018 MMS Management Mumbai, Maharashtra University of Mumbai
January 2014 B.Sc.  Bandra, MAHARASHTRA, IN Rivzi College
January 2011 HSC  Bandra, Maharashtra, IN St. Andrews College
January 2011 HSC   Allana Junior College
January 2009 SSC   Canossa High School
January 2008 SSC   Maharashtra State Board
Business Analyst 

Business Analyst - Mass Group of Companies
Skill Details 
EXCEL- Exprience - 23 months
HTML- Exprience - 6 months
JAVA- Exprience - 6 months
PHP- Exprience - 6 months
POWERPOINT- Exprience - 6 monthsCompany Details 
company - Mass Group of Companies
description - Key Role   Analyst
Responsibilities:
â¦ Manage risk and provide risk management process.
â¦ Liaise with other project areas to coordinate with interdependencies and resolve issues.
â¦ Analyse and map business process.
â¦ Guide stakeholders on devising effective and efficient approaches to achieving project objectives.
â¦ Preparation of various Derivable i.e Business requirement, functional requirement and report specification.
company - Commversion Pvt.LTD
description - Responsibilities:
â¦ Employee Satisfaction reports
â¦ Evaluate overall work
â¦ Internal Audit
â¦ Maintain timesheet validation.
â¦ Set Simple and performance driven compensation strategies and polices."
Business Analyst,"Education Details 
February 2006 to February 2006 TYBCOM Commerce  mumbai
Business Analyst 

Business Analyst
Skill Details 
Company Details 
company - Motilal Oswal
description - Business Analyst
Handling IT Operation for Institutional Equities 
Maintain Daily MIS in Excel for CAG, Research, Derivative, Sales team Preparing Auto Dashboard For Research, Sales, Trading team Working on Excel Macro to Create Innovative Report 
Working on Block Related Data Working on BD Fund from different GEO Working on Investors Corporate Meeting to track Corporate Block & Fund Interest in Sector
company - FSS
description - Project Description:
Maintain and prepare cash indent, cash report, cash position, and cash planning
Responsibilities:

â¢ Maintain Daily MIS in excel.
â¢ Provide complete information about MIS & ATM.
â¢ Maintain and prepare cash indent, cash report, cash position., cash planning
â¢ Co-ordinate with BANK CASH DEPARTMENT.
â¢ Co-ordinate with custodians

.
DEGREE/ COURSE          YEAR of PASSING   GRADE          INSTITUTE                              UNIVERSITY/ BOARD   PERCENTAGE
company - ANGEL BROKING
description - Reporting: Assistant Manager /   SR. Manger
Responsibilities:

â¢ Handling team of 14 Quality assurance team members
â¢ Maintain Daily MIS in excel of team productivity
â¢ Maintain and prepare repots, adding comments on remark
â¢ mailing client  for modification of given number
â¢ Mailing reports to different branches
â¢ Coordinating with RM
â¢ Provide complete information about script to client"
Business Analyst,"Key Skills - Requirement Gathering - Requirement Analysis -Design Specifications - Client Communication - System Documentation - Problem solving - SDLC Operating Systems: Windows OS, UNIX (Linux/Ubuntu) Languages: Java, C++ Web Languages: JavaScript, HTML Tools: Citrix Software, System Architect, Quality Center v9.0 & v10.0, Tortoise SVN, DOORS, Artifact Viewer, JformDesigner, JIRA, Microsoft D365 Other Skills: Microsoft Office, MS Excel, MS PowerPoint, MS Visio, AutoCAD, VLSI, MS-CIT Certified. Education Details 
January 2012 BE Electronics Mumbai, Maharashtra Mumbai University
January 2006    Maharashtra State Board
Business Analyst 

Business Analyst - Intertek India Pvt Ltd
Skill Details 
SDLC- Exprience - 75 months
VISIO- Exprience - 60 months
REQUIREMENT GATHERING- Exprience - 15 months
Documentation- Exprience - Less than 1 year months
Functional Testing- Exprience - Less than 1 year months
Business Analysis- Exprience - Less than 1 year months
Jira- Exprience - Less than 1 year monthsCompany Details 
company - Intertek India Pvt Ltd
description - Business Analyst. Key responsibilities include Requirements Gathering, Requirements Analysis. Documentation like FRD creation. Providing KT sessions to the team. Having Client Communication. Gap Analysis.
company - Intertek India Pvt Ltd
description - Requirement Gathering from Businesses. Creating FRDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Walkthrough to team regarding the requirement â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Creating UAT Test cases. Executing the same.
â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	Inlight (Feb 2018 till date)
Platform	.Net

Description:
Inlight - (Supplier Risk Assessment Application)
Inlight is an Application designed to assess the Suppliers within the Supply chain. The Application on boards the Importers, Exporters and Suppliers. Based on the role they perform a Questionnaire is assigned to them and they fill out the Questionnaire. Basis the answer a scoring methodology is defined and the Suppliers are assessed to be Critical, High, Medium and Low. This helps in assessing the risk involved in working with certain Suppliers in the Supply chain.

Beyond Curriculum â Completed Internship in L&T â Attended Logistics Business School Training in Germany.
â A1 Certified in German Language.
â Travelled Onsite for Business Meetings and Discussions with Clients.

Personal Dossier .
company - AllCargo India Pvt Ltd
description - FRD creation
Client communication
Vendor Management
Having product Walk through with the team.
company - AllCargo India Pvt Ltd
description - Requirement Gathering from Businesses. Creating BRDs and FSDs.
â Vendor interaction for functional and technical disciplines.
â Creating Project Plan.
â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
Project 1	CRM (Nov 2017 to Feb 2018)
Platform	Microsoft D365

Description:
CRM - (Sales Management System)
CRM is a Software solution specially designed for handling Sales Management. This is a product provided by Microsoft which helps in tracking the sales of company, the activities of the salesperson, 360-degree view of customer accounts. This basically helps to get the overall status and view of various businesses the company is achieving from different Customers. A platform where the salesperson provides the details of Lead, Opportunity, Accounts and Businesses. Available on Cloud.

Project 2	Credit Risk (Nov 2017 to Feb 2018)
Platform	.Net

Description:
Credit Risk - (Customer credit check Management System)
Credit Risk is a Software solution specially designed for checking the credit status of the customer from which businesses are gained. The software basically is designed to take the KYC and the consent from the customer. For those customers who provide the consent, the credit report and monitoring report are obtained from the Credit Bureau. Based on the reports the customer health can be determined and business with them can either be  or discontinued.

Work Experience 3:
company - Capgemini India Pvt Ltd
description - Client: DB Schenker â Analyzing business requirements and defining consistent, correct and complete specification â Change Proposal Management; Effort Estimation, Impact & Gap Analysis â Actively participate in Change proposal implementation & testing and define ways for improvement / enhancement â Defect analysis & clarifying functional queries of team members & developers â Prepare Requirement document, User manual, Test cases and training material â Impart business process knowledge transfer to the team members. Prepare business / functional process workflow using Visio, UML etc â Working knowledge of OOAD - Object Oriented Analysis & Design concept.
â Helping the Junior BAs in their work. Supervising their work.
â Tools & Applications: System Architect, DOORS, UML designs & concepts, HP Quality Center, MWB, Jformdesigner â Test Management: Test Data creation, Test Case writing, Test Case Execution (Manual), Regression tests at various stages in the SDLC

Project Details
company - Capgemini India Pvt Ltd
description - Platform	Java

Description:
TANGO - (Sea & Air Cargo Management System)
TANGO is a Software solution specially designed for handling sea and air cargo (Import & Export) Management. TANGO manages the creation of Shipment, Tracking the shipment via multiple service legs i.e. Pick-up, Delivery leg etc. It helps in managing the end to end shipment with respect to the entire department involvement (globally)

Work Experience 2:
company - Capgemini India Pvt Ltd
description - "
Business Analyst,"IT Skills: Area Exposure Modeling Tool: Bizagi, MS Visio Prototyping Tool: Indigo Studio. Documentation: MS Office (MS Word, MS Excel, MS Power Point) Testing Proficiency: Smoke, Sanity, Integration, Functional, Acceptance and UI Methodology implemented: Waterfall, Agile (Scrum) Database: SQL Testing Tool: HPQC Business Exposure Education Details 
 Bachelor Of Computer Engineering Computer Engineering Mumbai, Maharashtra Thadomal Shahani Engineering college
 Diploma Computer Engineering Ulhasnagar, Maharashtra Institute of Technology
 Secondary School Certificate  Ulhasnagar, Maharashtra New English High School
Senior Business Analyst - RPA 

Senior Business Analyst - RPA - Hexaware Technologies
Skill Details 
DOCUMENTATION- Exprience - 47 months
TESTING- Exprience - 29 months
INTEGRATION- Exprience - 25 months
INTEGRATOR- Exprience - 25 months
PROTOTYPE- Exprience - 13 monthsCompany Details 
company - Hexaware Technologies
description - Working as a RPA Business Analyst
company - BBH- Brown Brothers Harriman & Co
description - is a private bank that provides commercial banking, investment management, brokerage, and trust services to private companies and individuals. It also performs merger advisory, foreign exchange, custody services, commercial banking, and corporate financing services.

Responsibilities: â¢ Performed Automation Assessment of various Processes and identified processes which can be candidates of RPA.
â¢ Conducting Assessment that involves an initial Understanding of the Existing System, their technology, processes, Usage of the tools, Feasibility of tool with automation tool along with automation ROI analysis.
â¢ Preparing the Automation Potential Sheet which describes the steps in the process, the volume and frequency of the transaction, the AHT taken by SME to perform the process and depending on the steps that could be automated, Automation potential and the manual efforts that will be saved are calculated.
Calculating the complexity of the Process which is considered for automation and depending on all these factors Number of Bots and Number of Automation tool Licenses are determined.
â¢ Implementing a Proof of Concept (POC) to Validate Feasibility by executing the selected critical use cases for conducting a POC which will helps to identify financial and operational benefits and provide recommendations regarding the actual need for complete automation.
â¢ Gathering business requirements by conducting detailed interviews with business users, stakeholders, and Subject Matter Experts (SME's) â¢ Preparing Business Requirement Document and then converted Business requirements into Functional Requirements Specification.
 â¢ Constructing prototype early toward a design acceptable to the customer and feasible.
â¢ Assisting in designing test plans, test scenarios and test cases for integration, regression, and user acceptance testing (UAT) to improve the overall quality of the Automation.
â¢ Participating regularly in Walkthroughs and Review meetings with Project Manager, QA Engineers, and Development team.
â¢ Regularly interacting with offshore and onshore development teams.
company - FADV - First Advantage
description - is a criminal background check company that delivers global solutions ranging from employment screenings to background checks.
The following are the processes which were covered:
Email Process, Research Process, Review Process.

Responsibilities: â¢ Requirement Gathering through conducting Interviews & Brainstorming sessions with stakeholders â¢ To develop decision models and execute those rules as per the use case specifications.
â¢ To Test/validate the decision models against document test data.
â¢ To maintain and enhance the decision models for changes in regulations as per use case specifications.
â¢ Responsible for performing the business research that will make a business growth.
â¢ Developing a clear understanding of existing business functions and processes.
â¢ Effectively communicate with the onsite clients for the queries, suggestions, and update.
â¢ Giving suggestions to enhance the current processes.
â¢ Identifying areas for process improvement.
â¢ Flagging up potential problems at an early stage.
â¢ Preparing PowerPoint presentations and documents for business meetings.
â¢ Using any information gathered to write up detailed reports.
â¢ Highlighting risks and issues that could impact project delivery.
â¢ Able to work accurately.
â¢ To develop and maintain documentation for internal team training and client end user operations.
â¢ To work efficiently with team members and across teams.
â¢ To mentor and train junior team members.
company - Clinical Testing, Lab Work and Diagnostic Testing
description - IQVIA provides services to its customers this includes: Clinical Testing, Lab Work and Diagnostic Testing under clinical trial. These customers need to pay to IQVIA and aging details and invoices are generated for the same.
The following are the processes which were covered:

Tracking Payments, Automated Real Time Metrics Reporting (Dashboard), Past Due Notifications, AR Statements, Credit/Rebill.
Responsibilities: â¢ Conducting meetings with clients and key stakeholders to gather requirements, analyze, finalize and have formal sign-offs from approvers Gather and perform analysis of the business requirements â¢ Translating the business requirements into the Business Requirement Document [BRD], Functional Requirement Document [FRD].
â¢ Facilitating meetings with the appropriate subject matter experts in both business and technology teams â¢ Coordinating with business user community for the execution of user acceptance test as well as tracking issues â¢ Working, collaborating and coordinating with Offshore and Onsite team members to fulfill the BA responsibilities from project initiation to Post-Implementation â¢ Reviewing the test scripts with business users as well as technology team. Execute test scripts with expected results for the System Integration Test (SIT) and User Acceptance Test (UAT) â¢ Coordinating and conducting the Production Acceptance Testing (PAT) with the business users â¢ Creating flow diagrams, structure charts, and other types of system or process representations â¢ Managing changes to requirements and baseline through a change control process â¢ Utilizing standard methods, design and testing tools throughout project development life cycle â¢ Work closely with the operational functional teams, operations management, and personnel, and various technology teams to facilitate a shared understanding of requirements and priorities across all areas
company - Eduavenir IT Solution
description - Project: M.B.M.S

M.B.M.S. - is an Inventory management application that allows user to manage inventory details of different warehouses, having different products located at various locations and help extract what goods have been procured, sold or returned by customers. It generates automated invoicesalong withcustomized reports. It also managescustomer complaint and resolution system implementation along with automated MIS on monthly basis.Sales and forecastingis also developed on MIS System and the streamlining of process of warehousing and dispatch along with online proof of delivery management system (POD documentation) is generated.

Responsibilities: â¢ Participate in requirement gathering discussion with client to understand the flow of business processes â¢ Analyze the requirements and determine the core processes, develop Process Documentation and ensure to stay up-to-date in conjunction with on-going changes â¢ Participate in process flow analysis and preparing BRD, SRS.
â¢ Coordinating with developers, designers & operations teams for various nuances of the project, communicate the stakeholder requirements from requirement /enhancement to implementation and finally deliver the same within estimated timeframe.
â¢ Support UAT by reviewing test cases, manage version control of documents, software builds.
â¢ Coordinate with the stakeholders for UAT sign off and coordinate internally for production movement till Golive stage of the application.
â¢ Provide demo and training to internal and end user using PowerPoint presentation.
â¢ Resolving project functional &technical issues during UAT.
â¢ Prioritizing the Production bugs and resolving the same within the estimated timeframe.
â¢ Preparing Project Status Report and Production Bugs Status to all the stakeholders.
â¢ Promoting and Networking for online trading platform.
â¢ Designing query sheet for obtaining and comparison of quotes from various vendors.
â¢ Development of product codes / material codes for inventory management (Master Data Management)
company - CAPGEMINI Head Office
description - Type: Mobile and Device Testing.       Duration: January 2014 - August 2014

Follet - An application which takes an electronic request from the user for the books he requires from a particular follet store. This detailed information about books that will include the name of the book, its price, the date of the transaction and the parties involved which will then be sent to follet stores. User then create request for one or more books for a given date. This request is then processed further and user gets a mail of the date when he will be provided with that book.

Responsibilities: â¢ Understanding the needs and business requirements.
â¢ Preparing BRD, SRS by eliciting all the requirements from the client and SMEs â¢ Understanding the dependency of the modules in the system â¢ Preparation of test plan for Unit level and Integration level.
â¢ Preparation and execution of test cases.
â¢ Defect tracking, Issue Resolution, Risk Monitoring, Status Tracking, Reporting and Follow-up.
â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - 
company - CAPGEMINI Head Office
description - Humana is a health care insurance project of U.S. which deals with supplying various medicines to citizens as per the doctor's reference and patient's insurance policy. This application keeps track of all the medicines user has consumed in the past and generates a patient history. A citizen is given a drug only after the doctor's reference so the doctor's information is also linked with the patient's history.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Involved in writing test cases based on test scenarios and execute them.
â¢ Ensuring Test Coverage using Requirement Traceability Matrix (RTM) â¢ Preparation of Test Completion report.
company - CAPGEMINI Head Office
description - Testing Trends WQR (World Quality Report) is an application which allows the users to take a survey on different methods and technologies used for testing. Users can choose to answer any type of questions under three different categories. Users have a facility to search, view and export the data to excel. Also, users get daily and weekly reports through email about the new trends in testing implemented around the globe. Testing Trends WQR app is available on Android and IOS platforms.

Responsibilities: â¢ Understanding the requirements and getting clarifications from client.
â¢ Writing test cases based on test scenarios and executed them.
â¢ Performing different types of testing such as Functional, Integration, System, and UAT.
â¢ Defect resolution and maintenance of the application."
Business Analyst,"TECHNOLOGICAL SKILLS â¦ Knowledge of Computers on the Windows platform. â¦ Fluency in MS-Office Applications such as Excel, Word, PowerPoint, etc. â¦ HTML, JAVA, PHP ATTRIBUTES â¦ Hardworking towards achieving the Goal â¦ Good communication skills â¦ Quick learner â¦ Good interpersonal relationEducation Details 
January 2016 to January 2018 MMS  Mumbai, Maharashtra University of Mumbai
January 2016 to January 2018 MMS Management Mumbai, Maharashtra University of Mumbai
January 2014 B.Sc.  Bandra, MAHARASHTRA, IN Rivzi College
January 2011 HSC  Bandra, Maharashtra, IN St. Andrews College
January 2011 HSC   Allana Junior College
January 2009 SSC   Canossa High School
January 2008 SSC   Maharashtra State Board
Business Analyst 

Business Analyst - Mass Group of Companies
Skill Details 
EXCEL- Exprience - 23 months
HTML- Exprience - 6 months
JAVA- Exprience - 6 months
PHP- Exprience - 6 months
POWERPOINT- Exprience - 6 monthsCompany Details 
company - Mass Group of Companies
description - Key Role   Analyst
Responsibilities:
â¦ Manage risk and provide risk management process.
â¦ Liaise with other project areas to coordinate with interdependencies and resolve issues.
â¦ Analyse and map business process.
â¦ Guide stakeholders on devising effective and efficient approaches to achieving project objectives.
â¦ Preparation of various Derivable i.e Business requirement, functional requirement and report specification.
company - Commversion Pvt.LTD
description - Responsibilities:
â¦ Employee Satisfaction reports
â¦ Evaluate overall work
â¦ Internal Audit
â¦ Maintain timesheet validation.
â¦ Set Simple and performance driven compensation strategies and polices."
Business Analyst,"Education Details 
February 2006 to February 2006 TYBCOM Commerce  mumbai
Business Analyst 

Business Analyst
Skill Details 
Company Details 
company - Motilal Oswal
description - Business Analyst
Handling IT Operation for Institutional Equities 
Maintain Daily MIS in Excel for CAG, Research, Derivative, Sales team Preparing Auto Dashboard For Research, Sales, Trading team Working on Excel Macro to Create Innovative Report 
Working on Block Related Data Working on BD Fund from different GEO Working on Investors Corporate Meeting to track Corporate Block & Fund Interest in Sector
company - FSS
description - Project Description:
Maintain and prepare cash indent, cash report, cash position, and cash planning
Responsibilities:

â¢ Maintain Daily MIS in excel.
â¢ Provide complete information about MIS & ATM.
â¢ Maintain and prepare cash indent, cash report, cash position., cash planning
â¢ Co-ordinate with BANK CASH DEPARTMENT.
â¢ Co-ordinate with custodians

.
DEGREE/ COURSE          YEAR of PASSING   GRADE          INSTITUTE                              UNIVERSITY/ BOARD   PERCENTAGE
company - ANGEL BROKING
description - Reporting: Assistant Manager /   SR. Manger
Responsibilities:

â¢ Handling team of 14 Quality assurance team members
â¢ Maintain Daily MIS in excel of team productivity
â¢ Maintain and prepare repots, adding comments on remark
â¢ mailing client  for modification of given number
â¢ Mailing reports to different branches
â¢ Coordinating with RM
â¢ Provide complete information about script to client"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Python Developer,"Technical Skills / Responsibilities: â¢ Hands on Experience with Production and Maintenance of Projects. â¢ Experience in handling projects in agile methodology. â¢ Experience in handling projects in SDLC, Involved in each stage of Software Development Life Cycle. â¢ Responsible to gather requirement (Customer Interaction) and providing Estimate & solution document then as per process FS, TS, Coding, UTP, UTR, PTF, SOW submission to customer. â¢ Having strong knowledge of Debugging and Testing based on Python and AS/400. â¢ Worked as Change Controller - Responsible for promoting changes in Development to UAT and LIVE environment through Pivotal Cloud Foundry. â¢ Have good communication skills, Inter personal skills, hardworking and result oriented as an Individual and in team. Certification and Trainings: â¢ Completed Internal Python training. â¢ Completed Internal Python Web Crawling training. â¢ Completed Internal Python Web Scraping training. â¢ Completed Internal Python for Data Science training. â¢ Completed Internal MongoDB training. â¢ Completed Internal MySQL training. â¢ Completed Internal PostgreSQL training. â¢ Completed Internal DJango training. â¢ Completed Internal Angular 6, HTML, CSS training. â¢ Completed German A1 level and preparing for A2 from Goethe-Institute. â¢ Completed Internal Core Java training. â¢ Completed IBM I series AS\400 Training course at Maples Institute, Pune. â¢ Complete Internal MOVEX ERP training (Techn: AS400/RPG/RPGLE) â¢ Completed Internal M3 ERP training (Techn: Java) â¢ Completed Internal Stream serve training. â¢ Completed M3 Enterprise Collaborator (MEC) training.Education Details 
 M.Sc. Computer Science Pune, Maharashtra Pune University
 B.Sc. Computer Science Pune, Maharashtra Pune University
 H.S.C.  Pune, Maharashtra Pune University
Python RESTful API developer 

Python developer - KPIT Technologies
Skill Details 
Flask- Exprience - Less than 1 year months
Python- Exprience - Less than 1 year months
Restful- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Numpy- Exprience - Less than 1 year months
AS/400- Exprience - 90 monthsCompany Details 
company - KPIT Technologies
description - since 6th July 2011 to till date:

â¢ Currently working as a Python API developer having 2 years of experience in Python- MongoDB/MySQL development/support project.
â¢ Worked as a M3 Java developer and Stream serve developer of Movex/M3 ERP for 1
year.
â¢ Worked as a Senior AS400 and Stream serve developer of Movex/M3 ERP for 4 years.

Technical Expertise:
â¢ Python development:
â¢ Python - MongoDB
â¢ Python - MySql
â¢ Python Cache & Memoization
â¢ Python GIT
â¢ Python PWS (Pivotal Web Service - Cloud Foundry)
â¢ German A1 Level

â¢ M3/Movex ERP development:
â¢ M3 Java of Movex/M3 ERP
â¢ AS400 development of Movex/M3 ERP
â¢ Stream Server development of Movex/M3 ERP
â¢ Movex/M3 Standards, RPG/400, CL/400, ILE RPG, ILE CL, DB2/400, QUERY400 and SQL/400, Subfiles, Printer Files, PF ,LF
â¢ Movex/M3 Flows, Programs & database structure, MI Programs."
Python Developer,"Education Details 
June 2013 to June 2016 Diploma Computer science Pune, Maharashtra Aissms
June 2016 BE pursuing Computer science Pune, Maharashtra Anantrao pawar college of Engineering & Research centre
Python Developer 


Skill Details 
Company Details 
company - Cybage Software Pvt. Ltd
description - I want to work in organisation as a python developer to utilize my knowledge & To gain more knowledge with our organisation."
Python Developer,"TECHNICAL PROFICIENCIES Platform: Ubuntu/Fedora/Cent OS/Windows Database: MySQL Languages: Python, Tensorflow, Numpy, C, C++ Education Details 
January 2016 ME Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2014 B.E Computer Engineering Pune, Maharashtra Savitribai Phule Pune University
January 2010    RYK Science College, Maharashtra state board
January 2008    Maharashtra state board
Python developer 

Python Developer
Skill Details 
C++- Exprience - 6 months
MYSQL- Exprience - 6 months
PYTHON- Exprience - 6 monthsCompany Details 
company - Fresher
description - Python programming"
Python Developer,"Technical Skills: Languages Python Python Framework Django, DRF Databases MySQL, Oracle, Sqlite, MongoDB Web Technologies CSS, HTML, RESTful Web Services REST Methodologies Agile, Scrum Version Control Github Project Managent Tool Jira Operating Systems Window, Unix Education Details 
 BE   Dr.BAMU,Aurangabad
Python Developer 

Python Developer - Arsys Inovics pvt ltd
Skill Details 
CSS- Exprience - 31 months
DJANGO- Exprience - 31 months
HTML- Exprience - 31 months
MYSQL- Exprience - 31 months
PYTHON- Exprience - 31 months
web services- Exprience - Less than 1 year months
Logger- Exprience - Less than 1 year months
Mongodb- Exprience - Less than 1 year months
json- Exprience - Less than 1 year months
Unix- Exprience - Less than 1 year months
Rest- Exprience - Less than 1 year months
Sqlit3- Exprience - Less than 1 year monthsCompany Details 
company - Arsys inovics pvt ltd
description - Project - F-MAS (Frequency Monitoring and Analysis Systems - (F-MAS))

F-MAS is a project for managing network inventory, network communication, fault management & network traffic analysis. The telecommunications service providers, are used to support a range of telecommunication services. The Operations Support Systems (OSS) collectively provides support for various elements used in Public Switched Telephone Networks, for example processing an order may require information on the services the customer already has, the network they are using, and currently available resources.

Responsibilities:
â¢ Participated in entire lifecycle of the projects including Design, Development, and Deployment, Testing and Implementation and support.
â¢ Developed views and templates with Python and Django's view controller and templating language to created user-friendly website interface.
â¢ Implemented navigation rules for the application and page outcomes, written controllers using annotations.
â¢ Created this project using Django, Django REST API, MYSQL, PyMYSQL, Python, HTML5, CSS3.
â¢ Created CRUD methods (get, post, put, delete) to make requests to the API server and tested Restful API
using Postman.
â¢ Created Unit test cases for unit testing.
â¢ Worked with JSON based REST Web services
â¢ Wrote Python routines to log into the websites and fetch data for selected options.
â¢ Used Python modules such as requests, urllib for web crawling.
â¢ Added the navigations and paginations and filtering columns and adding and removing the desired columns for view.
â¢ Created a Git repository and added the project to GitHub.
â¢ Utilized Agile process and JIRA issue management to track sprint cycles.
â¢ Worked in an agile development environment.

Environment: Python, Django, MySQL, HTML, CSS, SQLAlchemy, JSON, agile, Web Services (REST), Urllib.
company - Arsys
description - 1. Working as back end as well as front end developer
2. working on rest and restfull api's.
3. Design and develop a project in Agile scrum.
4. Git hub for code deployment
5. Working on MVT ."
Python Developer,"Training attended: 1. Successfully completed ESD program conducted by Zensar Technologies, Pune in 2017. 2. Successfully completed Employability training conducted by Barclays, Global Talent Track, and NASSCOM foundation in 2015. Achievements: 1. Treasurer in IEEE student branch at JSCOE, Pune for 2017-18. 2. Worked as team leader in collegeâs various technical and cultural events from 2016 - 2017. 3. Project idea got selected for final prototyping round in KPIT-Sparkle 2018, Pune. 4. Participated in Avishkar 2017 conducted by Savitribai Phule Pune University. 5. Project idea submitted in Accenture Innovation 2018, Pune. 6. Brought sponsorship of Rs. 15,000 from Platinum Auto (formerly Royal Enfield) in 2017, Pune. 7. Secured 1 st Rank for college level competition of Poster presentation on Smart ambulance in 2017, Pune. 8. Organized IEEE workshop on âExcellence in English and Public Speakingâ in 2017, Pune Workshops attended: 1. Successfully completed 4 daysâ workshop on âMedical IOTâ conducted by IEEE standardâs association at VIP in 2017, Pune. 2. Successfully completed 2 daysâ workshop on âIntroduction to Arduinoâ at SCOE in 2016, Pune. 3. Successfully completed 3 daysâ workshop on âRobotics for Juniorsâ conducted by Computer Society of India at SKNCOE in 2016, Pune. 4. Participated in various inter-college technical competitions at SCOE, PICT, and AISSMS, Pune. Education Details 
June 2018 Bachelor of Engineering Computer Pune, Maharashtra Savitribai Phule Pune University
June 2014 HSC   Maharashtra State Board
June 2012 SSC   Maharashtra State Board
Python Developer 

Python Developer - Atos Syntel
Skill Details 
PYTHON- Exprience - 15 months
DATABASE- Exprience - 7 months
MYSQL- Exprience - 7 months
DJANGO- Exprience - 6 months
HTML5- Exprience - 6 months
REST API- Exprience - 6 monthsCompany Details 
company - Atos Syntel
description - Working as a developer in the field of computer vision for a US based client in banking domain.
1. Design and development of computer vision based algorithms for image preprocessing using OpenCV, PIL, and Numpy.
2. Unit testing and debugging the code and maintaining the versions using Git."
Python Developer,"â¢ Operating Systems: Windows â¢ Others: MS Excel, MS Office, MS Power Point Key Projects Handled Project Title: fruit sorting and disease detection Client: Kranti Dynamics Team Size: 5 Education Details 
January 2014 B.E. Electronics Mumbai, Maharashtra University of Mumbai
Python Developer/analyst 

python developer and data analyst
Skill Details 
python scripting,programming,developing- Exprience - 12 months
frontend  ,html- Exprience - 12 months
python liabrary, numpy,pandas,matplolib,requests,beautiful soap- Exprience - 12 months
mysql- Exprience - 12 months
django- Exprience - 12 months
web scrapping- Exprience - Less than 1 year monthsCompany Details 
company - Ace The Power of 5
description - The Accountabilities:

â Understanding the functional requirements of the application given by the client.

â Participated in walkthroughs of business requirements, functional requirements and technical design to ensure their testability.

â Responsible for Software Configuration Management of project deliverables.

Technical skill set:

â¢ Languages: C, C ++, Java, python,python liabray,mysql,django,html

â¢ Scripting: Python,
â¢ GUI development: Tk, Java
company - kranti dyanamics
description - programming,scripting,developer,web scrapping"
Database,"TECHNICAL EXPERTISE â¢ DB Languages: SQL â¢ Database Tools: SQL Server 2014/ 2017 Postgresql 9.5, 9.6, Oracle 11gR2 â¢ Operating Systems: Redhat Linux, Oracle Linux, Windows Server 2012/ 2016 OTHER TECHNICAL SKILLS ORACLE 11G R2 â¢ Proficient in Oracle Database Software Installation, Creation of Database using GUI/Silent DBCA, Architecture, File management, Space Management, User Management, Creating Roles and assigning Privileges/Roles in 11gR2 and troubleshooting them. â¢ Hands on experience Control files/Redolog/Archive/Undo Management â¢ Configuring Listener.ora/Tnsnames.ora file using Netmgr/netca â¢ Generating AWR reports, ADDM, ASH reports to diagnose the problems â¢ Database Backup, Cloning/Duplicate using hot & cold backups using RMAN. â¢ Knowledge in Flashback Technologies & Expdp/Impdp â¢ Implemented Oracle11gR2 RAC on Oracle Linux Platform and knowledge of services for troubleshooting RAC (CRSCTL, SRVCTL) â¢ Knowledge on installation and configuration of RAC. Add/Remove Nodes on RAC â¢ Configuration of physical standby database (Data guard) â¢ Successfully upgraded from 11.2.0.1 to 11.2.0.4 & PSU patching using O patch. STRENGTHS â¢ Good Communication skills. â¢ Self-confident and can adapt myself to all work environments. â¢ Enjoy responsibilities as lead and team player. â¢ Patient listener & quick learner. â¢ Capable of explaining issues & solving them.Education Details 
 B.E Computer Engineering Mumbai, Maharashtra Mumbai University
 Higher Secondary Certificate   Dr. DY Patil Jr College
Database Administrator 

Database Administrator - DBA in Marketplace Technologies Ltd
Skill Details 
DATABASE- Exprience - 61 months
BACKUPS- Exprience - 48 months
LINUX- Exprience - 48 months
MS SQL SERVER- Exprience - 48 months
SQL- Exprience - 48 monthsCompany Details 
company - DBA in Marketplace Technologies Ltd
description - Project Title: EBoss, Datafeed, MFDB, RTRMS, IndiaINX
company - Standard & Enterprise
description - Redhat Linux 7.4, Postgresql 9.5, 9.6
Duration: Feb 2017 - till date
Description: Bombay Stock Exchange BSE  is Asia's first & the Fastest Stock Exchange in world with the speed of 6 micro seconds and one of India's leading exchange groups provides an efficient and transparent market for trading in equity, currencies, debt instruments, derivatives, mutual funds. BSE SME is India's largest SME platform which has listed over 250 companies and continues to grow at a steady pace.

JOB ROLES & RESPONSIBILITIES
POSTGRESQL - â¢ Worked on Redhat Linux OS Cluster with Postgresql for High Availability (HA) using Pacemaker.
â¢ Coordinated with Developers/Linux teams for database knowledge and support.
â¢ Participated in implementation of new releases into production.
â¢ Installed /Configured Postgresql from source or packages on Redhat Linux servers.
â¢ Performed Postgresql Server Management tasks i.e. Backup & Restore, Configuration, Roles, Blockings, Tablespace creation and Troubleshooting.
â¢ Worked with Storage team for Disaster Recovery DR setup built on SAN using EMC technology â¢ Configured LDAP authentication & GSSAPI Authentication from Windows to Linux for Postgresql.
â¢ Configured logical replication for Database servers, hot standby Postgresql servers, faster database backup methods, schema and tablespace backups.
â¢ Configured maximum connections to database on Linux servers.
â¢ Installed tds_fdw from source for linked servers to connect to heterogeneous databases & other required extensions, backup configuration, PITR using base backups.

MSSQL - â¢ Day-to-day administration of live SQL Servers.
â¢ Participated in Live Primary Recovery PR & Disaster Recovery DR activities.
â¢ Participated in PR & DR mocks for new releases into production.
â¢ Configured Linked Servers, Transactional replication, Maintenance tasks like database backup & restore, recovery, scheduled jobs, maintenance plans.
â¢ Installed & Configured SQL server 2014, 2017 standalone and SQL Cluster servers.
â¢ Maintained the security of the database by providing appropriate SQL roles, logins and permissions to the users on demand.
â¢ Worked with teams on application rollouts, application issues and SQL server migrations.
â¢ Exposure in handling production system with skills and understand client's requirement.
â¢ Performed SQL Server service pack upgrades and hot fixes.
â¢ Handled multiple SQL Instances on Windows SQL Cluster environment built on  EMC SAN.
â¢ Worked on MSSQL DB clusters with active/active & active passive servers, Always-On Availability Groups (AAG) and HA/DR Setup.
â¢ Have experience on SAN and RAID levels and building and supporting SQL Cluster servers on SAN Environments.
company - BSE Bombay Stock Exchange
description - Environment: Windows server 2008 R2, 2012 R2, 2016 Enterprise & Standard,"
Database,"Technical Expertise Operating Systems Microsoft Window Server 2003/2008/2008 R2/2012 Database Technologies SQL Server, Sybase ASE Server, Oracle, MongoDB Monitoring and Ticketing Tools HP Service Manager 7.0/9.0, Solar winds DPA, JIRA and MongoDB OPS manager Web Server IIS 7.0 Database Tools SSMS, DBArtisan, Studio 3T, SnapShot Manager for SQL ServerEducation Details 
 B. Tech Computer Science Gulbarga, Karnataka PDACOE, Gulbarga, Autonomous Institution
Database Administrator II 

Database Administrator III - BNY Mellon International Operations (India) PVT. LTD
Skill Details 
Sql Dba- Exprience - Less than 1 year monthsCompany Details 
company - BNY Mellon International Operations (India) PVT. LTD
description - SQL Server :
ï	Installation, configuration of database servers using slipstream and setup all the maintenance jobs as per the standard policy on standalone as well as cluster environments with latest service packs
ï	Installation of SSRS, uploading of .rdls and assigning correct data sources to reports. Grant necessary access to users & developers on reporting website. Aware of SSIS and designing packages as well.
ï	Create and manage logins, users for database applications, assigning permissions as per requests, resolving user login issues.
ï	Migration of all SQL server 2005/2008 servers to higher versions.
ï	Setup of database refresh jobs on QA, DEV and UAT environments and fixing orphaned users.
ï	Troubleshoot performance related issues. 
ï	Part of multiple projects to work with developers and provide all required support for testing in QA, UAT & DEV environment. 
ï	Lead the DR tests for database team.
ï	Participate in database purge and archive activities.
ï	Writing codes for automating database administration tasks.
ï	Worked on automating DR tasks to start the agent jobs on multiple servers, restore databases for log shipped databases without manual intervention for online databases post DR activities.
ï	Provide support to vendor databases, follow up with the vendor calls and timely escalate to next level when there is no update in predefined timeline.
ï	Installation and configuration of smsql on windows server. Schedule jobs for creation and deletion of clones on sql server. Maintain backups using smsql.

MongoDB Server:
ï	Installation and configuration of MongoDB server.
ï	Creation of databases and collection.
ï	Creation new user and grant access using Ops manager.
ï	Monitor database servers using Ops manager.
Oracle & Sybase Server
ï	Managing and maintaining multiple instances of Databases on Linux and windows servers.
ï	Monitoring daily jobs includes backups, refresh and maintenance jobs.
company - Hewlett-Packard India Sales PVT. LTD. On the payroll of Softenger India PVT. LTD
description - ï	Installation of SQL Server on standalone as well as windows cluster environments with latest service packs
ï	SQL server installation using slipstream.
ï	Installation of reporting services
ï	Creating logins and users, assigning permissions as per requests.
ï	Security audit for all logins includes maintenance of unused and orphan user logins
ï	Create & Maintain daily and weekly jobs/maintenance plans includes backup, index rebuild/reorganize , update statistics and database consistency check
ï	Create linked servers and ensure connectivity between servers
ï	Monitor disk space proactively & Space management using data and log file shrinking
ï	Monitor blocking, deadlocks, open transactions and slow running queries during performance issues and highlight costly queries to developers.
ï	Configure alerts for deadlock and blocking to maintain performance
ï	Implementing high availability technologies like log shipping, AlwaysON, mirroring and its troubleshooting, also have knowledge on replication
ï	Successfully completed migration of Databases from one server to another
ï	Performing DR drills (Online/Offline) on quarterly basis
ï	Power shell scripting to monitor, restart SQL service and get Email alert for the service status.
ï	Maintain TNS entries for oracle client as per client requests.
ï	Interacting with customers for requirements
ï	Contacting customer to update the status of handling issues and service requests at every stage of resolution
ï	Managing proper escalation and notification matrix for all support levels"
Database,"TECHNICAL SKILLS Operating Systems MS Windows Server 2012/2008/XP Software and Tools MS LiteSpeed, Idera SQL Safe, SSMS, Upgrade Advisor, SQL Server Profiler, SCOM, Diagnostic Manager, Remedy, Jira, Infopacc, Tivoli TDP backup tool, SQL Pack DatabasesMS SQL Server 2016/2014/2012/ 2008 R2/ 2008, Oracle 10g, Netezza Microsoft azure Education Details 
 Masters of Science Computer Science Pune, Maharashtra Indira College, Pune University
Lead database administrator 

Microsoft Certified Professional with 11 years of experience in database administration on MS SQL Server 2016/2014/2012/2008 R2/ 2008
Skill Details 
MS SQL SERVER- Exprience - 110 months
Microsoft azure- Exprience - Less than 1 year months
Always on availabiity group- Exprience - Less than 1 year months
Database mirroring- Exprience - Less than 1 year months
Performance tuning- Exprience - Less than 1 year months
Log shipping- Exprience - Less than 1 year months
Installation , upgrade, migration and patching- Exprience - Less than 1 year monthsCompany Details 
company - Ensono
description - Employment transfer as a part of project acquisition to Ensono from Wipro.
SQL Server Database Administration
company - Wipro Technologies
description - Microsoft Certified Professional with 11 years of experience in database administration on MS SQL Server 2016/2014/2012/2008 R2/ 2008.
Experience with MS SQL Server 2016/2014/2012/2008 R2/ 2008 installation, upgrade, and administration
Microsoft Azure certified.
Have understanding of Azure VM, Azure Storage, Azure network, Azure AD and Azure SQL database.Â 
Incident management, change management and Problem management for SQL Server Database team.
Participating in meetings, conference calls with client, Service Delivery Manager and Application team for System improvements.
Participated in quarterly DR activity.
Involved in creation of SIP - Service Improvement Plans
Involved in handling of high severity issues and provided RCA for the same.
Worked on Always on availability groups, database mirroring, replication, clustering and log shipping.
Have basic understanding of Oracle and Netezza.
Provided on- call support during out of office hours and weekends.
Resource & shift management of 5 SQL DBAs from offshore in multi-client environment for Data center services.
Provided KT to team members, monitor and guide trainees.
company - Wipro Technologies
description - Responsibilities: â¢ MS SQL Server 2016/2014/2012/ 2008 R2/ 2008 installation, configuration, and administration.
â¢ Worked on Always on availability groups, log shipping, database mirroring and clustering.
â¢ Participated in  PCI scan report to perform installation of security hot fixes, service packs for SQL servers to remove vulnerability.
â¢ Participated in Holmes BOTS automation implementation of SQL Pack tool.
â¢ Worked on service requests, incidents and critical issues.
â¢ Involved in conference calls to provide DBA support for critical issues.
â¢ Performance tuning.
Environment: SQL Server 2016/2014/2012/2008R2/2008, Windows Server 2012/2008R2/2008
company - Mphasis
description - 
company - Mphasis
description - Responsibilities: â¢ MS SQL Server 2012/ 2008 R2/ 2008  installation, configuration, and administration.
â¢ Worked on Always on availability groups, log shipping, database mirroring and clustering.
â¢ Performed SQL server patching activity â¢ Worked on daily reports like cluster failover, backup, AG/LS/Mirror report and server disk space report.
â¢ Worked on service requests, incidents and critical issues.
â¢ Participated in quarterly DR activity.
â¢ Involved in conference calls to provide DBA support for critical issues.
â¢ Provided support to windows team during patching for AG-mirror-cluster failover/failback and database health check.
â¢ Performed all the health checks for market open servers and provided update in market open call â¢ Deeply involved in   resolution of the issue and finding the root cause analysis of the issue â¢ Performance tuning.
Environment: SQL Server 2012/2008R2/2008, Windows Server 2008R2/2008
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ SQL server, Oracle and Netezza databases support tasks.
â¢ MS SQL Server 2008 R2/ 2008 installation, upgrade, and administration.
â¢ Done capacity planning for database growth for all SQL servers.
â¢ Troubleshooting alerts.
â¢ Worked on log shipping and mirroring.
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008, Oracle 10g/RAC
company - Synechron Technologies Pvt. Ltd
description - 
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ Pursued in-depth training on Oracle 11g Architecture and SQL Server.
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008, Oracle 10g
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ Carried out version changes for schemas from PE8 version to EE11 version as per the process given
Environment: Oracle 11g
company - Mastek Ltd
description - Responsibilities: â¢ SQL Server 2008 R2/ 2008 installation, upgrade, and administration â¢ database backup/restore.
â¢ Performed MS SQL Server audits â¢ Worked with database mirroring, replication, log shipping and clustering.
â¢ Supported UAT and PROD environments â¢ Performed deployment document review.
â¢ Carried out deployments for different applications
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008
company - Mastek Ltd
description - 
company - PP Software and Systems Ltd
description - 
company - PP Software and Systems Ltd
description - Description: The system provides Master Data Management and Procurement modules for dairy industry.
Responsibilities: â¢ Designed, coded, and tested â¢ Customized ERP system as per the requirement
Environment: Core Java, PostgreSQL"
Database,"SKILLSET Oracle DBA, MySQL, MARIADB, PostgreSQL Database Administration ITSKILLS SQL Oracle 10g, 11g, MYSQL, MariaDB, postgreSQL Windows, Linux Putty Education Details 
January 2018 MCS  Pune, Maharashtra Pune University
Database administrator 

Database administrator  - Infiniteworx Omnichannel Pvt. Ltd
Skill Details 
DATABASE- Exprience - 17 months
MYSQL- Exprience - 17 months
ORACLE- Exprience - 17 months
SQL- Exprience - 17 months
DATABASE ADMINISTRATION- Exprience - 6 monthsCompany Details 
company - Infiniteworx Omnichannel Pvt. Ltd
description - Pune Sept 2017 to Present

RESPONSIBILITIES:
â¢ Creating tablespaces and planning the location of data, monitoring the tablespaces growth periodically.
â¢ All replication setup
â¢ Moved database Schema changes to stage.
â¢ Dba support query resolution.
â¢ Creating user and giving specific privileges
â¢ Database management.
â¢ Database recovery, moving data files to different locations.
â¢ Planning the backup policies and Backup/ Recovery of databases based on the criticality.
â¢ IMPORT/EXPORT.
â¢ Degine schemas

Key Result Areas:

â¢ Providing 24 /7 support to resolve database performance issues, Job failures, Sessions & diagnose root causes
â¢ Installation, configuring and updating Oracle server software and related Oracle products. Installation, configuraing and updating Mysql, Sql server, MariaDB, MongoDB
â¢ Supported multiple databases and administered Oracle Databases of Large DB Sizes for production, development & test setups.

â¢ Maintaining table spaces & data files, Control files, Online Redo log files

â¢ Creating Users, granting Roles & Privileges to users and managing tablespaces for different users by granting quota on Default & Temporary tablespaces.

â¢ Taking Oracle RMAN Backups (Scheduling for day wise backup)

â¢ Implementing the incremental, cumulative and full RMAN backup for each database to have space management and effective recovery.
â¢ Logical Backup Using Export & Import/datapump Export of important tables at regular intervals.

â¢ Regular checking of trace, alert log file, all ORA errors

â¢ Working on incidents like User creation/deletion incidents, backup failed incidents.
â¢ Checking Listener Status, connectivity Troubleshooting and fixing database listener issues.
â¢ Look for any new alert / error log entries / errors, error details in Trace files generated. Executing DDL & DML scripts as per customer requirements

â¢ Mentoring, coaching and appraising team members with active involvement in the recruitment process

â¢ Contributing in Project Documentation and generating daily reports

â¢ Ensuring compliance to quality norms and taking steps for any non-conformance Spearheading complete project activities ensuring timely completion of project
â¢ Implementing security policies on different database systems with granting and revoking privileges to the users

â¢ Following change management processes and participated in related meetings

â¢ Verifying all Instances/DB are running, Tablespaces are online, Monitor Backround processes and status.
company - InnovativeTechnologies
description - Clients: BANKING DOMAIN"
Database,"Education Details 
January 2016 BSc.  Mumbai, Maharashtra Mumbai University
January 2013 H.S.C.   Maharashtra Board
January 2011 S.S.C.   Maharashtra Board
MySQL Database Administrator 

2+ Years of experience in MySQL Database Administrator ( MySQL DBA)
Skill Details 
MySQL DBA , Centos , Backup , Restore , Replication , Query Optimazation- Exprience - 24 monthsCompany Details 
company - Trimax IT Infrastructure & Services Ltd
description - Â·Â Â Â Â Â Â Â MYSQL Installation, maintenance and Upgrades (Version 5.5 , 5.6)
Â·Â Â Â Â Â Â Â MySQL database administration on a large scale MySQL installation
Â·Â Â Â Â Â Â Â Experience with MySQL on both Linux and Windows
Â·Â Â Â Â Â Â Â MySQL processes, security management and queries optimization.
Â·Â Â Â Â Â Â Â Performed query analysis for slow and problematic queries.
Â·Â Â Â Â Â Â Â Performed Structural changes to Database like creating tables, adding columns according to business requirement
Â·Â Â Â Â Â Â Â Creating and MaintainingÂ Database Maintenance Plans.



Â·Â Â Â Â Â Â Â Writing scripts to Create Jobs for Backup & Restore Plans.
Â·Â Â Â Â Â Â Â Working on MYISAM to INNODB engine.
Â·Â Â Â Â Â Â Â Working on Server shifting , tuning parameter , database purging
Â·Â Â Â Â Â Â Â Working on Mysql master slave Replication
Â·Â Â Â Â Â Â Â Handling Release management and user acceptance.
Â·Â Â Â Â Â Â Â Restore using xtrabackup.
Â·Â Â Â Â Â Â Â Responsibilities include monitoring daily, weekly and monthly system maintenance tasks such as database backup, replication verification, database integrity verification and indexing updates
Â·Â Â Â Â Â Â Â Work in 24/7 production database support.
company - Trimax IT Infrastructure & Services Ltd
description - Â·Â Â Â Â Â Â Â MYSQL Installation, maintenance and Upgrades (Version 5.5 , 5.6)
Â·Â Â Â Â Â Â Â MySQL database administration on a large scale MySQL installation
Â·Â Â Â Â Â Â Â Experience with MySQL on both Linux and Windows
Â·Â Â Â Â Â Â Â MySQL processes, security management and queries optimization.
Â·Â Â Â Â Â Â Â Performed query analysis for slow and problematic queries.
Â·Â Â Â Â Â Â Â Performed Structural changes to Database like creating tables, adding columns according to business requirement
Â·Â Â Â Â Â Â Â Creating and MaintainingÂ Database Maintenance Plans.



Â·Â Â Â Â Â Â Â Writing scripts to Create Jobs for Backup & Restore Plans.
Â·Â Â Â Â Â Â Â Working on MYISAM to INNODB engine.
Â·Â Â Â Â Â Â Â Working on Server shifting , tuning parameter , database purging
Â·Â Â Â Â Â Â Â Working on Mysql master slave Replication
Â·Â Â Â Â Â Â Â Handling Release management and user acceptance.
Â·Â Â Â Â Â Â Â Restore using xtrabackup.
Â·Â Â Â Â Â Â Â Responsibilities include monitoring daily, weekly and monthly system maintenance tasks such as database backup, replication verification, database integrity verification and indexing updates
Â·Â Â Â Â Â Â Â Work in 24/7 production database support."
Database,"TECHNICAL SKILL: Operating System LINUX, Windows Server 2012 R2, Windows 98, Windows 2000/ XP Tools & Utility Packages SQL* Loader, SQL*PLUS, OEM, Datapump, expdp/impdp, PLSQL Developer, Jenkins Database Oracle 10g, Oracle 11g, Oracle 12c Scripting UNIX Shell Scripting Language SQL Education Details 
January 2011 M.B.A.  Amravati, Maharashtra Amravati University
January 2007 B.C.A.  Nagpur, Maharashtra Nagpur University
Oracle Database Administrator 

ORACLE DATABASE ADMINISTRATOR ON LINUX/MICROSOFT WITH 4 YEARS EXPERIENCE.
Skill Details 
ORACLE- Exprience - 48 months
LINUX- Exprience - 6 months
ORACLE DBA- Exprience - Less than 1 year months
RAC- Exprience - Less than 1 year months
GOLDEN GATE- Exprience - Less than 1 year months
ASM- Exprience - Less than 1 year months
DATAGUARD- Exprience - Less than 1 year monthsCompany Details 
company - TIETO INDIA PVT. LTD
description - Pune From February 2015 till present

Project Profile:
Oil and Gas unit of Tieto India Pvt. Ltd. is working for Environmental Components (EC) application. Tieto is the authorized service provider in EC. Energy Components is a complete end-to-end hydrocarbon accounting solution following the hydrocarbons from production to transport, sales and revenue recognition. Globally market-leading hydrocarbon accounting software with functionality coverage exceeding other available solutions. Modern, flexible and scalable technology platform. Selected as the global standard and best practice by oil & gas super majors.
Responsibilities: â¢ Oracle Database Administration 11g R2, 12c and 18c â¢ Supporting databases in 24x7 environments and coordinate with Application, OS, Storage and Development Teams. Test and Production environments â¢ Regularly monitoring the trace files and Alert log files for database related issues.
â¢ Experience in monitoring the CPU usage, IO and memory utilization at OS level.
â¢ Checking the Alert log file to analyze the ORA errors if any to raise SR with Oracle.
â¢ Monitoring the log files, backups, database space usage and the use of system resources.
â¢ Configuring Backup (RMAN) for database and restoring database.
â¢ Installation, configuring and updating Oracle server software and related Oracle products of 11g and 12C.
â¢ Oracle Server installation, client installation and configuration, PLSQL developer installation.
â¢ Creating database using DBCA and manually.
â¢ Creating of Oracle user and granting proper privileges to user as per request.
â¢ Creating AWR, ASH and ADDM reports for database performance analysis.
â¢ Handling space management and performance issues in Oracle databases.
â¢ Creating remote database link.
â¢ Renaming and resizing of data files in Oracle database if needed.
â¢ Tablespace shrinking with regular time interval to reclaim server space.
â¢ Expertise in Export and Import using data pump in Oracle database.
â¢ Expertise in Configuration of Listener and Tnsnames through NETMGR and NETCA and statically also.
â¢ Managing Oracle Listener and Oracle Network Files.
â¢ Creating user Profiles, granting specific privileges and roles to the users in Oracle database.
â¢ Maintaining tablespaces & data files, Control files, Online Redo log files in Oracle database.
â¢ Worked on AWS cloud services like EC2, S3, RDS, ELB, EBS, VPC, Route53, Auto, Cloud watch, Cloud Front, IAM for installing configuring and troubleshooting on various Amazon images for server migration from physical into cloud."
Database,"Technical Skills Databases: Oracle RDBMS- 10g, 11g & 12c Technology/utilities: Data Pump, RMAN, Data guard, ASM, RAC, Golden Gate Tools: OCC, PUTTY, SQLPLUS, SQL Developer, Netbackup, SCOM, SCCM, VMWare Vsphere Operating Systems: RHEL 6.0, RHEL 6.5, UNIX and Microsoft WindowsEducation Details 

Database Administrator 

Database Administrator - BNY Mellon
Skill Details 
DATABASES- Exprience - 24 months
ORACLE- Exprience - 24 months
RMAN- Exprience - 24 months
NETBACKUP- Exprience - 24 months
SCOM- Exprience - 24 monthsCompany Details 
company - BNY Mellon
description - Databases: 600+
Team Size: 8
Duration: Jan 2017 - Till Date
Clients: Over 130+ investment banking organizations who are hosted with Eagle

Responsibilities: Database Management (Support and managing critical production, Pre-production, test and reporting databases in different platforms), Capacity Management Upgrades.
â¢ Handling day to day database activities monitoring and incident management.
â¢ Building new databases as per the requirement and prepare them for go live with the help of multiple teams.
â¢ Working on scheduled activity of database patching (CPU, PSU) â¢ Installing latest path on production, Dev and Test databases as per the suggestion from Oracle support.
â¢ Database Upgrade from 11g and to 12c.
â¢ Adding disks to ASM disk groups.
â¢ Building DR database using Active Data guard, Make it sync with prod and resolving issues if persists â¢ Data Guard Management- Checking lagging status, removing lagging of archives, checking processes like RFS/MRP, Archives Management â¢ Working on tablespace related issues â¢ Managing user access and profiles â¢ Importing and exporting using datapump â¢ Maintaining inventory of all databases in the single centralize database â¢ Refreshing test environment from production database.
â¢ Working with Oracle Support to resolve oracle errors.
â¢ Schedule daily and weekly databases backup using RMAN, Troubleshooting in RMAN issues.
â¢ Database cloning using RMAN.
â¢ Take part in cutover to upgrade application to higher version.
â¢ Strictly following ITIL process in incident management and change management.
â¢ Providing weekly report of issues in team meeting also participating and suggesting service improvement plans.
â¢ Database Migrations from one server to another or to different platforms â¢ RCA and impact analysis reporting during any production outage.

Previous Organization: Brose India
Project I: Central IT Management
company - 
description - Responsibilities: Managing our internal databases and servers of Brose global.
â¢ Providing 24x7 on-call support in the rotational shifts.
â¢ Performing day-to-day database activity â¢ Monitoring and responding DBA group Mails for all alerts, issues and ad-hoc business user requests, etc.
â¢ Database creation, patching â¢ Backup of Database in frequent cycles using Data pump/RMAN.
â¢ Database refreshes using RMAN, Datapump.
â¢ Recovery using copy of data / RMAN â¢ Monitoring logs and trace for resolving issues.
â¢ Creating new VM servers and prepare it for go live, Also decommissioning as per requirements.
â¢ Weekly patching of windows servers using SCCM and taking actions for patching if needed â¢ Monitoring and troubleshooting of daily and weekly OS backup using Symantec Netbackup â¢ Managing user accounts of OS users and database users â¢ Monitoring OS level alerts using SCOM

Project II: Data Center Migration (Onsite Project)
Responsibilities: Data center migration was one of activity for migration of our datacenter from one location to another. Where our all servers and databases were moved successfully.
â¢ Installation of Oracle 11g on Linux platforms â¢ Worked on service requests (Incidents / Change / Request) â¢ Creation of users, managing user privileges â¢ Configured RMAN backup for databases â¢ Patching of databases â¢ Configuring physical standby database using Dataguard â¢ Cloning of servers and migrate to another cluster

ACADEMIA / PERSONAL DETAILS â¢ Bachelor of Engineering (B.E.) in Computer Science and Engineering From SGBAU Amravati University, Amravati in 2014 with CGPA of 7.21

Current Address:-       Mr. Yogesh Tikhat, C/O: Raj Ahmad, Flat# G2-702, Dreams Aakruti, Kalepadal, Hadapsar, Pune - 411028
Highest Qualification   BE (cse)

PAN: -                  AOFPT5052C"
Database,"Software Skills: * RDBMS: MS SQL SERVER 2000/2005/2008 & 2012, 2014 * Operating Systems: WINDOWS XP/7, WINDOWS SERVER 2008, 12 * Fundamentals: MS Office 03/07 * Tools: SSMS, Performance Monitor, Sql profiler, SQL lite speed. Company name: Barclays Technology Centre India. Team Size: 24 Role: Database Administrator Support Description: Barclays Technology is a UK based retail & invest bank and 300 years of old bank.. It has operations in over 40 countries and employs approximately 120, 000 people. Barclays is organised into four core businesses: Personal & Corporate (Personal Banking, Corporate Banking, Wealth & Investment Management), Barclaycard, Investment Banking. Responsibilities: â Attending various calls from all over the world on various database issues. â Working on Web Gui alerts and resolving incident tickets within the time lines. â Troubleshoooting log shipping issues and fixing the related alerts. â Identifying and Resolving Blocking and locking related issues. â Configuration and monitoring Replication, Log shipping and mirroring setup. â Working on replication issues and Always ON issue. â Granting and revoking permissions on various account provisioning tasks. â Working on call support during the weekend and performing DR test's. and working on weekly maintenance jobs and weekend change requests. Education Details 
 B.Sc. Maths  Kakatiya University Board secured
SQL server database administrator 

Database administrator
Skill Details 
DATABASE- Exprience - 120 months
DATABASE ADMINISTRATOR- Exprience - 72 months
MAINTENANCE- Exprience - 48 months
MS SQL SERVER- Exprience - 48 months
REPLICATION- Exprience - 48 monthsCompany Details 
company - Barclays global services centre
description - SQL server databases implementation and maintenances

Log shipping, replication, High availability, clustering, performance tuning, database mirroring, Installation, configuration, upgradation, migration
company - Wipro Infotech Pvt Ltd
description - SQL server database administrator
company - CITI Bank
description - Worked as Database Support at Accord Fintech, Sanpada from Sep 2008 to 2013 Feb.
company - 
description - 2012.
â¢     Sound knowledge in Database Backup, Restore, Attach, and Detach and Disaster Recovery procedures.
â¢     Developed backup and recovery strategies for production environment.
â¢     Ensuring data consistency in the database through DBCC and DMV commands.
â¢     Experience in query tuning and stored procedures and troubleshooting performance issues.
â¢     Having hands on experience in DR process including log shipping and database mirroring.
â¢     Experience in scheduling   monitoring of Jobs.
â¢     Experience in configure and troubleshooting in Always ON.
â¢     Creating and Maintaining of Maintenance Plan.
â¢     Expertise in planning and implementing MS SQL Server Security and Database permissions.
â¢     Clear understanding of Implementation of Log Shipping, Replication and mirroring of databases between the servers.
â¢     Performance Tuning (Performance Monitor, SQL Profiler Query Analyzer) â¢     Security for server & Databases Implementing security by creating roles/users,
Added users in roles, assigning rights to roles.
â¢     Create and maintaining the linked servers between sql Instances.
â¢     Create and maintaining and Database mail.
â¢     Monitor and troubleshoot database issues.
â¢     Creating DTS packages for executing the required tasks.
â¢     Experts in create indexes, Maintaining indexes and rebuilds and reorganizes.
â¢     Daily Maintenance of SQL Servers included reviewing
SQL error logs, Event Viewer."
Database,"Areas of Expertise â¢ Oracle Databases 12c, 11g, 10g â¢ Weblogic 12c, 11g â¢ Grid Infrastructure â¢ RMAN â¢ ASM â¢ Middleware: OIM, OAM, SOA â¢ Shell Scripts â¢ DataGuard â¢ Web servers - OHS, Apache â¢ Architecture Designs â¢ Proof of Concepts â¢ DevOpsEducation Details 
January 2007 Bachelor of Engineering Information Technology Sangli, Maharashtra Walchand College
January 2004 Diploma Computer Engineering Jalgaon, Maharashtra Govt. Polytechnic
Lead Database Administrator 

Lead Database Administrator - Tieto Software
Skill Details 
DATABASES- Exprience - 108 months
MIDDLEWARE- Exprience - 96 months
RMAN- Exprience - 84 months
SHELL SCRIPTS- Exprience - 48 monthsCompany Details 
company - Tieto Software
description - As a part of AO (Application Operations) team, scope in project is quite wide than typical database administration. Range of accomplishments are extended right from Data Tier to Middle Tier & Application Tier:
- Maximized availability of applications from 99.3% to 99.8%
- Raised business by presenting Proof of Concepts for 10+ cases
- Delivered upgrades of various applications time to time to keep it on supported platform
- Saved SLAs as per contract by means of handling P1, P2 issues effectively
- Produced Capacity reports comprising all layers (Data, Middleware, Web) of various applications
- Generated Work Orders as per customer need
company - Tieto Software
description - - Designed databases of various applications
- Planned RMAN backup and recovery, BCP strategies
- Executed Business Continuity Testing for various applications
- Introduced Zero Cost high availability solutions - Active-Passive Failover
- Optimized performance by means of scripting automation
- Established cloning procedures for all layers of various applications
- Delivered Infrastructure changes, like FW Openings & LoadBalancer configuration for new applications
- Eliminated downtime by troubleshoot issues for Middleware products - OIM, OAM, SOA
- Contributed to build & maintain Integration Layer- SMTP, ftp, Reverse Proxy, OCM
company - Tieto Software
description - - Provided database support to environments - PROD, UAT, TEST, DEV
- Performed Database Refresh/Cloning from production to development and support databases
- Reduced risk level by means of upgrading & patching databases time to time
- Protected databases by assigning appropriate roles and privileges as per SOD
- Generated & maintained Middleware schemas using RCU
- Exceeded scope of work by supporting & maintaining WebLogic platform - installation, patching, troubleshooting issues
- Expanded duty scope to web servers: Install & maintain- OHS, apache, tomcat
company - HSBC Software
description - Being part of project supporting HSBC Bank France, I achieved:
- Handled incidents & service requests as Day to day database administration tasks
- Delivered basic implementation services - Database installation, patching, upgrades
- Performed capacity planning - managing tablespaces, compressions
- Contributed in maintaining quality of databases - managing instances, indexes, re-organization, performance monitoring & tuning using AWR, ADDM reports
- Maintained backups & recovery of database - logical backups (exp/imp), datapump (expdp/impdp), cold backups, hot backups, RMAN backup/restore, RMAN Duplication
- Reduced efforts by automation - Value add initiatives which includes writing shell scripts for automated housekeeping operations, scheduling backups, use crontab/at to schedule tasks
- Implemented high availability solutions - Dataguard"
Database,"Education Details 
May 2011 to May 2014 Bachelor of science Information technology Mumbai, Maharashtra Mumbai university
Oracle DBA 

Oracle database administrator
Skill Details 
Installation of Oracle on RH Linux & Windows. Creating/Managing user profiles and analyzing their privileges and tablespace quotas Backup of database Logical and Physical procedures. Recovery of database in case of database crash, disk/media failure, etc. Standard DBA functions like space management, Rollback segments, Extents. Database Management and Monitoring the database. Willing to learn new things. Being a constructive team member, contributing practically to the success of the team.- Exprience - 48 monthsCompany Details 
company - Accelya kale solutions ltd
description - Database Administrator working in 24*7 support environment maintaining Databases running on Oracle 11g, 12c.
Database Up-gradation from Oracle 11g to Oracle 12c.
Installation of Database critical patches.
Taking cold and hot backups on scheduled times and monitoring backups.
Importing the export dump to another database as per demands.
Automating most of the daily activities through cronjobs, shell scripts or schedulers.
Making Plan of Actions for Various Activities.
Raising SR with Oracle Support for different severity issues.
Handling the Userâs request and proper client interaction.
Monitoring & managing database growth, tablespaces, adding ,resizing and renaming the datafiles.
Restoration of database using RMAN backups for backup consistency checks.
Migration of Database using export / import and RMAN backups.
Configuring & managing Physical Standby database.
Creating database links, Tablespaces, database directories.
Managing network settings through listener.ora and tnsnames.ora files.
Restoration of data using old logical backup as per client request.
Schema replication across databases through data pump tool.
Taking cold and hot backups on scheduled times and monitoring backups
Taking EXPDP of database, database objects and a particular schema
Using SCP ticketing tool in order keeping track of client requests.Â 
Performing Maintenance Activities such as Index Rebuilding and stats gather.
Troubleshooting the Basic LevelÂ performance issuesÂ 
Setting up a new environmentÂ from database perspective within the requested timelines
Adding/Deleting disks in ASM and monitoring the ASM diskgroups.
Creating users & privileges with appropriate roles and levels of security.Â 
Database Administrator working in 24*7 support environment maintaining Databases running on Oracle 11g, 12c.
Performing database online and offline database re-organization for database enhancement.Â 
Migrating database from Non-ASM to ASM file system.
Grid up-gradation from 11g to 12C.
company - Insolutions Global Ltd
description - Oracle software installation(graphical/silent),Database upgrade,Patch upgrade.
Maintaining around 80+ UAT DB servers, 40 production DB and 28 standby/DR DB.
Managing/creating DR & standby servers, DB sync.
Backup and recovery (RMAN/ Datapump).
Performing activities like switchover and failover .
Allocating system storage and planning future storage requirements for the database system
Enrolling users and maintaining system security.
Monitoring Alert log, Snap ID generation, db size, Server space, OEM reports, User validity.
Controlling and monitoring user access to the database .
Scheduling shell scripts or dbms_jobs using Crontab or DBMS_SCHEDULER (monitoring script, listener check, backup script, AWR reports) etc.
Planning for backup and recovery of database.
Managing the production database for Oracle and SQL Server and resize the space of database/Datafiles/Tablespace/Transactional Logs.
Managing Temp and Undo tablespaces.
Creating primary database storage structures (tablespaces) after application developers have designed an application."
Database,"TECHNICAL SKILLS â¢ SQL â¢ Oracle v10, v11, v12 â¢ R programming, Python, linear regression, machine learning and statistical modelling techniques(obtained certification through Edvancer Eduventures training institute) KEY SKILLS â¢ Multitasking, working to meet client SLA in high pressure scenarios, handling sensitive clients along with improved skills at being a team player. â¢ Excellent communication skills and quick learner. â¢ Leadership qualities, team networking and courage to take up the problems proactively.Education Details 
June 2012    Sadvidya Pre-University College
Application Database Administrator-DBMS (Oracle) 

Application Database Administrator-DBMS (Oracle) - IBM India Pvt Ltd
Skill Details 
CLIENTS- Exprience - 30 months
MACHINE LEARNING- Exprience - 30 months
ORACLE- Exprience - 30 months
SQL- Exprience - 30 months
EXCELLENT COMMUNICATION SKILLS- Exprience - 6 monthsCompany Details 
company - IBM India Pvt Ltd
description - Client: Blue Cross Blue Shield MA: Massachusetts Health Insurance
â¢   Used Oracle SQL to store and organize data. This includes capacity planning, installation, configuration, database
design, migration, security, troubleshooting, backup and data recovery.
â¢   Worked with client databases installed on Oracle v10, v11, v12 on a Linux platform.

â¢   Proficient communication with clients across locations facilitating data elicitation.

â¢   Handling numerous business requests and solving them diligently within the given time frame and responding quickly and effectively to production issues within SLA.

â¢   Leading a team in co ordination with business to conduct weekly checkouts of the database servers and systems

IBM Certifications
Statistics 101, Applied Data Science with R, Big Data Foundations, Data Science Foundations

Business Analytics Certification (Pune)
Worked on Retail and Banking projects, to design a predictive business model using machine learning techniques in
R programming for an efficient business and marketing strategy."
Database,"TECHNICAL EXPERTISE â¢ DB Languages: SQL â¢ Database Tools: SQL Server 2014/ 2017 Postgresql 9.5, 9.6, Oracle 11gR2 â¢ Operating Systems: Redhat Linux, Oracle Linux, Windows Server 2012/ 2016 OTHER TECHNICAL SKILLS ORACLE 11G R2 â¢ Proficient in Oracle Database Software Installation, Creation of Database using GUI/Silent DBCA, Architecture, File management, Space Management, User Management, Creating Roles and assigning Privileges/Roles in 11gR2 and troubleshooting them. â¢ Hands on experience Control files/Redolog/Archive/Undo Management â¢ Configuring Listener.ora/Tnsnames.ora file using Netmgr/netca â¢ Generating AWR reports, ADDM, ASH reports to diagnose the problems â¢ Database Backup, Cloning/Duplicate using hot & cold backups using RMAN. â¢ Knowledge in Flashback Technologies & Expdp/Impdp â¢ Implemented Oracle11gR2 RAC on Oracle Linux Platform and knowledge of services for troubleshooting RAC (CRSCTL, SRVCTL) â¢ Knowledge on installation and configuration of RAC. Add/Remove Nodes on RAC â¢ Configuration of physical standby database (Data guard) â¢ Successfully upgraded from 11.2.0.1 to 11.2.0.4 & PSU patching using O patch. STRENGTHS â¢ Good Communication skills. â¢ Self-confident and can adapt myself to all work environments. â¢ Enjoy responsibilities as lead and team player. â¢ Patient listener & quick learner. â¢ Capable of explaining issues & solving them.Education Details 
 B.E Computer Engineering Mumbai, Maharashtra Mumbai University
 Higher Secondary Certificate   Dr. DY Patil Jr College
Database Administrator 

Database Administrator - DBA in Marketplace Technologies Ltd
Skill Details 
DATABASE- Exprience - 61 months
BACKUPS- Exprience - 48 months
LINUX- Exprience - 48 months
MS SQL SERVER- Exprience - 48 months
SQL- Exprience - 48 monthsCompany Details 
company - DBA in Marketplace Technologies Ltd
description - Project Title: EBoss, Datafeed, MFDB, RTRMS, IndiaINX
company - Standard & Enterprise
description - Redhat Linux 7.4, Postgresql 9.5, 9.6
Duration: Feb 2017 - till date
Description: Bombay Stock Exchange BSE  is Asia's first & the Fastest Stock Exchange in world with the speed of 6 micro seconds and one of India's leading exchange groups provides an efficient and transparent market for trading in equity, currencies, debt instruments, derivatives, mutual funds. BSE SME is India's largest SME platform which has listed over 250 companies and continues to grow at a steady pace.

JOB ROLES & RESPONSIBILITIES
POSTGRESQL - â¢ Worked on Redhat Linux OS Cluster with Postgresql for High Availability (HA) using Pacemaker.
â¢ Coordinated with Developers/Linux teams for database knowledge and support.
â¢ Participated in implementation of new releases into production.
â¢ Installed /Configured Postgresql from source or packages on Redhat Linux servers.
â¢ Performed Postgresql Server Management tasks i.e. Backup & Restore, Configuration, Roles, Blockings, Tablespace creation and Troubleshooting.
â¢ Worked with Storage team for Disaster Recovery DR setup built on SAN using EMC technology â¢ Configured LDAP authentication & GSSAPI Authentication from Windows to Linux for Postgresql.
â¢ Configured logical replication for Database servers, hot standby Postgresql servers, faster database backup methods, schema and tablespace backups.
â¢ Configured maximum connections to database on Linux servers.
â¢ Installed tds_fdw from source for linked servers to connect to heterogeneous databases & other required extensions, backup configuration, PITR using base backups.

MSSQL - â¢ Day-to-day administration of live SQL Servers.
â¢ Participated in Live Primary Recovery PR & Disaster Recovery DR activities.
â¢ Participated in PR & DR mocks for new releases into production.
â¢ Configured Linked Servers, Transactional replication, Maintenance tasks like database backup & restore, recovery, scheduled jobs, maintenance plans.
â¢ Installed & Configured SQL server 2014, 2017 standalone and SQL Cluster servers.
â¢ Maintained the security of the database by providing appropriate SQL roles, logins and permissions to the users on demand.
â¢ Worked with teams on application rollouts, application issues and SQL server migrations.
â¢ Exposure in handling production system with skills and understand client's requirement.
â¢ Performed SQL Server service pack upgrades and hot fixes.
â¢ Handled multiple SQL Instances on Windows SQL Cluster environment built on  EMC SAN.
â¢ Worked on MSSQL DB clusters with active/active & active passive servers, Always-On Availability Groups (AAG) and HA/DR Setup.
â¢ Have experience on SAN and RAID levels and building and supporting SQL Cluster servers on SAN Environments.
company - BSE Bombay Stock Exchange
description - Environment: Windows server 2008 R2, 2012 R2, 2016 Enterprise & Standard,"
Database,"Technical Expertise Operating Systems Microsoft Window Server 2003/2008/2008 R2/2012 Database Technologies SQL Server, Sybase ASE Server, Oracle, MongoDB Monitoring and Ticketing Tools HP Service Manager 7.0/9.0, Solar winds DPA, JIRA and MongoDB OPS manager Web Server IIS 7.0 Database Tools SSMS, DBArtisan, Studio 3T, SnapShot Manager for SQL ServerEducation Details 
 B. Tech Computer Science Gulbarga, Karnataka PDACOE, Gulbarga, Autonomous Institution
Database Administrator II 

Database Administrator III - BNY Mellon International Operations (India) PVT. LTD
Skill Details 
Sql Dba- Exprience - Less than 1 year monthsCompany Details 
company - BNY Mellon International Operations (India) PVT. LTD
description - SQL Server :
ï	Installation, configuration of database servers using slipstream and setup all the maintenance jobs as per the standard policy on standalone as well as cluster environments with latest service packs
ï	Installation of SSRS, uploading of .rdls and assigning correct data sources to reports. Grant necessary access to users & developers on reporting website. Aware of SSIS and designing packages as well.
ï	Create and manage logins, users for database applications, assigning permissions as per requests, resolving user login issues.
ï	Migration of all SQL server 2005/2008 servers to higher versions.
ï	Setup of database refresh jobs on QA, DEV and UAT environments and fixing orphaned users.
ï	Troubleshoot performance related issues. 
ï	Part of multiple projects to work with developers and provide all required support for testing in QA, UAT & DEV environment. 
ï	Lead the DR tests for database team.
ï	Participate in database purge and archive activities.
ï	Writing codes for automating database administration tasks.
ï	Worked on automating DR tasks to start the agent jobs on multiple servers, restore databases for log shipped databases without manual intervention for online databases post DR activities.
ï	Provide support to vendor databases, follow up with the vendor calls and timely escalate to next level when there is no update in predefined timeline.
ï	Installation and configuration of smsql on windows server. Schedule jobs for creation and deletion of clones on sql server. Maintain backups using smsql.

MongoDB Server:
ï	Installation and configuration of MongoDB server.
ï	Creation of databases and collection.
ï	Creation new user and grant access using Ops manager.
ï	Monitor database servers using Ops manager.
Oracle & Sybase Server
ï	Managing and maintaining multiple instances of Databases on Linux and windows servers.
ï	Monitoring daily jobs includes backups, refresh and maintenance jobs.
company - Hewlett-Packard India Sales PVT. LTD. On the payroll of Softenger India PVT. LTD
description - ï	Installation of SQL Server on standalone as well as windows cluster environments with latest service packs
ï	SQL server installation using slipstream.
ï	Installation of reporting services
ï	Creating logins and users, assigning permissions as per requests.
ï	Security audit for all logins includes maintenance of unused and orphan user logins
ï	Create & Maintain daily and weekly jobs/maintenance plans includes backup, index rebuild/reorganize , update statistics and database consistency check
ï	Create linked servers and ensure connectivity between servers
ï	Monitor disk space proactively & Space management using data and log file shrinking
ï	Monitor blocking, deadlocks, open transactions and slow running queries during performance issues and highlight costly queries to developers.
ï	Configure alerts for deadlock and blocking to maintain performance
ï	Implementing high availability technologies like log shipping, AlwaysON, mirroring and its troubleshooting, also have knowledge on replication
ï	Successfully completed migration of Databases from one server to another
ï	Performing DR drills (Online/Offline) on quarterly basis
ï	Power shell scripting to monitor, restart SQL service and get Email alert for the service status.
ï	Maintain TNS entries for oracle client as per client requests.
ï	Interacting with customers for requirements
ï	Contacting customer to update the status of handling issues and service requests at every stage of resolution
ï	Managing proper escalation and notification matrix for all support levels"
Database,"TECHNICAL SKILLS Operating Systems MS Windows Server 2012/2008/XP Software and Tools MS LiteSpeed, Idera SQL Safe, SSMS, Upgrade Advisor, SQL Server Profiler, SCOM, Diagnostic Manager, Remedy, Jira, Infopacc, Tivoli TDP backup tool, SQL Pack DatabasesMS SQL Server 2016/2014/2012/ 2008 R2/ 2008, Oracle 10g, Netezza Microsoft azure Education Details 
 Masters of Science Computer Science Pune, Maharashtra Indira College, Pune University
Lead database administrator 

Microsoft Certified Professional with 11 years of experience in database administration on MS SQL Server 2016/2014/2012/2008 R2/ 2008
Skill Details 
MS SQL SERVER- Exprience - 110 months
Microsoft azure- Exprience - Less than 1 year months
Always on availabiity group- Exprience - Less than 1 year months
Database mirroring- Exprience - Less than 1 year months
Performance tuning- Exprience - Less than 1 year months
Log shipping- Exprience - Less than 1 year months
Installation , upgrade, migration and patching- Exprience - Less than 1 year monthsCompany Details 
company - Ensono
description - Employment transfer as a part of project acquisition to Ensono from Wipro.
SQL Server Database Administration
company - Wipro Technologies
description - Microsoft Certified Professional with 11 years of experience in database administration on MS SQL Server 2016/2014/2012/2008 R2/ 2008.
Experience with MS SQL Server 2016/2014/2012/2008 R2/ 2008 installation, upgrade, and administration
Microsoft Azure certified.
Have understanding of Azure VM, Azure Storage, Azure network, Azure AD and Azure SQL database.Â 
Incident management, change management and Problem management for SQL Server Database team.
Participating in meetings, conference calls with client, Service Delivery Manager and Application team for System improvements.
Participated in quarterly DR activity.
Involved in creation of SIP - Service Improvement Plans
Involved in handling of high severity issues and provided RCA for the same.
Worked on Always on availability groups, database mirroring, replication, clustering and log shipping.
Have basic understanding of Oracle and Netezza.
Provided on- call support during out of office hours and weekends.
Resource & shift management of 5 SQL DBAs from offshore in multi-client environment for Data center services.
Provided KT to team members, monitor and guide trainees.
company - Wipro Technologies
description - Responsibilities: â¢ MS SQL Server 2016/2014/2012/ 2008 R2/ 2008 installation, configuration, and administration.
â¢ Worked on Always on availability groups, log shipping, database mirroring and clustering.
â¢ Participated in  PCI scan report to perform installation of security hot fixes, service packs for SQL servers to remove vulnerability.
â¢ Participated in Holmes BOTS automation implementation of SQL Pack tool.
â¢ Worked on service requests, incidents and critical issues.
â¢ Involved in conference calls to provide DBA support for critical issues.
â¢ Performance tuning.
Environment: SQL Server 2016/2014/2012/2008R2/2008, Windows Server 2012/2008R2/2008
company - Mphasis
description - 
company - Mphasis
description - Responsibilities: â¢ MS SQL Server 2012/ 2008 R2/ 2008  installation, configuration, and administration.
â¢ Worked on Always on availability groups, log shipping, database mirroring and clustering.
â¢ Performed SQL server patching activity â¢ Worked on daily reports like cluster failover, backup, AG/LS/Mirror report and server disk space report.
â¢ Worked on service requests, incidents and critical issues.
â¢ Participated in quarterly DR activity.
â¢ Involved in conference calls to provide DBA support for critical issues.
â¢ Provided support to windows team during patching for AG-mirror-cluster failover/failback and database health check.
â¢ Performed all the health checks for market open servers and provided update in market open call â¢ Deeply involved in   resolution of the issue and finding the root cause analysis of the issue â¢ Performance tuning.
Environment: SQL Server 2012/2008R2/2008, Windows Server 2008R2/2008
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ SQL server, Oracle and Netezza databases support tasks.
â¢ MS SQL Server 2008 R2/ 2008 installation, upgrade, and administration.
â¢ Done capacity planning for database growth for all SQL servers.
â¢ Troubleshooting alerts.
â¢ Worked on log shipping and mirroring.
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008, Oracle 10g/RAC
company - Synechron Technologies Pvt. Ltd
description - 
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ Pursued in-depth training on Oracle 11g Architecture and SQL Server.
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008, Oracle 10g
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ Carried out version changes for schemas from PE8 version to EE11 version as per the process given
Environment: Oracle 11g
company - Mastek Ltd
description - Responsibilities: â¢ SQL Server 2008 R2/ 2008 installation, upgrade, and administration â¢ database backup/restore.
â¢ Performed MS SQL Server audits â¢ Worked with database mirroring, replication, log shipping and clustering.
â¢ Supported UAT and PROD environments â¢ Performed deployment document review.
â¢ Carried out deployments for different applications
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008
company - Mastek Ltd
description - 
company - PP Software and Systems Ltd
description - 
company - PP Software and Systems Ltd
description - Description: The system provides Master Data Management and Procurement modules for dairy industry.
Responsibilities: â¢ Designed, coded, and tested â¢ Customized ERP system as per the requirement
Environment: Core Java, PostgreSQL"
Database,"SKILLSET Oracle DBA, MySQL, MARIADB, PostgreSQL Database Administration ITSKILLS SQL Oracle 10g, 11g, MYSQL, MariaDB, postgreSQL Windows, Linux Putty Education Details 
January 2018 MCS  Pune, Maharashtra Pune University
Database administrator 

Database administrator  - Infiniteworx Omnichannel Pvt. Ltd
Skill Details 
DATABASE- Exprience - 17 months
MYSQL- Exprience - 17 months
ORACLE- Exprience - 17 months
SQL- Exprience - 17 months
DATABASE ADMINISTRATION- Exprience - 6 monthsCompany Details 
company - Infiniteworx Omnichannel Pvt. Ltd
description - Pune Sept 2017 to Present

RESPONSIBILITIES:
â¢ Creating tablespaces and planning the location of data, monitoring the tablespaces growth periodically.
â¢ All replication setup
â¢ Moved database Schema changes to stage.
â¢ Dba support query resolution.
â¢ Creating user and giving specific privileges
â¢ Database management.
â¢ Database recovery, moving data files to different locations.
â¢ Planning the backup policies and Backup/ Recovery of databases based on the criticality.
â¢ IMPORT/EXPORT.
â¢ Degine schemas

Key Result Areas:

â¢ Providing 24 /7 support to resolve database performance issues, Job failures, Sessions & diagnose root causes
â¢ Installation, configuring and updating Oracle server software and related Oracle products. Installation, configuraing and updating Mysql, Sql server, MariaDB, MongoDB
â¢ Supported multiple databases and administered Oracle Databases of Large DB Sizes for production, development & test setups.

â¢ Maintaining table spaces & data files, Control files, Online Redo log files

â¢ Creating Users, granting Roles & Privileges to users and managing tablespaces for different users by granting quota on Default & Temporary tablespaces.

â¢ Taking Oracle RMAN Backups (Scheduling for day wise backup)

â¢ Implementing the incremental, cumulative and full RMAN backup for each database to have space management and effective recovery.
â¢ Logical Backup Using Export & Import/datapump Export of important tables at regular intervals.

â¢ Regular checking of trace, alert log file, all ORA errors

â¢ Working on incidents like User creation/deletion incidents, backup failed incidents.
â¢ Checking Listener Status, connectivity Troubleshooting and fixing database listener issues.
â¢ Look for any new alert / error log entries / errors, error details in Trace files generated. Executing DDL & DML scripts as per customer requirements

â¢ Mentoring, coaching and appraising team members with active involvement in the recruitment process

â¢ Contributing in Project Documentation and generating daily reports

â¢ Ensuring compliance to quality norms and taking steps for any non-conformance Spearheading complete project activities ensuring timely completion of project
â¢ Implementing security policies on different database systems with granting and revoking privileges to the users

â¢ Following change management processes and participated in related meetings

â¢ Verifying all Instances/DB are running, Tablespaces are online, Monitor Backround processes and status.
company - InnovativeTechnologies
description - Clients: BANKING DOMAIN"
Database,"Education Details 
January 2016 BSc.  Mumbai, Maharashtra Mumbai University
January 2013 H.S.C.   Maharashtra Board
January 2011 S.S.C.   Maharashtra Board
MySQL Database Administrator 

2+ Years of experience in MySQL Database Administrator ( MySQL DBA)
Skill Details 
MySQL DBA , Centos , Backup , Restore , Replication , Query Optimazation- Exprience - 24 monthsCompany Details 
company - Trimax IT Infrastructure & Services Ltd
description - Â·Â Â Â Â Â Â Â MYSQL Installation, maintenance and Upgrades (Version 5.5 , 5.6)
Â·Â Â Â Â Â Â Â MySQL database administration on a large scale MySQL installation
Â·Â Â Â Â Â Â Â Experience with MySQL on both Linux and Windows
Â·Â Â Â Â Â Â Â MySQL processes, security management and queries optimization.
Â·Â Â Â Â Â Â Â Performed query analysis for slow and problematic queries.
Â·Â Â Â Â Â Â Â Performed Structural changes to Database like creating tables, adding columns according to business requirement
Â·Â Â Â Â Â Â Â Creating and MaintainingÂ Database Maintenance Plans.



Â·Â Â Â Â Â Â Â Writing scripts to Create Jobs for Backup & Restore Plans.
Â·Â Â Â Â Â Â Â Working on MYISAM to INNODB engine.
Â·Â Â Â Â Â Â Â Working on Server shifting , tuning parameter , database purging
Â·Â Â Â Â Â Â Â Working on Mysql master slave Replication
Â·Â Â Â Â Â Â Â Handling Release management and user acceptance.
Â·Â Â Â Â Â Â Â Restore using xtrabackup.
Â·Â Â Â Â Â Â Â Responsibilities include monitoring daily, weekly and monthly system maintenance tasks such as database backup, replication verification, database integrity verification and indexing updates
Â·Â Â Â Â Â Â Â Work in 24/7 production database support.
company - Trimax IT Infrastructure & Services Ltd
description - Â·Â Â Â Â Â Â Â MYSQL Installation, maintenance and Upgrades (Version 5.5 , 5.6)
Â·Â Â Â Â Â Â Â MySQL database administration on a large scale MySQL installation
Â·Â Â Â Â Â Â Â Experience with MySQL on both Linux and Windows
Â·Â Â Â Â Â Â Â MySQL processes, security management and queries optimization.
Â·Â Â Â Â Â Â Â Performed query analysis for slow and problematic queries.
Â·Â Â Â Â Â Â Â Performed Structural changes to Database like creating tables, adding columns according to business requirement
Â·Â Â Â Â Â Â Â Creating and MaintainingÂ Database Maintenance Plans.



Â·Â Â Â Â Â Â Â Writing scripts to Create Jobs for Backup & Restore Plans.
Â·Â Â Â Â Â Â Â Working on MYISAM to INNODB engine.
Â·Â Â Â Â Â Â Â Working on Server shifting , tuning parameter , database purging
Â·Â Â Â Â Â Â Â Working on Mysql master slave Replication
Â·Â Â Â Â Â Â Â Handling Release management and user acceptance.
Â·Â Â Â Â Â Â Â Restore using xtrabackup.
Â·Â Â Â Â Â Â Â Responsibilities include monitoring daily, weekly and monthly system maintenance tasks such as database backup, replication verification, database integrity verification and indexing updates
Â·Â Â Â Â Â Â Â Work in 24/7 production database support."
Database,"TECHNICAL SKILL: Operating System LINUX, Windows Server 2012 R2, Windows 98, Windows 2000/ XP Tools & Utility Packages SQL* Loader, SQL*PLUS, OEM, Datapump, expdp/impdp, PLSQL Developer, Jenkins Database Oracle 10g, Oracle 11g, Oracle 12c Scripting UNIX Shell Scripting Language SQL Education Details 
January 2011 M.B.A.  Amravati, Maharashtra Amravati University
January 2007 B.C.A.  Nagpur, Maharashtra Nagpur University
Oracle Database Administrator 

ORACLE DATABASE ADMINISTRATOR ON LINUX/MICROSOFT WITH 4 YEARS EXPERIENCE.
Skill Details 
ORACLE- Exprience - 48 months
LINUX- Exprience - 6 months
ORACLE DBA- Exprience - Less than 1 year months
RAC- Exprience - Less than 1 year months
GOLDEN GATE- Exprience - Less than 1 year months
ASM- Exprience - Less than 1 year months
DATAGUARD- Exprience - Less than 1 year monthsCompany Details 
company - TIETO INDIA PVT. LTD
description - Pune From February 2015 till present

Project Profile:
Oil and Gas unit of Tieto India Pvt. Ltd. is working for Environmental Components (EC) application. Tieto is the authorized service provider in EC. Energy Components is a complete end-to-end hydrocarbon accounting solution following the hydrocarbons from production to transport, sales and revenue recognition. Globally market-leading hydrocarbon accounting software with functionality coverage exceeding other available solutions. Modern, flexible and scalable technology platform. Selected as the global standard and best practice by oil & gas super majors.
Responsibilities: â¢ Oracle Database Administration 11g R2, 12c and 18c â¢ Supporting databases in 24x7 environments and coordinate with Application, OS, Storage and Development Teams. Test and Production environments â¢ Regularly monitoring the trace files and Alert log files for database related issues.
â¢ Experience in monitoring the CPU usage, IO and memory utilization at OS level.
â¢ Checking the Alert log file to analyze the ORA errors if any to raise SR with Oracle.
â¢ Monitoring the log files, backups, database space usage and the use of system resources.
â¢ Configuring Backup (RMAN) for database and restoring database.
â¢ Installation, configuring and updating Oracle server software and related Oracle products of 11g and 12C.
â¢ Oracle Server installation, client installation and configuration, PLSQL developer installation.
â¢ Creating database using DBCA and manually.
â¢ Creating of Oracle user and granting proper privileges to user as per request.
â¢ Creating AWR, ASH and ADDM reports for database performance analysis.
â¢ Handling space management and performance issues in Oracle databases.
â¢ Creating remote database link.
â¢ Renaming and resizing of data files in Oracle database if needed.
â¢ Tablespace shrinking with regular time interval to reclaim server space.
â¢ Expertise in Export and Import using data pump in Oracle database.
â¢ Expertise in Configuration of Listener and Tnsnames through NETMGR and NETCA and statically also.
â¢ Managing Oracle Listener and Oracle Network Files.
â¢ Creating user Profiles, granting specific privileges and roles to the users in Oracle database.
â¢ Maintaining tablespaces & data files, Control files, Online Redo log files in Oracle database.
â¢ Worked on AWS cloud services like EC2, S3, RDS, ELB, EBS, VPC, Route53, Auto, Cloud watch, Cloud Front, IAM for installing configuring and troubleshooting on various Amazon images for server migration from physical into cloud."
Database,"Technical Skills Databases: Oracle RDBMS- 10g, 11g & 12c Technology/utilities: Data Pump, RMAN, Data guard, ASM, RAC, Golden Gate Tools: OCC, PUTTY, SQLPLUS, SQL Developer, Netbackup, SCOM, SCCM, VMWare Vsphere Operating Systems: RHEL 6.0, RHEL 6.5, UNIX and Microsoft WindowsEducation Details 

Database Administrator 

Database Administrator - BNY Mellon
Skill Details 
DATABASES- Exprience - 24 months
ORACLE- Exprience - 24 months
RMAN- Exprience - 24 months
NETBACKUP- Exprience - 24 months
SCOM- Exprience - 24 monthsCompany Details 
company - BNY Mellon
description - Databases: 600+
Team Size: 8
Duration: Jan 2017 - Till Date
Clients: Over 130+ investment banking organizations who are hosted with Eagle

Responsibilities: Database Management (Support and managing critical production, Pre-production, test and reporting databases in different platforms), Capacity Management Upgrades.
â¢ Handling day to day database activities monitoring and incident management.
â¢ Building new databases as per the requirement and prepare them for go live with the help of multiple teams.
â¢ Working on scheduled activity of database patching (CPU, PSU) â¢ Installing latest path on production, Dev and Test databases as per the suggestion from Oracle support.
â¢ Database Upgrade from 11g and to 12c.
â¢ Adding disks to ASM disk groups.
â¢ Building DR database using Active Data guard, Make it sync with prod and resolving issues if persists â¢ Data Guard Management- Checking lagging status, removing lagging of archives, checking processes like RFS/MRP, Archives Management â¢ Working on tablespace related issues â¢ Managing user access and profiles â¢ Importing and exporting using datapump â¢ Maintaining inventory of all databases in the single centralize database â¢ Refreshing test environment from production database.
â¢ Working with Oracle Support to resolve oracle errors.
â¢ Schedule daily and weekly databases backup using RMAN, Troubleshooting in RMAN issues.
â¢ Database cloning using RMAN.
â¢ Take part in cutover to upgrade application to higher version.
â¢ Strictly following ITIL process in incident management and change management.
â¢ Providing weekly report of issues in team meeting also participating and suggesting service improvement plans.
â¢ Database Migrations from one server to another or to different platforms â¢ RCA and impact analysis reporting during any production outage.

Previous Organization: Brose India
Project I: Central IT Management
company - 
description - Responsibilities: Managing our internal databases and servers of Brose global.
â¢ Providing 24x7 on-call support in the rotational shifts.
â¢ Performing day-to-day database activity â¢ Monitoring and responding DBA group Mails for all alerts, issues and ad-hoc business user requests, etc.
â¢ Database creation, patching â¢ Backup of Database in frequent cycles using Data pump/RMAN.
â¢ Database refreshes using RMAN, Datapump.
â¢ Recovery using copy of data / RMAN â¢ Monitoring logs and trace for resolving issues.
â¢ Creating new VM servers and prepare it for go live, Also decommissioning as per requirements.
â¢ Weekly patching of windows servers using SCCM and taking actions for patching if needed â¢ Monitoring and troubleshooting of daily and weekly OS backup using Symantec Netbackup â¢ Managing user accounts of OS users and database users â¢ Monitoring OS level alerts using SCOM

Project II: Data Center Migration (Onsite Project)
Responsibilities: Data center migration was one of activity for migration of our datacenter from one location to another. Where our all servers and databases were moved successfully.
â¢ Installation of Oracle 11g on Linux platforms â¢ Worked on service requests (Incidents / Change / Request) â¢ Creation of users, managing user privileges â¢ Configured RMAN backup for databases â¢ Patching of databases â¢ Configuring physical standby database using Dataguard â¢ Cloning of servers and migrate to another cluster

ACADEMIA / PERSONAL DETAILS â¢ Bachelor of Engineering (B.E.) in Computer Science and Engineering From SGBAU Amravati University, Amravati in 2014 with CGPA of 7.21

Current Address:-       Mr. Yogesh Tikhat, C/O: Raj Ahmad, Flat# G2-702, Dreams Aakruti, Kalepadal, Hadapsar, Pune - 411028
Highest Qualification   BE (cse)

PAN: -                  AOFPT5052C"
Database,"Software Skills: * RDBMS: MS SQL SERVER 2000/2005/2008 & 2012, 2014 * Operating Systems: WINDOWS XP/7, WINDOWS SERVER 2008, 12 * Fundamentals: MS Office 03/07 * Tools: SSMS, Performance Monitor, Sql profiler, SQL lite speed. Company name: Barclays Technology Centre India. Team Size: 24 Role: Database Administrator Support Description: Barclays Technology is a UK based retail & invest bank and 300 years of old bank.. It has operations in over 40 countries and employs approximately 120, 000 people. Barclays is organised into four core businesses: Personal & Corporate (Personal Banking, Corporate Banking, Wealth & Investment Management), Barclaycard, Investment Banking. Responsibilities: â Attending various calls from all over the world on various database issues. â Working on Web Gui alerts and resolving incident tickets within the time lines. â Troubleshoooting log shipping issues and fixing the related alerts. â Identifying and Resolving Blocking and locking related issues. â Configuration and monitoring Replication, Log shipping and mirroring setup. â Working on replication issues and Always ON issue. â Granting and revoking permissions on various account provisioning tasks. â Working on call support during the weekend and performing DR test's. and working on weekly maintenance jobs and weekend change requests. Education Details 
 B.Sc. Maths  Kakatiya University Board secured
SQL server database administrator 

Database administrator
Skill Details 
DATABASE- Exprience - 120 months
DATABASE ADMINISTRATOR- Exprience - 72 months
MAINTENANCE- Exprience - 48 months
MS SQL SERVER- Exprience - 48 months
REPLICATION- Exprience - 48 monthsCompany Details 
company - Barclays global services centre
description - SQL server databases implementation and maintenances

Log shipping, replication, High availability, clustering, performance tuning, database mirroring, Installation, configuration, upgradation, migration
company - Wipro Infotech Pvt Ltd
description - SQL server database administrator
company - CITI Bank
description - Worked as Database Support at Accord Fintech, Sanpada from Sep 2008 to 2013 Feb.
company - 
description - 2012.
â¢     Sound knowledge in Database Backup, Restore, Attach, and Detach and Disaster Recovery procedures.
â¢     Developed backup and recovery strategies for production environment.
â¢     Ensuring data consistency in the database through DBCC and DMV commands.
â¢     Experience in query tuning and stored procedures and troubleshooting performance issues.
â¢     Having hands on experience in DR process including log shipping and database mirroring.
â¢     Experience in scheduling   monitoring of Jobs.
â¢     Experience in configure and troubleshooting in Always ON.
â¢     Creating and Maintaining of Maintenance Plan.
â¢     Expertise in planning and implementing MS SQL Server Security and Database permissions.
â¢     Clear understanding of Implementation of Log Shipping, Replication and mirroring of databases between the servers.
â¢     Performance Tuning (Performance Monitor, SQL Profiler Query Analyzer) â¢     Security for server & Databases Implementing security by creating roles/users,
Added users in roles, assigning rights to roles.
â¢     Create and maintaining the linked servers between sql Instances.
â¢     Create and maintaining and Database mail.
â¢     Monitor and troubleshoot database issues.
â¢     Creating DTS packages for executing the required tasks.
â¢     Experts in create indexes, Maintaining indexes and rebuilds and reorganizes.
â¢     Daily Maintenance of SQL Servers included reviewing
SQL error logs, Event Viewer."
Database,"Areas of Expertise â¢ Oracle Databases 12c, 11g, 10g â¢ Weblogic 12c, 11g â¢ Grid Infrastructure â¢ RMAN â¢ ASM â¢ Middleware: OIM, OAM, SOA â¢ Shell Scripts â¢ DataGuard â¢ Web servers - OHS, Apache â¢ Architecture Designs â¢ Proof of Concepts â¢ DevOpsEducation Details 
January 2007 Bachelor of Engineering Information Technology Sangli, Maharashtra Walchand College
January 2004 Diploma Computer Engineering Jalgaon, Maharashtra Govt. Polytechnic
Lead Database Administrator 

Lead Database Administrator - Tieto Software
Skill Details 
DATABASES- Exprience - 108 months
MIDDLEWARE- Exprience - 96 months
RMAN- Exprience - 84 months
SHELL SCRIPTS- Exprience - 48 monthsCompany Details 
company - Tieto Software
description - As a part of AO (Application Operations) team, scope in project is quite wide than typical database administration. Range of accomplishments are extended right from Data Tier to Middle Tier & Application Tier:
- Maximized availability of applications from 99.3% to 99.8%
- Raised business by presenting Proof of Concepts for 10+ cases
- Delivered upgrades of various applications time to time to keep it on supported platform
- Saved SLAs as per contract by means of handling P1, P2 issues effectively
- Produced Capacity reports comprising all layers (Data, Middleware, Web) of various applications
- Generated Work Orders as per customer need
company - Tieto Software
description - - Designed databases of various applications
- Planned RMAN backup and recovery, BCP strategies
- Executed Business Continuity Testing for various applications
- Introduced Zero Cost high availability solutions - Active-Passive Failover
- Optimized performance by means of scripting automation
- Established cloning procedures for all layers of various applications
- Delivered Infrastructure changes, like FW Openings & LoadBalancer configuration for new applications
- Eliminated downtime by troubleshoot issues for Middleware products - OIM, OAM, SOA
- Contributed to build & maintain Integration Layer- SMTP, ftp, Reverse Proxy, OCM
company - Tieto Software
description - - Provided database support to environments - PROD, UAT, TEST, DEV
- Performed Database Refresh/Cloning from production to development and support databases
- Reduced risk level by means of upgrading & patching databases time to time
- Protected databases by assigning appropriate roles and privileges as per SOD
- Generated & maintained Middleware schemas using RCU
- Exceeded scope of work by supporting & maintaining WebLogic platform - installation, patching, troubleshooting issues
- Expanded duty scope to web servers: Install & maintain- OHS, apache, tomcat
company - HSBC Software
description - Being part of project supporting HSBC Bank France, I achieved:
- Handled incidents & service requests as Day to day database administration tasks
- Delivered basic implementation services - Database installation, patching, upgrades
- Performed capacity planning - managing tablespaces, compressions
- Contributed in maintaining quality of databases - managing instances, indexes, re-organization, performance monitoring & tuning using AWR, ADDM reports
- Maintained backups & recovery of database - logical backups (exp/imp), datapump (expdp/impdp), cold backups, hot backups, RMAN backup/restore, RMAN Duplication
- Reduced efforts by automation - Value add initiatives which includes writing shell scripts for automated housekeeping operations, scheduling backups, use crontab/at to schedule tasks
- Implemented high availability solutions - Dataguard"
Database,"Education Details 
May 2011 to May 2014 Bachelor of science Information technology Mumbai, Maharashtra Mumbai university
Oracle DBA 

Oracle database administrator
Skill Details 
Installation of Oracle on RH Linux & Windows. Creating/Managing user profiles and analyzing their privileges and tablespace quotas Backup of database Logical and Physical procedures. Recovery of database in case of database crash, disk/media failure, etc. Standard DBA functions like space management, Rollback segments, Extents. Database Management and Monitoring the database. Willing to learn new things. Being a constructive team member, contributing practically to the success of the team.- Exprience - 48 monthsCompany Details 
company - Accelya kale solutions ltd
description - Database Administrator working in 24*7 support environment maintaining Databases running on Oracle 11g, 12c.
Database Up-gradation from Oracle 11g to Oracle 12c.
Installation of Database critical patches.
Taking cold and hot backups on scheduled times and monitoring backups.
Importing the export dump to another database as per demands.
Automating most of the daily activities through cronjobs, shell scripts or schedulers.
Making Plan of Actions for Various Activities.
Raising SR with Oracle Support for different severity issues.
Handling the Userâs request and proper client interaction.
Monitoring & managing database growth, tablespaces, adding ,resizing and renaming the datafiles.
Restoration of database using RMAN backups for backup consistency checks.
Migration of Database using export / import and RMAN backups.
Configuring & managing Physical Standby database.
Creating database links, Tablespaces, database directories.
Managing network settings through listener.ora and tnsnames.ora files.
Restoration of data using old logical backup as per client request.
Schema replication across databases through data pump tool.
Taking cold and hot backups on scheduled times and monitoring backups
Taking EXPDP of database, database objects and a particular schema
Using SCP ticketing tool in order keeping track of client requests.Â 
Performing Maintenance Activities such as Index Rebuilding and stats gather.
Troubleshooting the Basic LevelÂ performance issuesÂ 
Setting up a new environmentÂ from database perspective within the requested timelines
Adding/Deleting disks in ASM and monitoring the ASM diskgroups.
Creating users & privileges with appropriate roles and levels of security.Â 
Database Administrator working in 24*7 support environment maintaining Databases running on Oracle 11g, 12c.
Performing database online and offline database re-organization for database enhancement.Â 
Migrating database from Non-ASM to ASM file system.
Grid up-gradation from 11g to 12C.
company - Insolutions Global Ltd
description - Oracle software installation(graphical/silent),Database upgrade,Patch upgrade.
Maintaining around 80+ UAT DB servers, 40 production DB and 28 standby/DR DB.
Managing/creating DR & standby servers, DB sync.
Backup and recovery (RMAN/ Datapump).
Performing activities like switchover and failover .
Allocating system storage and planning future storage requirements for the database system
Enrolling users and maintaining system security.
Monitoring Alert log, Snap ID generation, db size, Server space, OEM reports, User validity.
Controlling and monitoring user access to the database .
Scheduling shell scripts or dbms_jobs using Crontab or DBMS_SCHEDULER (monitoring script, listener check, backup script, AWR reports) etc.
Planning for backup and recovery of database.
Managing the production database for Oracle and SQL Server and resize the space of database/Datafiles/Tablespace/Transactional Logs.
Managing Temp and Undo tablespaces.
Creating primary database storage structures (tablespaces) after application developers have designed an application."
Database,"TECHNICAL SKILLS â¢ SQL â¢ Oracle v10, v11, v12 â¢ R programming, Python, linear regression, machine learning and statistical modelling techniques(obtained certification through Edvancer Eduventures training institute) KEY SKILLS â¢ Multitasking, working to meet client SLA in high pressure scenarios, handling sensitive clients along with improved skills at being a team player. â¢ Excellent communication skills and quick learner. â¢ Leadership qualities, team networking and courage to take up the problems proactively.Education Details 
June 2012    Sadvidya Pre-University College
Application Database Administrator-DBMS (Oracle) 

Application Database Administrator-DBMS (Oracle) - IBM India Pvt Ltd
Skill Details 
CLIENTS- Exprience - 30 months
MACHINE LEARNING- Exprience - 30 months
ORACLE- Exprience - 30 months
SQL- Exprience - 30 months
EXCELLENT COMMUNICATION SKILLS- Exprience - 6 monthsCompany Details 
company - IBM India Pvt Ltd
description - Client: Blue Cross Blue Shield MA: Massachusetts Health Insurance
â¢   Used Oracle SQL to store and organize data. This includes capacity planning, installation, configuration, database
design, migration, security, troubleshooting, backup and data recovery.
â¢   Worked with client databases installed on Oracle v10, v11, v12 on a Linux platform.

â¢   Proficient communication with clients across locations facilitating data elicitation.

â¢   Handling numerous business requests and solving them diligently within the given time frame and responding quickly and effectively to production issues within SLA.

â¢   Leading a team in co ordination with business to conduct weekly checkouts of the database servers and systems

IBM Certifications
Statistics 101, Applied Data Science with R, Big Data Foundations, Data Science Foundations

Business Analytics Certification (Pune)
Worked on Retail and Banking projects, to design a predictive business model using machine learning techniques in
R programming for an efficient business and marketing strategy."
Database,"TECHNICAL EXPERTISE â¢ DB Languages: SQL â¢ Database Tools: SQL Server 2014/ 2017 Postgresql 9.5, 9.6, Oracle 11gR2 â¢ Operating Systems: Redhat Linux, Oracle Linux, Windows Server 2012/ 2016 OTHER TECHNICAL SKILLS ORACLE 11G R2 â¢ Proficient in Oracle Database Software Installation, Creation of Database using GUI/Silent DBCA, Architecture, File management, Space Management, User Management, Creating Roles and assigning Privileges/Roles in 11gR2 and troubleshooting them. â¢ Hands on experience Control files/Redolog/Archive/Undo Management â¢ Configuring Listener.ora/Tnsnames.ora file using Netmgr/netca â¢ Generating AWR reports, ADDM, ASH reports to diagnose the problems â¢ Database Backup, Cloning/Duplicate using hot & cold backups using RMAN. â¢ Knowledge in Flashback Technologies & Expdp/Impdp â¢ Implemented Oracle11gR2 RAC on Oracle Linux Platform and knowledge of services for troubleshooting RAC (CRSCTL, SRVCTL) â¢ Knowledge on installation and configuration of RAC. Add/Remove Nodes on RAC â¢ Configuration of physical standby database (Data guard) â¢ Successfully upgraded from 11.2.0.1 to 11.2.0.4 & PSU patching using O patch. STRENGTHS â¢ Good Communication skills. â¢ Self-confident and can adapt myself to all work environments. â¢ Enjoy responsibilities as lead and team player. â¢ Patient listener & quick learner. â¢ Capable of explaining issues & solving them.Education Details 
 B.E Computer Engineering Mumbai, Maharashtra Mumbai University
 Higher Secondary Certificate   Dr. DY Patil Jr College
Database Administrator 

Database Administrator - DBA in Marketplace Technologies Ltd
Skill Details 
DATABASE- Exprience - 61 months
BACKUPS- Exprience - 48 months
LINUX- Exprience - 48 months
MS SQL SERVER- Exprience - 48 months
SQL- Exprience - 48 monthsCompany Details 
company - DBA in Marketplace Technologies Ltd
description - Project Title: EBoss, Datafeed, MFDB, RTRMS, IndiaINX
company - Standard & Enterprise
description - Redhat Linux 7.4, Postgresql 9.5, 9.6
Duration: Feb 2017 - till date
Description: Bombay Stock Exchange BSE  is Asia's first & the Fastest Stock Exchange in world with the speed of 6 micro seconds and one of India's leading exchange groups provides an efficient and transparent market for trading in equity, currencies, debt instruments, derivatives, mutual funds. BSE SME is India's largest SME platform which has listed over 250 companies and continues to grow at a steady pace.

JOB ROLES & RESPONSIBILITIES
POSTGRESQL - â¢ Worked on Redhat Linux OS Cluster with Postgresql for High Availability (HA) using Pacemaker.
â¢ Coordinated with Developers/Linux teams for database knowledge and support.
â¢ Participated in implementation of new releases into production.
â¢ Installed /Configured Postgresql from source or packages on Redhat Linux servers.
â¢ Performed Postgresql Server Management tasks i.e. Backup & Restore, Configuration, Roles, Blockings, Tablespace creation and Troubleshooting.
â¢ Worked with Storage team for Disaster Recovery DR setup built on SAN using EMC technology â¢ Configured LDAP authentication & GSSAPI Authentication from Windows to Linux for Postgresql.
â¢ Configured logical replication for Database servers, hot standby Postgresql servers, faster database backup methods, schema and tablespace backups.
â¢ Configured maximum connections to database on Linux servers.
â¢ Installed tds_fdw from source for linked servers to connect to heterogeneous databases & other required extensions, backup configuration, PITR using base backups.

MSSQL - â¢ Day-to-day administration of live SQL Servers.
â¢ Participated in Live Primary Recovery PR & Disaster Recovery DR activities.
â¢ Participated in PR & DR mocks for new releases into production.
â¢ Configured Linked Servers, Transactional replication, Maintenance tasks like database backup & restore, recovery, scheduled jobs, maintenance plans.
â¢ Installed & Configured SQL server 2014, 2017 standalone and SQL Cluster servers.
â¢ Maintained the security of the database by providing appropriate SQL roles, logins and permissions to the users on demand.
â¢ Worked with teams on application rollouts, application issues and SQL server migrations.
â¢ Exposure in handling production system with skills and understand client's requirement.
â¢ Performed SQL Server service pack upgrades and hot fixes.
â¢ Handled multiple SQL Instances on Windows SQL Cluster environment built on  EMC SAN.
â¢ Worked on MSSQL DB clusters with active/active & active passive servers, Always-On Availability Groups (AAG) and HA/DR Setup.
â¢ Have experience on SAN and RAID levels and building and supporting SQL Cluster servers on SAN Environments.
company - BSE Bombay Stock Exchange
description - Environment: Windows server 2008 R2, 2012 R2, 2016 Enterprise & Standard,"
Database,"Technical Expertise Operating Systems Microsoft Window Server 2003/2008/2008 R2/2012 Database Technologies SQL Server, Sybase ASE Server, Oracle, MongoDB Monitoring and Ticketing Tools HP Service Manager 7.0/9.0, Solar winds DPA, JIRA and MongoDB OPS manager Web Server IIS 7.0 Database Tools SSMS, DBArtisan, Studio 3T, SnapShot Manager for SQL ServerEducation Details 
 B. Tech Computer Science Gulbarga, Karnataka PDACOE, Gulbarga, Autonomous Institution
Database Administrator II 

Database Administrator III - BNY Mellon International Operations (India) PVT. LTD
Skill Details 
Sql Dba- Exprience - Less than 1 year monthsCompany Details 
company - BNY Mellon International Operations (India) PVT. LTD
description - SQL Server :
ï	Installation, configuration of database servers using slipstream and setup all the maintenance jobs as per the standard policy on standalone as well as cluster environments with latest service packs
ï	Installation of SSRS, uploading of .rdls and assigning correct data sources to reports. Grant necessary access to users & developers on reporting website. Aware of SSIS and designing packages as well.
ï	Create and manage logins, users for database applications, assigning permissions as per requests, resolving user login issues.
ï	Migration of all SQL server 2005/2008 servers to higher versions.
ï	Setup of database refresh jobs on QA, DEV and UAT environments and fixing orphaned users.
ï	Troubleshoot performance related issues. 
ï	Part of multiple projects to work with developers and provide all required support for testing in QA, UAT & DEV environment. 
ï	Lead the DR tests for database team.
ï	Participate in database purge and archive activities.
ï	Writing codes for automating database administration tasks.
ï	Worked on automating DR tasks to start the agent jobs on multiple servers, restore databases for log shipped databases without manual intervention for online databases post DR activities.
ï	Provide support to vendor databases, follow up with the vendor calls and timely escalate to next level when there is no update in predefined timeline.
ï	Installation and configuration of smsql on windows server. Schedule jobs for creation and deletion of clones on sql server. Maintain backups using smsql.

MongoDB Server:
ï	Installation and configuration of MongoDB server.
ï	Creation of databases and collection.
ï	Creation new user and grant access using Ops manager.
ï	Monitor database servers using Ops manager.
Oracle & Sybase Server
ï	Managing and maintaining multiple instances of Databases on Linux and windows servers.
ï	Monitoring daily jobs includes backups, refresh and maintenance jobs.
company - Hewlett-Packard India Sales PVT. LTD. On the payroll of Softenger India PVT. LTD
description - ï	Installation of SQL Server on standalone as well as windows cluster environments with latest service packs
ï	SQL server installation using slipstream.
ï	Installation of reporting services
ï	Creating logins and users, assigning permissions as per requests.
ï	Security audit for all logins includes maintenance of unused and orphan user logins
ï	Create & Maintain daily and weekly jobs/maintenance plans includes backup, index rebuild/reorganize , update statistics and database consistency check
ï	Create linked servers and ensure connectivity between servers
ï	Monitor disk space proactively & Space management using data and log file shrinking
ï	Monitor blocking, deadlocks, open transactions and slow running queries during performance issues and highlight costly queries to developers.
ï	Configure alerts for deadlock and blocking to maintain performance
ï	Implementing high availability technologies like log shipping, AlwaysON, mirroring and its troubleshooting, also have knowledge on replication
ï	Successfully completed migration of Databases from one server to another
ï	Performing DR drills (Online/Offline) on quarterly basis
ï	Power shell scripting to monitor, restart SQL service and get Email alert for the service status.
ï	Maintain TNS entries for oracle client as per client requests.
ï	Interacting with customers for requirements
ï	Contacting customer to update the status of handling issues and service requests at every stage of resolution
ï	Managing proper escalation and notification matrix for all support levels"
Database,"TECHNICAL SKILLS Operating Systems MS Windows Server 2012/2008/XP Software and Tools MS LiteSpeed, Idera SQL Safe, SSMS, Upgrade Advisor, SQL Server Profiler, SCOM, Diagnostic Manager, Remedy, Jira, Infopacc, Tivoli TDP backup tool, SQL Pack DatabasesMS SQL Server 2016/2014/2012/ 2008 R2/ 2008, Oracle 10g, Netezza Microsoft azure Education Details 
 Masters of Science Computer Science Pune, Maharashtra Indira College, Pune University
Lead database administrator 

Microsoft Certified Professional with 11 years of experience in database administration on MS SQL Server 2016/2014/2012/2008 R2/ 2008
Skill Details 
MS SQL SERVER- Exprience - 110 months
Microsoft azure- Exprience - Less than 1 year months
Always on availabiity group- Exprience - Less than 1 year months
Database mirroring- Exprience - Less than 1 year months
Performance tuning- Exprience - Less than 1 year months
Log shipping- Exprience - Less than 1 year months
Installation , upgrade, migration and patching- Exprience - Less than 1 year monthsCompany Details 
company - Ensono
description - Employment transfer as a part of project acquisition to Ensono from Wipro.
SQL Server Database Administration
company - Wipro Technologies
description - Microsoft Certified Professional with 11 years of experience in database administration on MS SQL Server 2016/2014/2012/2008 R2/ 2008.
Experience with MS SQL Server 2016/2014/2012/2008 R2/ 2008 installation, upgrade, and administration
Microsoft Azure certified.
Have understanding of Azure VM, Azure Storage, Azure network, Azure AD and Azure SQL database.Â 
Incident management, change management and Problem management for SQL Server Database team.
Participating in meetings, conference calls with client, Service Delivery Manager and Application team for System improvements.
Participated in quarterly DR activity.
Involved in creation of SIP - Service Improvement Plans
Involved in handling of high severity issues and provided RCA for the same.
Worked on Always on availability groups, database mirroring, replication, clustering and log shipping.
Have basic understanding of Oracle and Netezza.
Provided on- call support during out of office hours and weekends.
Resource & shift management of 5 SQL DBAs from offshore in multi-client environment for Data center services.
Provided KT to team members, monitor and guide trainees.
company - Wipro Technologies
description - Responsibilities: â¢ MS SQL Server 2016/2014/2012/ 2008 R2/ 2008 installation, configuration, and administration.
â¢ Worked on Always on availability groups, log shipping, database mirroring and clustering.
â¢ Participated in  PCI scan report to perform installation of security hot fixes, service packs for SQL servers to remove vulnerability.
â¢ Participated in Holmes BOTS automation implementation of SQL Pack tool.
â¢ Worked on service requests, incidents and critical issues.
â¢ Involved in conference calls to provide DBA support for critical issues.
â¢ Performance tuning.
Environment: SQL Server 2016/2014/2012/2008R2/2008, Windows Server 2012/2008R2/2008
company - Mphasis
description - 
company - Mphasis
description - Responsibilities: â¢ MS SQL Server 2012/ 2008 R2/ 2008  installation, configuration, and administration.
â¢ Worked on Always on availability groups, log shipping, database mirroring and clustering.
â¢ Performed SQL server patching activity â¢ Worked on daily reports like cluster failover, backup, AG/LS/Mirror report and server disk space report.
â¢ Worked on service requests, incidents and critical issues.
â¢ Participated in quarterly DR activity.
â¢ Involved in conference calls to provide DBA support for critical issues.
â¢ Provided support to windows team during patching for AG-mirror-cluster failover/failback and database health check.
â¢ Performed all the health checks for market open servers and provided update in market open call â¢ Deeply involved in   resolution of the issue and finding the root cause analysis of the issue â¢ Performance tuning.
Environment: SQL Server 2012/2008R2/2008, Windows Server 2008R2/2008
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ SQL server, Oracle and Netezza databases support tasks.
â¢ MS SQL Server 2008 R2/ 2008 installation, upgrade, and administration.
â¢ Done capacity planning for database growth for all SQL servers.
â¢ Troubleshooting alerts.
â¢ Worked on log shipping and mirroring.
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008, Oracle 10g/RAC
company - Synechron Technologies Pvt. Ltd
description - 
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ Pursued in-depth training on Oracle 11g Architecture and SQL Server.
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008, Oracle 10g
company - Synechron Technologies Pvt. Ltd
description - Responsibilities: â¢ Carried out version changes for schemas from PE8 version to EE11 version as per the process given
Environment: Oracle 11g
company - Mastek Ltd
description - Responsibilities: â¢ SQL Server 2008 R2/ 2008 installation, upgrade, and administration â¢ database backup/restore.
â¢ Performed MS SQL Server audits â¢ Worked with database mirroring, replication, log shipping and clustering.
â¢ Supported UAT and PROD environments â¢ Performed deployment document review.
â¢ Carried out deployments for different applications
Environment: SQL Server 2008R2/2008, Windows Server 2008R2/2008
company - Mastek Ltd
description - 
company - PP Software and Systems Ltd
description - 
company - PP Software and Systems Ltd
description - Description: The system provides Master Data Management and Procurement modules for dairy industry.
Responsibilities: â¢ Designed, coded, and tested â¢ Customized ERP system as per the requirement
Environment: Core Java, PostgreSQL"
Database,"SKILLSET Oracle DBA, MySQL, MARIADB, PostgreSQL Database Administration ITSKILLS SQL Oracle 10g, 11g, MYSQL, MariaDB, postgreSQL Windows, Linux Putty Education Details 
January 2018 MCS  Pune, Maharashtra Pune University
Database administrator 

Database administrator  - Infiniteworx Omnichannel Pvt. Ltd
Skill Details 
DATABASE- Exprience - 17 months
MYSQL- Exprience - 17 months
ORACLE- Exprience - 17 months
SQL- Exprience - 17 months
DATABASE ADMINISTRATION- Exprience - 6 monthsCompany Details 
company - Infiniteworx Omnichannel Pvt. Ltd
description - Pune Sept 2017 to Present

RESPONSIBILITIES:
â¢ Creating tablespaces and planning the location of data, monitoring the tablespaces growth periodically.
â¢ All replication setup
â¢ Moved database Schema changes to stage.
â¢ Dba support query resolution.
â¢ Creating user and giving specific privileges
â¢ Database management.
â¢ Database recovery, moving data files to different locations.
â¢ Planning the backup policies and Backup/ Recovery of databases based on the criticality.
â¢ IMPORT/EXPORT.
â¢ Degine schemas

Key Result Areas:

â¢ Providing 24 /7 support to resolve database performance issues, Job failures, Sessions & diagnose root causes
â¢ Installation, configuring and updating Oracle server software and related Oracle products. Installation, configuraing and updating Mysql, Sql server, MariaDB, MongoDB
â¢ Supported multiple databases and administered Oracle Databases of Large DB Sizes for production, development & test setups.

â¢ Maintaining table spaces & data files, Control files, Online Redo log files

â¢ Creating Users, granting Roles & Privileges to users and managing tablespaces for different users by granting quota on Default & Temporary tablespaces.

â¢ Taking Oracle RMAN Backups (Scheduling for day wise backup)

â¢ Implementing the incremental, cumulative and full RMAN backup for each database to have space management and effective recovery.
â¢ Logical Backup Using Export & Import/datapump Export of important tables at regular intervals.

â¢ Regular checking of trace, alert log file, all ORA errors

â¢ Working on incidents like User creation/deletion incidents, backup failed incidents.
â¢ Checking Listener Status, connectivity Troubleshooting and fixing database listener issues.
â¢ Look for any new alert / error log entries / errors, error details in Trace files generated. Executing DDL & DML scripts as per customer requirements

â¢ Mentoring, coaching and appraising team members with active involvement in the recruitment process

â¢ Contributing in Project Documentation and generating daily reports

â¢ Ensuring compliance to quality norms and taking steps for any non-conformance Spearheading complete project activities ensuring timely completion of project
â¢ Implementing security policies on different database systems with granting and revoking privileges to the users

â¢ Following change management processes and participated in related meetings

â¢ Verifying all Instances/DB are running, Tablespaces are online, Monitor Backround processes and status.
company - InnovativeTechnologies
description - Clients: BANKING DOMAIN"
Database,"Education Details 
January 2016 BSc.  Mumbai, Maharashtra Mumbai University
January 2013 H.S.C.   Maharashtra Board
January 2011 S.S.C.   Maharashtra Board
MySQL Database Administrator 

2+ Years of experience in MySQL Database Administrator ( MySQL DBA)
Skill Details 
MySQL DBA , Centos , Backup , Restore , Replication , Query Optimazation- Exprience - 24 monthsCompany Details 
company - Trimax IT Infrastructure & Services Ltd
description - Â·Â Â Â Â Â Â Â MYSQL Installation, maintenance and Upgrades (Version 5.5 , 5.6)
Â·Â Â Â Â Â Â Â MySQL database administration on a large scale MySQL installation
Â·Â Â Â Â Â Â Â Experience with MySQL on both Linux and Windows
Â·Â Â Â Â Â Â Â MySQL processes, security management and queries optimization.
Â·Â Â Â Â Â Â Â Performed query analysis for slow and problematic queries.
Â·Â Â Â Â Â Â Â Performed Structural changes to Database like creating tables, adding columns according to business requirement
Â·Â Â Â Â Â Â Â Creating and MaintainingÂ Database Maintenance Plans.



Â·Â Â Â Â Â Â Â Writing scripts to Create Jobs for Backup & Restore Plans.
Â·Â Â Â Â Â Â Â Working on MYISAM to INNODB engine.
Â·Â Â Â Â Â Â Â Working on Server shifting , tuning parameter , database purging
Â·Â Â Â Â Â Â Â Working on Mysql master slave Replication
Â·Â Â Â Â Â Â Â Handling Release management and user acceptance.
Â·Â Â Â Â Â Â Â Restore using xtrabackup.
Â·Â Â Â Â Â Â Â Responsibilities include monitoring daily, weekly and monthly system maintenance tasks such as database backup, replication verification, database integrity verification and indexing updates
Â·Â Â Â Â Â Â Â Work in 24/7 production database support.
company - Trimax IT Infrastructure & Services Ltd
description - Â·Â Â Â Â Â Â Â MYSQL Installation, maintenance and Upgrades (Version 5.5 , 5.6)
Â·Â Â Â Â Â Â Â MySQL database administration on a large scale MySQL installation
Â·Â Â Â Â Â Â Â Experience with MySQL on both Linux and Windows
Â·Â Â Â Â Â Â Â MySQL processes, security management and queries optimization.
Â·Â Â Â Â Â Â Â Performed query analysis for slow and problematic queries.
Â·Â Â Â Â Â Â Â Performed Structural changes to Database like creating tables, adding columns according to business requirement
Â·Â Â Â Â Â Â Â Creating and MaintainingÂ Database Maintenance Plans.



Â·Â Â Â Â Â Â Â Writing scripts to Create Jobs for Backup & Restore Plans.
Â·Â Â Â Â Â Â Â Working on MYISAM to INNODB engine.
Â·Â Â Â Â Â Â Â Working on Server shifting , tuning parameter , database purging
Â·Â Â Â Â Â Â Â Working on Mysql master slave Replication
Â·Â Â Â Â Â Â Â Handling Release management and user acceptance.
Â·Â Â Â Â Â Â Â Restore using xtrabackup.
Â·Â Â Â Â Â Â Â Responsibilities include monitoring daily, weekly and monthly system maintenance tasks such as database backup, replication verification, database integrity verification and indexing updates
Â·Â Â Â Â Â Â Â Work in 24/7 production database support."
Database,"TECHNICAL SKILL: Operating System LINUX, Windows Server 2012 R2, Windows 98, Windows 2000/ XP Tools & Utility Packages SQL* Loader, SQL*PLUS, OEM, Datapump, expdp/impdp, PLSQL Developer, Jenkins Database Oracle 10g, Oracle 11g, Oracle 12c Scripting UNIX Shell Scripting Language SQL Education Details 
January 2011 M.B.A.  Amravati, Maharashtra Amravati University
January 2007 B.C.A.  Nagpur, Maharashtra Nagpur University
Oracle Database Administrator 

ORACLE DATABASE ADMINISTRATOR ON LINUX/MICROSOFT WITH 4 YEARS EXPERIENCE.
Skill Details 
ORACLE- Exprience - 48 months
LINUX- Exprience - 6 months
ORACLE DBA- Exprience - Less than 1 year months
RAC- Exprience - Less than 1 year months
GOLDEN GATE- Exprience - Less than 1 year months
ASM- Exprience - Less than 1 year months
DATAGUARD- Exprience - Less than 1 year monthsCompany Details 
company - TIETO INDIA PVT. LTD
description - Pune From February 2015 till present

Project Profile:
Oil and Gas unit of Tieto India Pvt. Ltd. is working for Environmental Components (EC) application. Tieto is the authorized service provider in EC. Energy Components is a complete end-to-end hydrocarbon accounting solution following the hydrocarbons from production to transport, sales and revenue recognition. Globally market-leading hydrocarbon accounting software with functionality coverage exceeding other available solutions. Modern, flexible and scalable technology platform. Selected as the global standard and best practice by oil & gas super majors.
Responsibilities: â¢ Oracle Database Administration 11g R2, 12c and 18c â¢ Supporting databases in 24x7 environments and coordinate with Application, OS, Storage and Development Teams. Test and Production environments â¢ Regularly monitoring the trace files and Alert log files for database related issues.
â¢ Experience in monitoring the CPU usage, IO and memory utilization at OS level.
â¢ Checking the Alert log file to analyze the ORA errors if any to raise SR with Oracle.
â¢ Monitoring the log files, backups, database space usage and the use of system resources.
â¢ Configuring Backup (RMAN) for database and restoring database.
â¢ Installation, configuring and updating Oracle server software and related Oracle products of 11g and 12C.
â¢ Oracle Server installation, client installation and configuration, PLSQL developer installation.
â¢ Creating database using DBCA and manually.
â¢ Creating of Oracle user and granting proper privileges to user as per request.
â¢ Creating AWR, ASH and ADDM reports for database performance analysis.
â¢ Handling space management and performance issues in Oracle databases.
â¢ Creating remote database link.
â¢ Renaming and resizing of data files in Oracle database if needed.
â¢ Tablespace shrinking with regular time interval to reclaim server space.
â¢ Expertise in Export and Import using data pump in Oracle database.
â¢ Expertise in Configuration of Listener and Tnsnames through NETMGR and NETCA and statically also.
â¢ Managing Oracle Listener and Oracle Network Files.
â¢ Creating user Profiles, granting specific privileges and roles to the users in Oracle database.
â¢ Maintaining tablespaces & data files, Control files, Online Redo log files in Oracle database.
â¢ Worked on AWS cloud services like EC2, S3, RDS, ELB, EBS, VPC, Route53, Auto, Cloud watch, Cloud Front, IAM for installing configuring and troubleshooting on various Amazon images for server migration from physical into cloud."
Database,"Technical Skills Databases: Oracle RDBMS- 10g, 11g & 12c Technology/utilities: Data Pump, RMAN, Data guard, ASM, RAC, Golden Gate Tools: OCC, PUTTY, SQLPLUS, SQL Developer, Netbackup, SCOM, SCCM, VMWare Vsphere Operating Systems: RHEL 6.0, RHEL 6.5, UNIX and Microsoft WindowsEducation Details 

Database Administrator 

Database Administrator - BNY Mellon
Skill Details 
DATABASES- Exprience - 24 months
ORACLE- Exprience - 24 months
RMAN- Exprience - 24 months
NETBACKUP- Exprience - 24 months
SCOM- Exprience - 24 monthsCompany Details 
company - BNY Mellon
description - Databases: 600+
Team Size: 8
Duration: Jan 2017 - Till Date
Clients: Over 130+ investment banking organizations who are hosted with Eagle

Responsibilities: Database Management (Support and managing critical production, Pre-production, test and reporting databases in different platforms), Capacity Management Upgrades.
â¢ Handling day to day database activities monitoring and incident management.
â¢ Building new databases as per the requirement and prepare them for go live with the help of multiple teams.
â¢ Working on scheduled activity of database patching (CPU, PSU) â¢ Installing latest path on production, Dev and Test databases as per the suggestion from Oracle support.
â¢ Database Upgrade from 11g and to 12c.
â¢ Adding disks to ASM disk groups.
â¢ Building DR database using Active Data guard, Make it sync with prod and resolving issues if persists â¢ Data Guard Management- Checking lagging status, removing lagging of archives, checking processes like RFS/MRP, Archives Management â¢ Working on tablespace related issues â¢ Managing user access and profiles â¢ Importing and exporting using datapump â¢ Maintaining inventory of all databases in the single centralize database â¢ Refreshing test environment from production database.
â¢ Working with Oracle Support to resolve oracle errors.
â¢ Schedule daily and weekly databases backup using RMAN, Troubleshooting in RMAN issues.
â¢ Database cloning using RMAN.
â¢ Take part in cutover to upgrade application to higher version.
â¢ Strictly following ITIL process in incident management and change management.
â¢ Providing weekly report of issues in team meeting also participating and suggesting service improvement plans.
â¢ Database Migrations from one server to another or to different platforms â¢ RCA and impact analysis reporting during any production outage.

Previous Organization: Brose India
Project I: Central IT Management
company - 
description - Responsibilities: Managing our internal databases and servers of Brose global.
â¢ Providing 24x7 on-call support in the rotational shifts.
â¢ Performing day-to-day database activity â¢ Monitoring and responding DBA group Mails for all alerts, issues and ad-hoc business user requests, etc.
â¢ Database creation, patching â¢ Backup of Database in frequent cycles using Data pump/RMAN.
â¢ Database refreshes using RMAN, Datapump.
â¢ Recovery using copy of data / RMAN â¢ Monitoring logs and trace for resolving issues.
â¢ Creating new VM servers and prepare it for go live, Also decommissioning as per requirements.
â¢ Weekly patching of windows servers using SCCM and taking actions for patching if needed â¢ Monitoring and troubleshooting of daily and weekly OS backup using Symantec Netbackup â¢ Managing user accounts of OS users and database users â¢ Monitoring OS level alerts using SCOM

Project II: Data Center Migration (Onsite Project)
Responsibilities: Data center migration was one of activity for migration of our datacenter from one location to another. Where our all servers and databases were moved successfully.
â¢ Installation of Oracle 11g on Linux platforms â¢ Worked on service requests (Incidents / Change / Request) â¢ Creation of users, managing user privileges â¢ Configured RMAN backup for databases â¢ Patching of databases â¢ Configuring physical standby database using Dataguard â¢ Cloning of servers and migrate to another cluster

ACADEMIA / PERSONAL DETAILS â¢ Bachelor of Engineering (B.E.) in Computer Science and Engineering From SGBAU Amravati University, Amravati in 2014 with CGPA of 7.21

Current Address:-       Mr. Yogesh Tikhat, C/O: Raj Ahmad, Flat# G2-702, Dreams Aakruti, Kalepadal, Hadapsar, Pune - 411028
Highest Qualification   BE (cse)

PAN: -                  AOFPT5052C"
Database,"Software Skills: * RDBMS: MS SQL SERVER 2000/2005/2008 & 2012, 2014 * Operating Systems: WINDOWS XP/7, WINDOWS SERVER 2008, 12 * Fundamentals: MS Office 03/07 * Tools: SSMS, Performance Monitor, Sql profiler, SQL lite speed. Company name: Barclays Technology Centre India. Team Size: 24 Role: Database Administrator Support Description: Barclays Technology is a UK based retail & invest bank and 300 years of old bank.. It has operations in over 40 countries and employs approximately 120, 000 people. Barclays is organised into four core businesses: Personal & Corporate (Personal Banking, Corporate Banking, Wealth & Investment Management), Barclaycard, Investment Banking. Responsibilities: â Attending various calls from all over the world on various database issues. â Working on Web Gui alerts and resolving incident tickets within the time lines. â Troubleshoooting log shipping issues and fixing the related alerts. â Identifying and Resolving Blocking and locking related issues. â Configuration and monitoring Replication, Log shipping and mirroring setup. â Working on replication issues and Always ON issue. â Granting and revoking permissions on various account provisioning tasks. â Working on call support during the weekend and performing DR test's. and working on weekly maintenance jobs and weekend change requests. Education Details 
 B.Sc. Maths  Kakatiya University Board secured
SQL server database administrator 

Database administrator
Skill Details 
DATABASE- Exprience - 120 months
DATABASE ADMINISTRATOR- Exprience - 72 months
MAINTENANCE- Exprience - 48 months
MS SQL SERVER- Exprience - 48 months
REPLICATION- Exprience - 48 monthsCompany Details 
company - Barclays global services centre
description - SQL server databases implementation and maintenances

Log shipping, replication, High availability, clustering, performance tuning, database mirroring, Installation, configuration, upgradation, migration
company - Wipro Infotech Pvt Ltd
description - SQL server database administrator
company - CITI Bank
description - Worked as Database Support at Accord Fintech, Sanpada from Sep 2008 to 2013 Feb.
company - 
description - 2012.
â¢     Sound knowledge in Database Backup, Restore, Attach, and Detach and Disaster Recovery procedures.
â¢     Developed backup and recovery strategies for production environment.
â¢     Ensuring data consistency in the database through DBCC and DMV commands.
â¢     Experience in query tuning and stored procedures and troubleshooting performance issues.
â¢     Having hands on experience in DR process including log shipping and database mirroring.
â¢     Experience in scheduling   monitoring of Jobs.
â¢     Experience in configure and troubleshooting in Always ON.
â¢     Creating and Maintaining of Maintenance Plan.
â¢     Expertise in planning and implementing MS SQL Server Security and Database permissions.
â¢     Clear understanding of Implementation of Log Shipping, Replication and mirroring of databases between the servers.
â¢     Performance Tuning (Performance Monitor, SQL Profiler Query Analyzer) â¢     Security for server & Databases Implementing security by creating roles/users,
Added users in roles, assigning rights to roles.
â¢     Create and maintaining the linked servers between sql Instances.
â¢     Create and maintaining and Database mail.
â¢     Monitor and troubleshoot database issues.
â¢     Creating DTS packages for executing the required tasks.
â¢     Experts in create indexes, Maintaining indexes and rebuilds and reorganizes.
â¢     Daily Maintenance of SQL Servers included reviewing
SQL error logs, Event Viewer."
Database,"Areas of Expertise â¢ Oracle Databases 12c, 11g, 10g â¢ Weblogic 12c, 11g â¢ Grid Infrastructure â¢ RMAN â¢ ASM â¢ Middleware: OIM, OAM, SOA â¢ Shell Scripts â¢ DataGuard â¢ Web servers - OHS, Apache â¢ Architecture Designs â¢ Proof of Concepts â¢ DevOpsEducation Details 
January 2007 Bachelor of Engineering Information Technology Sangli, Maharashtra Walchand College
January 2004 Diploma Computer Engineering Jalgaon, Maharashtra Govt. Polytechnic
Lead Database Administrator 

Lead Database Administrator - Tieto Software
Skill Details 
DATABASES- Exprience - 108 months
MIDDLEWARE- Exprience - 96 months
RMAN- Exprience - 84 months
SHELL SCRIPTS- Exprience - 48 monthsCompany Details 
company - Tieto Software
description - As a part of AO (Application Operations) team, scope in project is quite wide than typical database administration. Range of accomplishments are extended right from Data Tier to Middle Tier & Application Tier:
- Maximized availability of applications from 99.3% to 99.8%
- Raised business by presenting Proof of Concepts for 10+ cases
- Delivered upgrades of various applications time to time to keep it on supported platform
- Saved SLAs as per contract by means of handling P1, P2 issues effectively
- Produced Capacity reports comprising all layers (Data, Middleware, Web) of various applications
- Generated Work Orders as per customer need
company - Tieto Software
description - - Designed databases of various applications
- Planned RMAN backup and recovery, BCP strategies
- Executed Business Continuity Testing for various applications
- Introduced Zero Cost high availability solutions - Active-Passive Failover
- Optimized performance by means of scripting automation
- Established cloning procedures for all layers of various applications
- Delivered Infrastructure changes, like FW Openings & LoadBalancer configuration for new applications
- Eliminated downtime by troubleshoot issues for Middleware products - OIM, OAM, SOA
- Contributed to build & maintain Integration Layer- SMTP, ftp, Reverse Proxy, OCM
company - Tieto Software
description - - Provided database support to environments - PROD, UAT, TEST, DEV
- Performed Database Refresh/Cloning from production to development and support databases
- Reduced risk level by means of upgrading & patching databases time to time
- Protected databases by assigning appropriate roles and privileges as per SOD
- Generated & maintained Middleware schemas using RCU
- Exceeded scope of work by supporting & maintaining WebLogic platform - installation, patching, troubleshooting issues
- Expanded duty scope to web servers: Install & maintain- OHS, apache, tomcat
company - HSBC Software
description - Being part of project supporting HSBC Bank France, I achieved:
- Handled incidents & service requests as Day to day database administration tasks
- Delivered basic implementation services - Database installation, patching, upgrades
- Performed capacity planning - managing tablespaces, compressions
- Contributed in maintaining quality of databases - managing instances, indexes, re-organization, performance monitoring & tuning using AWR, ADDM reports
- Maintained backups & recovery of database - logical backups (exp/imp), datapump (expdp/impdp), cold backups, hot backups, RMAN backup/restore, RMAN Duplication
- Reduced efforts by automation - Value add initiatives which includes writing shell scripts for automated housekeeping operations, scheduling backups, use crontab/at to schedule tasks
- Implemented high availability solutions - Dataguard"
Database,"Education Details 
May 2011 to May 2014 Bachelor of science Information technology Mumbai, Maharashtra Mumbai university
Oracle DBA 

Oracle database administrator
Skill Details 
Installation of Oracle on RH Linux & Windows. Creating/Managing user profiles and analyzing their privileges and tablespace quotas Backup of database Logical and Physical procedures. Recovery of database in case of database crash, disk/media failure, etc. Standard DBA functions like space management, Rollback segments, Extents. Database Management and Monitoring the database. Willing to learn new things. Being a constructive team member, contributing practically to the success of the team.- Exprience - 48 monthsCompany Details 
company - Accelya kale solutions ltd
description - Database Administrator working in 24*7 support environment maintaining Databases running on Oracle 11g, 12c.
Database Up-gradation from Oracle 11g to Oracle 12c.
Installation of Database critical patches.
Taking cold and hot backups on scheduled times and monitoring backups.
Importing the export dump to another database as per demands.
Automating most of the daily activities through cronjobs, shell scripts or schedulers.
Making Plan of Actions for Various Activities.
Raising SR with Oracle Support for different severity issues.
Handling the Userâs request and proper client interaction.
Monitoring & managing database growth, tablespaces, adding ,resizing and renaming the datafiles.
Restoration of database using RMAN backups for backup consistency checks.
Migration of Database using export / import and RMAN backups.
Configuring & managing Physical Standby database.
Creating database links, Tablespaces, database directories.
Managing network settings through listener.ora and tnsnames.ora files.
Restoration of data using old logical backup as per client request.
Schema replication across databases through data pump tool.
Taking cold and hot backups on scheduled times and monitoring backups
Taking EXPDP of database, database objects and a particular schema
Using SCP ticketing tool in order keeping track of client requests.Â 
Performing Maintenance Activities such as Index Rebuilding and stats gather.
Troubleshooting the Basic LevelÂ performance issuesÂ 
Setting up a new environmentÂ from database perspective within the requested timelines
Adding/Deleting disks in ASM and monitoring the ASM diskgroups.
Creating users & privileges with appropriate roles and levels of security.Â 
Database Administrator working in 24*7 support environment maintaining Databases running on Oracle 11g, 12c.
Performing database online and offline database re-organization for database enhancement.Â 
Migrating database from Non-ASM to ASM file system.
Grid up-gradation from 11g to 12C.
company - Insolutions Global Ltd
description - Oracle software installation(graphical/silent),Database upgrade,Patch upgrade.
Maintaining around 80+ UAT DB servers, 40 production DB and 28 standby/DR DB.
Managing/creating DR & standby servers, DB sync.
Backup and recovery (RMAN/ Datapump).
Performing activities like switchover and failover .
Allocating system storage and planning future storage requirements for the database system
Enrolling users and maintaining system security.
Monitoring Alert log, Snap ID generation, db size, Server space, OEM reports, User validity.
Controlling and monitoring user access to the database .
Scheduling shell scripts or dbms_jobs using Crontab or DBMS_SCHEDULER (monitoring script, listener check, backup script, AWR reports) etc.
Planning for backup and recovery of database.
Managing the production database for Oracle and SQL Server and resize the space of database/Datafiles/Tablespace/Transactional Logs.
Managing Temp and Undo tablespaces.
Creating primary database storage structures (tablespaces) after application developers have designed an application."
Database,"TECHNICAL SKILLS â¢ SQL â¢ Oracle v10, v11, v12 â¢ R programming, Python, linear regression, machine learning and statistical modelling techniques(obtained certification through Edvancer Eduventures training institute) KEY SKILLS â¢ Multitasking, working to meet client SLA in high pressure scenarios, handling sensitive clients along with improved skills at being a team player. â¢ Excellent communication skills and quick learner. â¢ Leadership qualities, team networking and courage to take up the problems proactively.Education Details 
June 2012    Sadvidya Pre-University College
Application Database Administrator-DBMS (Oracle) 

Application Database Administrator-DBMS (Oracle) - IBM India Pvt Ltd
Skill Details 
CLIENTS- Exprience - 30 months
MACHINE LEARNING- Exprience - 30 months
ORACLE- Exprience - 30 months
SQL- Exprience - 30 months
EXCELLENT COMMUNICATION SKILLS- Exprience - 6 monthsCompany Details 
company - IBM India Pvt Ltd
description - Client: Blue Cross Blue Shield MA: Massachusetts Health Insurance
â¢   Used Oracle SQL to store and organize data. This includes capacity planning, installation, configuration, database
design, migration, security, troubleshooting, backup and data recovery.
â¢   Worked with client databases installed on Oracle v10, v11, v12 on a Linux platform.

â¢   Proficient communication with clients across locations facilitating data elicitation.

â¢   Handling numerous business requests and solving them diligently within the given time frame and responding quickly and effectively to production issues within SLA.

â¢   Leading a team in co ordination with business to conduct weekly checkouts of the database servers and systems

IBM Certifications
Statistics 101, Applied Data Science with R, Big Data Foundations, Data Science Foundations

Business Analytics Certification (Pune)
Worked on Retail and Banking projects, to design a predictive business model using machine learning techniques in
R programming for an efficient business and marketing strategy."
Hadoop,"Education Details 

Hadoop Developer 

Hadoop Developer - INFOSYS
Skill Details 
Company Details 
company - INFOSYS
description - Project Description: The banking information had stored the data in different data ware house systems for each department but it becomes difficult for the organization to manage the data and to perform some analytics on the past data, so it is combined them into a single global repository in Hadoop for analysis.

Responsibilities:
â¢       Analyze the banking rates data set.
â¢       Create specification document.
â¢       Provide effort estimation.
â¢       Develop SPARK Scala, SPARK SQL Programs using Eclipse IDE on Windows/Linux environment.
â¢       Create KPI's test scenarios, test cases, test result document.
â¢       Test the Scala programs in Linux Spark Standalone mode.
â¢       setup multi cluster on AWS, deploy the Spark Scala programs
â¢       Provided solution using Hadoop ecosystem - HDFS, MapReduce, Pig, Hive, HBase, and Zookeeper.
â¢       Provided solution using large scale server-side systems with distributed processing algorithms.
â¢       Created reports for the BI team using Sqoop to export data into HDFS and Hive.
â¢       Provided solution in supporting and assisting in troubleshooting and optimization of MapReduce jobs and
Pig Latin scripts.
â¢       Deep understanding of Hadoop design principles, cluster connectivity, security and the factors that affect
system performance.
â¢       Worked on Importing and exporting data from different databases like Oracle, Teradata into HDFS and Hive
using Sqoop, TPT and Connect Direct.
â¢       Import and export the data from RDBMS to HDFS/HBASE
â¢       Wrote script and placed it in client side so that the data moved to HDFS will be stored in temporary file and then it will start loading it in hive tables.
â¢       Developed the Sqoop scripts in order to make the interaction between Pig and MySQL Database.
â¢       Involved in developing the Hive Reports, Partitions of Hive tables.
â¢       Created and maintained technical documentation for launching HADOOP Clusters and for executing HIVE
queries and PIG scripts.
â¢       Involved in running Hadoop jobs for processing millions of records of text data

Environment: Java, Hadoop, HDFS, Map-Reduce, Pig, Hive, Sqoop, Flume, Oozie, HBase, Spark, Scala,
Linux, NoSQL, Storm, Tomcat, Putty, SVN, GitHub, IBM WebSphere v8.5.

Project #1: TELECOMMUNICATIONS
Hadoop Developer

Description To identify customers who are likely to churn and 360-degree view of the customer is created from different heterogeneous data sources. The data is brought into data lake (HDFS) from different sources and analyzed using different Hadoop tools like pig and hive.

Responsibilities:
â¢       Installed and Configured Apache Hadoop tools like Hive, Pig, HBase and Sqoop for application development and unit testing.
â¢       Wrote MapReduce jobs to discover trends in data usage by users.
â¢       Involved in database connection using SQOOP.
â¢       Involved in creating Hive tables, loading data and writing hive queries Using the HiveQL.
â¢       Involved in partitioning and joining Hive tables for Hive query optimization.
â¢       Experienced in SQL DB Migration to HDFS.
â¢       Used NoSQL(HBase) for faster performance, which maintains the data in the De-Normalized way for OLTP.
â¢       The data is collected from distributed sources into Avro models. Applied transformations and standardizations and loaded into HBase for further data processing.
â¢       Experienced in defining job flows.
â¢       Used Oozie to orchestrate the workflow.
â¢       Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce
jobs given by the users.
â¢       Exported the analyzed data to the relational databases using HIVE for visualization and to generate reports for the BI team.

Environment: Hadoop, Hive, Linux, MapReduce, HDFS, Hive, Python, Pig, Sqoop, Cloudera, Shell Scripting,
Java (JDK 1.6), Java 6, Oracle 10g, PL/SQL, SQL*PLUS"
Hadoop,"Skill Set: Hadoop, Map Reduce, HDFS, Hive, Sqoop, java. Duration: 2016 to 2017. Role: Hadoop Developer Rplus offers an quick, simple and powerful cloud based Solution, Demand Sense to accurately predict demand for your product in all your markets which Combines Enterprise and External Data to predict demand more accurately through Uses Social Conversation and Sentiments to derive demand and Identifies significant drivers of sale out of hordes of factors that Selects the best suited model out of multiple forecasting models for each product. Responsibilities: â¢ Involved in deploying the product for customers, gathering requirements and algorithm optimization at backend of the product. â¢ Load and transform Large Datasets of structured semi structured. â¢ Responsible to manage data coming from different sources and application â¢ Supported Map Reduce Programs those are running on the cluster â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.Education Details 

Hadoop Developer 

Hadoop Developer - Braindatawire
Skill Details 
APACHE HADOOP HDFS- Exprience - 49 months
APACHE HADOOP SQOOP- Exprience - 49 months
Hadoop- Exprience - 49 months
HADOOP- Exprience - 49 months
HADOOP DISTRIBUTED FILE SYSTEM- Exprience - 49 monthsCompany Details 
company - Braindatawire
description - Technical Skills:
â¢   Programming: Core Java, Map Reduce, Scala
â¢   Hadoop Tools: HDFS, Spark, Map Reduce, Sqoop, Hive, Hbase
â¢   Database: MySQL, Oracle
â¢   Scripting: Shell Scripting
â¢   IDE: Eclipse
â¢   Operating Systems: Linux (CentOS), Windows
â¢   Source Control: Git (Github)"
Hadoop,"â¢ Operating systems:-Linux- Ubuntu, Windows 2007/08 â¢ Other tools:- Tableau, SVN, Beyond Compare.Education Details 
January 2016 Bachelors of Engineering Engineering  Gujarat Technological University
Systems Engineer/Hadoop Developer 

Systems Engineer/Hadoop Developer - Tata Consultancy Services
Skill Details 
Hadoop,Spark,Sqoop,Hive,Flume,Pig- Exprience - 24 monthsCompany Details 
company - Tata Consultancy Services
description - Roles and responsibility:

Working for a American pharmaceutical company (one of the world's premier
biopharmaceutical) who develops and produces medicines and vaccines for a wide range of medical
disciplines, including immunology, oncology, cardiology, endocrinology, and neurology. To handle large
amount of United Healthcare data big data analytics is used. Data from all possible data sources like records of all Patients(Old and New), records of medicines, Treatment Pathways & Patient Journey for
Health Outcomes, Patient Finder (or Rare Disease Patient Finder), etc being gathered, stored and processed at one place.

â¢     Worked on cluster with specs as:
o    Cluster Architecture: Fully
Distributed Package Used:
CDH3
o    Cluster Capacity: 20 TB
o    No. of Nodes: 10 Data Nodes + 3 Masters + NFS Backup For NN

â¢     Developed proof of concepts for enterprise adoption of Hadoop.
â¢   Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera
distribution.
â¢   Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and trouble-shooting, manage and review data backups, and reviewing Hadoop log files.
â¢   Imported & exported large data sets of data into HDFS and vice-versa using sqoop.
â¢   Involved developing the Pig scripts and Hive Reports
â¢   Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive
partition.Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive.
â¢   Developed Spark scripts by using Scala shell commands as per the requirement and worked with both
Data frames/SQL/Data sets and RDD/MapReduce in Spark. Optimizing of existing algorithms in Hadoop
using SparkContext, Spark-SQL, Data Frames and RDD's.
â¢   Collaborated with infrastructure, network, database, application and BI to ensure data, quality and availability.
â¢   Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop.
â¢   Used ORC & Parquet file formats for serialization of data, and Snappy for the compression of the data.

Achievements

â¢   Appreciation for showing articulate leadership qualities in doing work with the team.
â¢   Completed the internal certification of TCS Certified Hadoop Developer.

Ongoing Learning
â¢   Preparing and scheduled the Cloudera Certified Spark Developer CCA 175."
Hadoop,"Areas of expertise â¢ Big Data Ecosystems: Hadoop-HDFS, MapReduce, Hive, Pig, Sqoop, HBase Oozie, Spark, Pyspark, HUE and having knowledge on cassandra â¢ Programming Languages: Python, Core Java and have an idea on Scala â¢ Databases: Oracle 10g, MySQL, Sqlserver NoSQL - HBase, Cassandra â¢ Tools: Eclipse, Toad, FTP, Tectia, Putty, Autosys, Anaconda, Jupyter notebool and Devops - RTC, RLM. â¢ Scripting Languages: JSP â¢ Platforms: Windows, UnixEducation Details 
 M.Tech (IT-DBS) B.Tech (CSE)  SRM University
Software Engineer 

Software Engineer - Larsen and Toubro
Skill Details 
Company Details 
company - Larsen and Toubro
description - Worked as a Software Engineer in Technosoft Corporation, Chennai from Aug 2015 to sep 2016.
company - Current Project
description - Duration: September 2016 to Till date
Vendor: Citi bank
Description:
Citibank's (Citi) Anti-Money Laundering (AML) Transaction Monitoring (TM) program is a future state solution and a rules-based system for transaction monitoring of ICG-Markets business.
Roles and Responesbilities:
â¢ Building and providing domain knowledge for Anti Money Laundering among team members.
â¢ The layered architecture has Data Warehouse and Workspace layers which are used by Business Analysts.
â¢ Actively involved in designing of star-schema model involving various Dimensions and Fact tables.
â¢ Designed SCD2 for maintaining history of the DIM data.
â¢ Developing Hive Queries for mapping data between different layers of architecture, and it's usage in Oozie Workflows.
â¢ Integration with Data Quality and Reconciliation Module.
â¢ Regression and Integration testing of solution for any issues in integration with other modules and effectively testing the data flow from layer-to-layer.
â¢ Transaction monitoring system development to generate Alerts for the suspicious and fraudulent transactions based on requirements provide by BAs.
â¢ Developing spark Jobs for various business rules.
â¢ Learning ""Machine Learning"", which will be used further in the project for developing an effective model for Fraud detection for Anti Money Laundering system.
â¢ Scheduling Jobs using Autosys tool.
â¢ Deployment and Code Management using RTC and RLM(Release Lifecycle Management)

Hadoop Developer
#  Current Project: PRTS - RAN
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Sqoop, HBase, Tez, Tableau, Sqlserver, Teradata
Cluster Size: 96 Node Cluster.
Distribution: Horton works - HDP2.3
company - Alcatel lucent
description - 1X) and  Ruckus Wireless
Description:
The scope of this project is to maintain and store the operational and parameters data collected from the multiple vendors networks by the mediation team into the OMS data store and make it available for RF engineers to boost the network performance.
Responsibilities:
â¢ Working with Hadoop Distributed File System.
â¢ Involved in importing data from MySQL to HDFS using SQOOP.
â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run  on top of  Tez execution Engine.
â¢ Involved in Preparing Test cases Document.
â¢ Involved in Integrating Hive and HBase to store the operational data.
â¢ Monitoring the Jobs through Oozie.
company - Current Project
description - Anti - Money laundering
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Oozie, Spark, Unix, Autosys, Python, RTC, RLM, ETL Framwe work
Cluster Size: 56 Node Cluster.
Distribution: Cloudera 5.9.14"
Hadoop,"Technical Skill Set: Programming Languages Apache Hadoop, Python, shell scripting, SQL Technologies Hive, Pig, Sqoop, Flume, Oozie, Impala, hdfs Tools Dataiku, Unravel, Cloudera, Putty, HUE, Cloudera Manager, Eclipse, Resource Manager Initial Learning Program: Tata Consultancy Services: June 2015 to August 2015 Description: This is a learning program conducted by TCS for the newly joined employees, to accomplish them to learn the working standard of the organization. During this period employee are groomed with various technical as well as ethical aspects. Education Details 
 B.E. Electronics & Communication Indore, Madhya Pradesh Medi-caps Institute of Technology & Management
Hadoop developer 

hadoop,hive,sqoop,flume,pig,mapreduce,python,impala,spark,scala,sql,unix.
Skill Details 
APACHE HADOOP SQOOP- Exprience - 31 months
Hadoop- Exprience - 31 months
HADOOP- Exprience - 31 months
Hive- Exprience - 31 months
SQOOP- Exprience - 31 months
python- Exprience - Less than 1 year months
hdfs- Exprience - Less than 1 year months
unix- Exprience - Less than 1 year months
impala- Exprience - Less than 1 year months
pig- Exprience - Less than 1 year months
unravel- Exprience - Less than 1 year months
mapreduce- Exprience - Less than 1 year months
dataiku- Exprience - Less than 1 year monthsCompany Details 
company - Tata Consultancy Services
description - Project Description
Data warehouse division has multiple products for injecting, storing, analysing and presenting data. The Data Lake program is started to provide multi-talent, secure data hub to store application's data on Hadoop platform with strong data governance, lineage, auditing and monitoring capabilities. The object of the project is to provide necessary engineering support to analytics and application teams so that they can focus on the business logic development. In this project, the major task is to set up the Hadoop cluster and govern all the activities which are required for the smooth functioning of various Hadoop ecosystems. As the day and day data increasing so to provide stability to the ecosystem and smooth working of it, Developing and automating the various requirement specific utilities.

Responsibility 1. Developed proactive Health Check utility for Data Lake. The utility proactively checks the smooth functioning of all Hadoop components on the cluster and sends the result to email in HTML format. The utility is being used for daily Health Checks as well as after upgrades.
2. Getting the data in different formats and processing the data in Hadoop ecosystem after filtering the data using the appropriate techniques.
3. Developed data pipeline utility to ingest data from RDBMS database to Hive external tables using Sqoop commands. The utility also offers the data quality check like row count validation.
4. Developed and automated various cluster health check, usage, capacity related reports using Unix shell scripting.
5. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
6. Creating flume agents to process the data to Hadoop ecosystem side.
7. Performed benchmark testing on the Hive Queries and impala queries.
8. Involved in setting up the cluster and its components like edge node and HA implementation of the services: Hive Server2, Impala, and HDFS.
9. Filtering the required data from available data using different technologies like pig, regex Serde etc.
10. Dataiku benchmark testing on top of impala and hive in compare to Greenplum database.
11. Moving the data from Greenplum database to Hadoop side with help of Sqoop pipeline, process the data to Hadoop side and storing the data into hive tables to do the performance testing.
12. Dealing with the Hadoop ecosystem related issues in order to provide stability to WM Hadoop ecosystem.
13. Rescheduling of job from autosys job hosting to TWS job hosting for better performance.

Declaration:
I hereby declare that the above mentioned information is authentic to the best of my knowledge
company - Tata Consultancy Services
description - Clients: 1. Barclays 2. Union bank of California (UBC) 3. Morgan Stanley (MS)

KEY PROJECTS HANDLED
Project Name ABSA- Reconciliations, UBC and WMDATALAKE COE
company - Tata Consultancy Services
description - Project Description
Migration of data from RDBMS database to Hive (Hadoop ecosystem) . Hadoop platform ability with strong data governance, lineage, auditing and monitoring capabilities. The objective of this project was to speed up the data processing so that the analysis and decision making become easy. Due to RDBMS limitations to process waste amount of data at once and produce the results at the earliest, Client wanted to move the data to Hadoop ecosystem so that they can over-come from those limitations and focus on business improvement only.

Responsibility 1. Optimising the SQL queries for those data which were not required to move from RDBMS to any other platform.
2. Writing the Hive queries and logic to move the data from RDBMS to Hadoop ecosystem.
3. Writing the hive queries to analyse the required data as per the business requirements.
4. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
5. Writing the sqoop commands and scripts to move the data from RDBMS to Hadoop side.
company - Tata Consultancy Services
description - Project Description
Create recs and migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.

Responsibility 1. Have worked on extracting business requirements, analyzing and implementing them in developing Recs 2. Worked on migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.
3. Done the back end work where most of the things were related to writing the sql queries and provide the data for the new recs.

Project Name   PSO"
Hadoop,"Technical Skills Programming Languages: C, C++, Java, .Net., J2EE, HTML5, CSS, MapReduce Scripting Languages: Javascript, Python Databases: Oracle (PL-SQL), MY-SQL, IBM DB2 Tools:IBM Rational Rose, R, Weka Operating Systems: Windows XP, Vista, UNIX, Windows 7, Red Hat 7Education Details 
January 2015 B.E  Pimpri Chinchwad, MAHARASHTRA, IN Pimpri Chinchwad College of Engineering
January 2012 Diploma MSBTE  Dnyanganaga Polytechnic
 S.S.C   New English School Takali
Hadoop/Big Data Developer 

Hadoop/Big Data Developer - British Telecom
Skill Details 
APACHE HADOOP MAPREDUCE- Exprience - 37 months
MapReduce- Exprience - 37 months
MAPREDUCE- Exprience - 37 months
JAVA- Exprience - 32 months
.NET- Exprience - 6 monthsCompany Details 
company - British Telecom
description - Project: British Telecom project (UK)
Responsibilities:
â¢ Working on HDFS, MapReduce, Hive, Spark, Scala, Sqoop, Kerberos etc. technologies
â¢ Implemented various data mining algorithms on Spark like K-means clustering, Random forest, NaÃ¯ve bayes etc.
â¢ A knowledge of installing, configuring, maintaining and securing Hadoop.
company - DXC technology
description - HPE legacy), Bangalore
â¢ Worked on Hadoop + Java programming
â¢ Worked on Azure and AWS (EMR) services.
â¢ Worked on HDInsight Hadoop cluster..
â¢ Design, develop, document and architect Hadoop applications
â¢ Develop MapReduce coding that works seamlessly on Hadoop clusters.
â¢ Analyzing and processing the large data sets on HDFS.
â¢ An analytical bent of mind and ability to learn-unlearn-relearn surely comes in handy."
Hadoop,"Technical Skill Set Big Data Ecosystems: Hadoop, HDFS, HBase, Map Reduce, Sqoop, Hive, Pig, Spark-Core, Flume. Other Language: Scala, Core-Java, SQL, PLSQL, Sell Scripting ETL Tools: Informatica Power Center8.x/9.6, Talend 5.6 Tools: Eclipse, Intellij Idea. Platforms: Windows Family, Linux /UNIX, Cloudera. Databases: MySQL, Oracle.10/11gEducation Details 
 M.C.A  Pune, MAHARASHTRA, IN Pune University
Hodoop Developer 

Hodoop Developer - PRGX India Private Limited Pune
Skill Details 
Company Details 
company - PRGX India Private Limited Pune
description - Team Size: 10+
Environment: Hive, Spark, Sqoop, Scala and Flume.

Project Description:
The bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. The customers spending ranges from online shopping, medical expenses in hospitals, cash transactions, and debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. The portal allows the customers to login and see their transactions which they make on a day to day basis .the analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. The portal used hadoop framework to analyes the data as per the rules and regulations placed by the regulators from the respective countries. The offers and the interest rates also complied with the regulations and all these processing was done using the hadoop framework as big data analytics system.

Role & Responsibilities:
â Import data from legacy system to hadoop using Sqoop, flume.
â Implement the business logic to analyses  the data
â Per-process data using spark.
â Create hive script and loading data into hive.
â Sourcing various attributes to the data processing logic to retrieve the correct results.

Project 2
company - PRGX India Private Limited Pune
description - 
company - PRGX India Private Limited Pune
description - Team Size: 11+
Environment: Hadoop, HDFS, Hive, Sqoop, MySQL, Map Reduce

Project Description:-
The Purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it.the solution was based on the open source s/w hadoop. The data will be stored in hadoop file system and processed using Map/Reduce jobs. Which in trun includes getting the raw html data from the micro websites, process the html to obtain product and user information, extract various reports out of the vistor tracking information and export the information for further processing

Role & Responsibilities:
â Move all crawl data flat files generated from various micro sites to HDFS for further processing.
â Sqoop implementation for interaction with database
â Write Map Reduce scripts to process the data file.
â Create hive tables to store the processed data in tabular formats.
â Reports creation from hive data.

Project 3
company - PRGX India Private Limited Pune
description - Team Size: 15+
Environment: Informatica 9.5, Oracle11g, UNIX

Project Description:
Pfizer Inc. is an American global pharmaceutical corporation headquartered in New York City. The main objective of the project is to build a Development Data Repository for Pfizer Inc. Because all the downstream application are like Etrack, TSP database, RTS, SADMS, GFS, GDO having their own sql request on the OLTP system directly due to which the performance of OLTP system goes slows down. For this we have created a Development Data Repository to replace the entire sql request directly on the OLTP system. DDR process extracts all clinical, pre-clinical, study, product, subject, sites related information from the upstream applications like EPECS, CDSS, RCM, PRC, E-CLINICAL, EDH and after applying some business logic put it into DDR core tables. From these snapshot and dimensional layer are created which are used for reporting application.

Role & Responsibilities:
â To understand & analyze the requirement documents and resolve the queries.
â To design Informatica mappings by using various basic transformations like Filter, Router, Source qualifier, Lookup etc and advance transformations like Aggregators, Joiner, Sorters and so on.
â Perform cross Unit and Integration testing for mappings developed within the team. Reporting bugs and bug fixing.
â Create workflow/batches and set the session dependencies.
â Implemented Change Data Capture using mapping parameters, SCD and SK generation.
â Developed Mapplet, reusable transformations to populate the data into data warehouse.
â Created Sessions & Worklets using workflow Manager to load the data into the Target Database.
â Involved in Unit Case Testing (UTC)
â Performing Unit Testing and UAT for SCD Type1/Type2, fact load and CDC implementation.

Personal Scan

Address: Jijayi Heights, Flat no 118, Narhe, (Police chowki) Pune- 411041"
Hadoop,"Education Details 

Hadoop Developer 

Hadoop Developer - INFOSYS
Skill Details 
Company Details 
company - INFOSYS
description - Project Description: The banking information had stored the data in different data ware house systems for each department but it becomes difficult for the organization to manage the data and to perform some analytics on the past data, so it is combined them into a single global repository in Hadoop for analysis.

Responsibilities:
â¢       Analyze the banking rates data set.
â¢       Create specification document.
â¢       Provide effort estimation.
â¢       Develop SPARK Scala, SPARK SQL Programs using Eclipse IDE on Windows/Linux environment.
â¢       Create KPI's test scenarios, test cases, test result document.
â¢       Test the Scala programs in Linux Spark Standalone mode.
â¢       setup multi cluster on AWS, deploy the Spark Scala programs
â¢       Provided solution using Hadoop ecosystem - HDFS, MapReduce, Pig, Hive, HBase, and Zookeeper.
â¢       Provided solution using large scale server-side systems with distributed processing algorithms.
â¢       Created reports for the BI team using Sqoop to export data into HDFS and Hive.
â¢       Provided solution in supporting and assisting in troubleshooting and optimization of MapReduce jobs and
Pig Latin scripts.
â¢       Deep understanding of Hadoop design principles, cluster connectivity, security and the factors that affect
system performance.
â¢       Worked on Importing and exporting data from different databases like Oracle, Teradata into HDFS and Hive
using Sqoop, TPT and Connect Direct.
â¢       Import and export the data from RDBMS to HDFS/HBASE
â¢       Wrote script and placed it in client side so that the data moved to HDFS will be stored in temporary file and then it will start loading it in hive tables.
â¢       Developed the Sqoop scripts in order to make the interaction between Pig and MySQL Database.
â¢       Involved in developing the Hive Reports, Partitions of Hive tables.
â¢       Created and maintained technical documentation for launching HADOOP Clusters and for executing HIVE
queries and PIG scripts.
â¢       Involved in running Hadoop jobs for processing millions of records of text data

Environment: Java, Hadoop, HDFS, Map-Reduce, Pig, Hive, Sqoop, Flume, Oozie, HBase, Spark, Scala,
Linux, NoSQL, Storm, Tomcat, Putty, SVN, GitHub, IBM WebSphere v8.5.

Project #1: TELECOMMUNICATIONS
Hadoop Developer

Description To identify customers who are likely to churn and 360-degree view of the customer is created from different heterogeneous data sources. The data is brought into data lake (HDFS) from different sources and analyzed using different Hadoop tools like pig and hive.

Responsibilities:
â¢       Installed and Configured Apache Hadoop tools like Hive, Pig, HBase and Sqoop for application development and unit testing.
â¢       Wrote MapReduce jobs to discover trends in data usage by users.
â¢       Involved in database connection using SQOOP.
â¢       Involved in creating Hive tables, loading data and writing hive queries Using the HiveQL.
â¢       Involved in partitioning and joining Hive tables for Hive query optimization.
â¢       Experienced in SQL DB Migration to HDFS.
â¢       Used NoSQL(HBase) for faster performance, which maintains the data in the De-Normalized way for OLTP.
â¢       The data is collected from distributed sources into Avro models. Applied transformations and standardizations and loaded into HBase for further data processing.
â¢       Experienced in defining job flows.
â¢       Used Oozie to orchestrate the workflow.
â¢       Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce
jobs given by the users.
â¢       Exported the analyzed data to the relational databases using HIVE for visualization and to generate reports for the BI team.

Environment: Hadoop, Hive, Linux, MapReduce, HDFS, Hive, Python, Pig, Sqoop, Cloudera, Shell Scripting,
Java (JDK 1.6), Java 6, Oracle 10g, PL/SQL, SQL*PLUS"
Hadoop,"Skill Set: Hadoop, Map Reduce, HDFS, Hive, Sqoop, java. Duration: 2016 to 2017. Role: Hadoop Developer Rplus offers an quick, simple and powerful cloud based Solution, Demand Sense to accurately predict demand for your product in all your markets which Combines Enterprise and External Data to predict demand more accurately through Uses Social Conversation and Sentiments to derive demand and Identifies significant drivers of sale out of hordes of factors that Selects the best suited model out of multiple forecasting models for each product. Responsibilities: â¢ Involved in deploying the product for customers, gathering requirements and algorithm optimization at backend of the product. â¢ Load and transform Large Datasets of structured semi structured. â¢ Responsible to manage data coming from different sources and application â¢ Supported Map Reduce Programs those are running on the cluster â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.Education Details 

Hadoop Developer 

Hadoop Developer - Braindatawire
Skill Details 
APACHE HADOOP HDFS- Exprience - 49 months
APACHE HADOOP SQOOP- Exprience - 49 months
Hadoop- Exprience - 49 months
HADOOP- Exprience - 49 months
HADOOP DISTRIBUTED FILE SYSTEM- Exprience - 49 monthsCompany Details 
company - Braindatawire
description - Technical Skills:
â¢   Programming: Core Java, Map Reduce, Scala
â¢   Hadoop Tools: HDFS, Spark, Map Reduce, Sqoop, Hive, Hbase
â¢   Database: MySQL, Oracle
â¢   Scripting: Shell Scripting
â¢   IDE: Eclipse
â¢   Operating Systems: Linux (CentOS), Windows
â¢   Source Control: Git (Github)"
Hadoop,"â¢ Operating systems:-Linux- Ubuntu, Windows 2007/08 â¢ Other tools:- Tableau, SVN, Beyond Compare.Education Details 
January 2016 Bachelors of Engineering Engineering  Gujarat Technological University
Systems Engineer/Hadoop Developer 

Systems Engineer/Hadoop Developer - Tata Consultancy Services
Skill Details 
Hadoop,Spark,Sqoop,Hive,Flume,Pig- Exprience - 24 monthsCompany Details 
company - Tata Consultancy Services
description - Roles and responsibility:

Working for a American pharmaceutical company (one of the world's premier
biopharmaceutical) who develops and produces medicines and vaccines for a wide range of medical
disciplines, including immunology, oncology, cardiology, endocrinology, and neurology. To handle large
amount of United Healthcare data big data analytics is used. Data from all possible data sources like records of all Patients(Old and New), records of medicines, Treatment Pathways & Patient Journey for
Health Outcomes, Patient Finder (or Rare Disease Patient Finder), etc being gathered, stored and processed at one place.

â¢     Worked on cluster with specs as:
o    Cluster Architecture: Fully
Distributed Package Used:
CDH3
o    Cluster Capacity: 20 TB
o    No. of Nodes: 10 Data Nodes + 3 Masters + NFS Backup For NN

â¢     Developed proof of concepts for enterprise adoption of Hadoop.
â¢   Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera
distribution.
â¢   Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and trouble-shooting, manage and review data backups, and reviewing Hadoop log files.
â¢   Imported & exported large data sets of data into HDFS and vice-versa using sqoop.
â¢   Involved developing the Pig scripts and Hive Reports
â¢   Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive
partition.Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive.
â¢   Developed Spark scripts by using Scala shell commands as per the requirement and worked with both
Data frames/SQL/Data sets and RDD/MapReduce in Spark. Optimizing of existing algorithms in Hadoop
using SparkContext, Spark-SQL, Data Frames and RDD's.
â¢   Collaborated with infrastructure, network, database, application and BI to ensure data, quality and availability.
â¢   Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop.
â¢   Used ORC & Parquet file formats for serialization of data, and Snappy for the compression of the data.

Achievements

â¢   Appreciation for showing articulate leadership qualities in doing work with the team.
â¢   Completed the internal certification of TCS Certified Hadoop Developer.

Ongoing Learning
â¢   Preparing and scheduled the Cloudera Certified Spark Developer CCA 175."
Hadoop,"Areas of expertise â¢ Big Data Ecosystems: Hadoop-HDFS, MapReduce, Hive, Pig, Sqoop, HBase Oozie, Spark, Pyspark, HUE and having knowledge on cassandra â¢ Programming Languages: Python, Core Java and have an idea on Scala â¢ Databases: Oracle 10g, MySQL, Sqlserver NoSQL - HBase, Cassandra â¢ Tools: Eclipse, Toad, FTP, Tectia, Putty, Autosys, Anaconda, Jupyter notebool and Devops - RTC, RLM. â¢ Scripting Languages: JSP â¢ Platforms: Windows, UnixEducation Details 
 M.Tech (IT-DBS) B.Tech (CSE)  SRM University
Software Engineer 

Software Engineer - Larsen and Toubro
Skill Details 
Company Details 
company - Larsen and Toubro
description - Worked as a Software Engineer in Technosoft Corporation, Chennai from Aug 2015 to sep 2016.
company - Current Project
description - Duration: September 2016 to Till date
Vendor: Citi bank
Description:
Citibank's (Citi) Anti-Money Laundering (AML) Transaction Monitoring (TM) program is a future state solution and a rules-based system for transaction monitoring of ICG-Markets business.
Roles and Responesbilities:
â¢ Building and providing domain knowledge for Anti Money Laundering among team members.
â¢ The layered architecture has Data Warehouse and Workspace layers which are used by Business Analysts.
â¢ Actively involved in designing of star-schema model involving various Dimensions and Fact tables.
â¢ Designed SCD2 for maintaining history of the DIM data.
â¢ Developing Hive Queries for mapping data between different layers of architecture, and it's usage in Oozie Workflows.
â¢ Integration with Data Quality and Reconciliation Module.
â¢ Regression and Integration testing of solution for any issues in integration with other modules and effectively testing the data flow from layer-to-layer.
â¢ Transaction monitoring system development to generate Alerts for the suspicious and fraudulent transactions based on requirements provide by BAs.
â¢ Developing spark Jobs for various business rules.
â¢ Learning ""Machine Learning"", which will be used further in the project for developing an effective model for Fraud detection for Anti Money Laundering system.
â¢ Scheduling Jobs using Autosys tool.
â¢ Deployment and Code Management using RTC and RLM(Release Lifecycle Management)

Hadoop Developer
#  Current Project: PRTS - RAN
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Sqoop, HBase, Tez, Tableau, Sqlserver, Teradata
Cluster Size: 96 Node Cluster.
Distribution: Horton works - HDP2.3
company - Alcatel lucent
description - 1X) and  Ruckus Wireless
Description:
The scope of this project is to maintain and store the operational and parameters data collected from the multiple vendors networks by the mediation team into the OMS data store and make it available for RF engineers to boost the network performance.
Responsibilities:
â¢ Working with Hadoop Distributed File System.
â¢ Involved in importing data from MySQL to HDFS using SQOOP.
â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run  on top of  Tez execution Engine.
â¢ Involved in Preparing Test cases Document.
â¢ Involved in Integrating Hive and HBase to store the operational data.
â¢ Monitoring the Jobs through Oozie.
company - Current Project
description - Anti - Money laundering
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Oozie, Spark, Unix, Autosys, Python, RTC, RLM, ETL Framwe work
Cluster Size: 56 Node Cluster.
Distribution: Cloudera 5.9.14"
Hadoop,"Technical Skill Set: Programming Languages Apache Hadoop, Python, shell scripting, SQL Technologies Hive, Pig, Sqoop, Flume, Oozie, Impala, hdfs Tools Dataiku, Unravel, Cloudera, Putty, HUE, Cloudera Manager, Eclipse, Resource Manager Initial Learning Program: Tata Consultancy Services: June 2015 to August 2015 Description: This is a learning program conducted by TCS for the newly joined employees, to accomplish them to learn the working standard of the organization. During this period employee are groomed with various technical as well as ethical aspects. Education Details 
 B.E. Electronics & Communication Indore, Madhya Pradesh Medi-caps Institute of Technology & Management
Hadoop developer 

hadoop,hive,sqoop,flume,pig,mapreduce,python,impala,spark,scala,sql,unix.
Skill Details 
APACHE HADOOP SQOOP- Exprience - 31 months
Hadoop- Exprience - 31 months
HADOOP- Exprience - 31 months
Hive- Exprience - 31 months
SQOOP- Exprience - 31 months
python- Exprience - Less than 1 year months
hdfs- Exprience - Less than 1 year months
unix- Exprience - Less than 1 year months
impala- Exprience - Less than 1 year months
pig- Exprience - Less than 1 year months
unravel- Exprience - Less than 1 year months
mapreduce- Exprience - Less than 1 year months
dataiku- Exprience - Less than 1 year monthsCompany Details 
company - Tata Consultancy Services
description - Project Description
Data warehouse division has multiple products for injecting, storing, analysing and presenting data. The Data Lake program is started to provide multi-talent, secure data hub to store application's data on Hadoop platform with strong data governance, lineage, auditing and monitoring capabilities. The object of the project is to provide necessary engineering support to analytics and application teams so that they can focus on the business logic development. In this project, the major task is to set up the Hadoop cluster and govern all the activities which are required for the smooth functioning of various Hadoop ecosystems. As the day and day data increasing so to provide stability to the ecosystem and smooth working of it, Developing and automating the various requirement specific utilities.

Responsibility 1. Developed proactive Health Check utility for Data Lake. The utility proactively checks the smooth functioning of all Hadoop components on the cluster and sends the result to email in HTML format. The utility is being used for daily Health Checks as well as after upgrades.
2. Getting the data in different formats and processing the data in Hadoop ecosystem after filtering the data using the appropriate techniques.
3. Developed data pipeline utility to ingest data from RDBMS database to Hive external tables using Sqoop commands. The utility also offers the data quality check like row count validation.
4. Developed and automated various cluster health check, usage, capacity related reports using Unix shell scripting.
5. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
6. Creating flume agents to process the data to Hadoop ecosystem side.
7. Performed benchmark testing on the Hive Queries and impala queries.
8. Involved in setting up the cluster and its components like edge node and HA implementation of the services: Hive Server2, Impala, and HDFS.
9. Filtering the required data from available data using different technologies like pig, regex Serde etc.
10. Dataiku benchmark testing on top of impala and hive in compare to Greenplum database.
11. Moving the data from Greenplum database to Hadoop side with help of Sqoop pipeline, process the data to Hadoop side and storing the data into hive tables to do the performance testing.
12. Dealing with the Hadoop ecosystem related issues in order to provide stability to WM Hadoop ecosystem.
13. Rescheduling of job from autosys job hosting to TWS job hosting for better performance.

Declaration:
I hereby declare that the above mentioned information is authentic to the best of my knowledge
company - Tata Consultancy Services
description - Clients: 1. Barclays 2. Union bank of California (UBC) 3. Morgan Stanley (MS)

KEY PROJECTS HANDLED
Project Name ABSA- Reconciliations, UBC and WMDATALAKE COE
company - Tata Consultancy Services
description - Project Description
Migration of data from RDBMS database to Hive (Hadoop ecosystem) . Hadoop platform ability with strong data governance, lineage, auditing and monitoring capabilities. The objective of this project was to speed up the data processing so that the analysis and decision making become easy. Due to RDBMS limitations to process waste amount of data at once and produce the results at the earliest, Client wanted to move the data to Hadoop ecosystem so that they can over-come from those limitations and focus on business improvement only.

Responsibility 1. Optimising the SQL queries for those data which were not required to move from RDBMS to any other platform.
2. Writing the Hive queries and logic to move the data from RDBMS to Hadoop ecosystem.
3. Writing the hive queries to analyse the required data as per the business requirements.
4. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
5. Writing the sqoop commands and scripts to move the data from RDBMS to Hadoop side.
company - Tata Consultancy Services
description - Project Description
Create recs and migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.

Responsibility 1. Have worked on extracting business requirements, analyzing and implementing them in developing Recs 2. Worked on migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.
3. Done the back end work where most of the things were related to writing the sql queries and provide the data for the new recs.

Project Name   PSO"
Hadoop,"Technical Skills Programming Languages: C, C++, Java, .Net., J2EE, HTML5, CSS, MapReduce Scripting Languages: Javascript, Python Databases: Oracle (PL-SQL), MY-SQL, IBM DB2 Tools:IBM Rational Rose, R, Weka Operating Systems: Windows XP, Vista, UNIX, Windows 7, Red Hat 7Education Details 
January 2015 B.E  Pimpri Chinchwad, MAHARASHTRA, IN Pimpri Chinchwad College of Engineering
January 2012 Diploma MSBTE  Dnyanganaga Polytechnic
 S.S.C   New English School Takali
Hadoop/Big Data Developer 

Hadoop/Big Data Developer - British Telecom
Skill Details 
APACHE HADOOP MAPREDUCE- Exprience - 37 months
MapReduce- Exprience - 37 months
MAPREDUCE- Exprience - 37 months
JAVA- Exprience - 32 months
.NET- Exprience - 6 monthsCompany Details 
company - British Telecom
description - Project: British Telecom project (UK)
Responsibilities:
â¢ Working on HDFS, MapReduce, Hive, Spark, Scala, Sqoop, Kerberos etc. technologies
â¢ Implemented various data mining algorithms on Spark like K-means clustering, Random forest, NaÃ¯ve bayes etc.
â¢ A knowledge of installing, configuring, maintaining and securing Hadoop.
company - DXC technology
description - HPE legacy), Bangalore
â¢ Worked on Hadoop + Java programming
â¢ Worked on Azure and AWS (EMR) services.
â¢ Worked on HDInsight Hadoop cluster..
â¢ Design, develop, document and architect Hadoop applications
â¢ Develop MapReduce coding that works seamlessly on Hadoop clusters.
â¢ Analyzing and processing the large data sets on HDFS.
â¢ An analytical bent of mind and ability to learn-unlearn-relearn surely comes in handy."
Hadoop,"Technical Skill Set Big Data Ecosystems: Hadoop, HDFS, HBase, Map Reduce, Sqoop, Hive, Pig, Spark-Core, Flume. Other Language: Scala, Core-Java, SQL, PLSQL, Sell Scripting ETL Tools: Informatica Power Center8.x/9.6, Talend 5.6 Tools: Eclipse, Intellij Idea. Platforms: Windows Family, Linux /UNIX, Cloudera. Databases: MySQL, Oracle.10/11gEducation Details 
 M.C.A  Pune, MAHARASHTRA, IN Pune University
Hodoop Developer 

Hodoop Developer - PRGX India Private Limited Pune
Skill Details 
Company Details 
company - PRGX India Private Limited Pune
description - Team Size: 10+
Environment: Hive, Spark, Sqoop, Scala and Flume.

Project Description:
The bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. The customers spending ranges from online shopping, medical expenses in hospitals, cash transactions, and debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. The portal allows the customers to login and see their transactions which they make on a day to day basis .the analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. The portal used hadoop framework to analyes the data as per the rules and regulations placed by the regulators from the respective countries. The offers and the interest rates also complied with the regulations and all these processing was done using the hadoop framework as big data analytics system.

Role & Responsibilities:
â Import data from legacy system to hadoop using Sqoop, flume.
â Implement the business logic to analyses  the data
â Per-process data using spark.
â Create hive script and loading data into hive.
â Sourcing various attributes to the data processing logic to retrieve the correct results.

Project 2
company - PRGX India Private Limited Pune
description - 
company - PRGX India Private Limited Pune
description - Team Size: 11+
Environment: Hadoop, HDFS, Hive, Sqoop, MySQL, Map Reduce

Project Description:-
The Purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it.the solution was based on the open source s/w hadoop. The data will be stored in hadoop file system and processed using Map/Reduce jobs. Which in trun includes getting the raw html data from the micro websites, process the html to obtain product and user information, extract various reports out of the vistor tracking information and export the information for further processing

Role & Responsibilities:
â Move all crawl data flat files generated from various micro sites to HDFS for further processing.
â Sqoop implementation for interaction with database
â Write Map Reduce scripts to process the data file.
â Create hive tables to store the processed data in tabular formats.
â Reports creation from hive data.

Project 3
company - PRGX India Private Limited Pune
description - Team Size: 15+
Environment: Informatica 9.5, Oracle11g, UNIX

Project Description:
Pfizer Inc. is an American global pharmaceutical corporation headquartered in New York City. The main objective of the project is to build a Development Data Repository for Pfizer Inc. Because all the downstream application are like Etrack, TSP database, RTS, SADMS, GFS, GDO having their own sql request on the OLTP system directly due to which the performance of OLTP system goes slows down. For this we have created a Development Data Repository to replace the entire sql request directly on the OLTP system. DDR process extracts all clinical, pre-clinical, study, product, subject, sites related information from the upstream applications like EPECS, CDSS, RCM, PRC, E-CLINICAL, EDH and after applying some business logic put it into DDR core tables. From these snapshot and dimensional layer are created which are used for reporting application.

Role & Responsibilities:
â To understand & analyze the requirement documents and resolve the queries.
â To design Informatica mappings by using various basic transformations like Filter, Router, Source qualifier, Lookup etc and advance transformations like Aggregators, Joiner, Sorters and so on.
â Perform cross Unit and Integration testing for mappings developed within the team. Reporting bugs and bug fixing.
â Create workflow/batches and set the session dependencies.
â Implemented Change Data Capture using mapping parameters, SCD and SK generation.
â Developed Mapplet, reusable transformations to populate the data into data warehouse.
â Created Sessions & Worklets using workflow Manager to load the data into the Target Database.
â Involved in Unit Case Testing (UTC)
â Performing Unit Testing and UAT for SCD Type1/Type2, fact load and CDC implementation.

Personal Scan

Address: Jijayi Heights, Flat no 118, Narhe, (Police chowki) Pune- 411041"
Hadoop,"Education Details 

Hadoop Developer 

Hadoop Developer - INFOSYS
Skill Details 
Company Details 
company - INFOSYS
description - Project Description: The banking information had stored the data in different data ware house systems for each department but it becomes difficult for the organization to manage the data and to perform some analytics on the past data, so it is combined them into a single global repository in Hadoop for analysis.

Responsibilities:
â¢       Analyze the banking rates data set.
â¢       Create specification document.
â¢       Provide effort estimation.
â¢       Develop SPARK Scala, SPARK SQL Programs using Eclipse IDE on Windows/Linux environment.
â¢       Create KPI's test scenarios, test cases, test result document.
â¢       Test the Scala programs in Linux Spark Standalone mode.
â¢       setup multi cluster on AWS, deploy the Spark Scala programs
â¢       Provided solution using Hadoop ecosystem - HDFS, MapReduce, Pig, Hive, HBase, and Zookeeper.
â¢       Provided solution using large scale server-side systems with distributed processing algorithms.
â¢       Created reports for the BI team using Sqoop to export data into HDFS and Hive.
â¢       Provided solution in supporting and assisting in troubleshooting and optimization of MapReduce jobs and
Pig Latin scripts.
â¢       Deep understanding of Hadoop design principles, cluster connectivity, security and the factors that affect
system performance.
â¢       Worked on Importing and exporting data from different databases like Oracle, Teradata into HDFS and Hive
using Sqoop, TPT and Connect Direct.
â¢       Import and export the data from RDBMS to HDFS/HBASE
â¢       Wrote script and placed it in client side so that the data moved to HDFS will be stored in temporary file and then it will start loading it in hive tables.
â¢       Developed the Sqoop scripts in order to make the interaction between Pig and MySQL Database.
â¢       Involved in developing the Hive Reports, Partitions of Hive tables.
â¢       Created and maintained technical documentation for launching HADOOP Clusters and for executing HIVE
queries and PIG scripts.
â¢       Involved in running Hadoop jobs for processing millions of records of text data

Environment: Java, Hadoop, HDFS, Map-Reduce, Pig, Hive, Sqoop, Flume, Oozie, HBase, Spark, Scala,
Linux, NoSQL, Storm, Tomcat, Putty, SVN, GitHub, IBM WebSphere v8.5.

Project #1: TELECOMMUNICATIONS
Hadoop Developer

Description To identify customers who are likely to churn and 360-degree view of the customer is created from different heterogeneous data sources. The data is brought into data lake (HDFS) from different sources and analyzed using different Hadoop tools like pig and hive.

Responsibilities:
â¢       Installed and Configured Apache Hadoop tools like Hive, Pig, HBase and Sqoop for application development and unit testing.
â¢       Wrote MapReduce jobs to discover trends in data usage by users.
â¢       Involved in database connection using SQOOP.
â¢       Involved in creating Hive tables, loading data and writing hive queries Using the HiveQL.
â¢       Involved in partitioning and joining Hive tables for Hive query optimization.
â¢       Experienced in SQL DB Migration to HDFS.
â¢       Used NoSQL(HBase) for faster performance, which maintains the data in the De-Normalized way for OLTP.
â¢       The data is collected from distributed sources into Avro models. Applied transformations and standardizations and loaded into HBase for further data processing.
â¢       Experienced in defining job flows.
â¢       Used Oozie to orchestrate the workflow.
â¢       Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce
jobs given by the users.
â¢       Exported the analyzed data to the relational databases using HIVE for visualization and to generate reports for the BI team.

Environment: Hadoop, Hive, Linux, MapReduce, HDFS, Hive, Python, Pig, Sqoop, Cloudera, Shell Scripting,
Java (JDK 1.6), Java 6, Oracle 10g, PL/SQL, SQL*PLUS"
Hadoop,"Skill Set: Hadoop, Map Reduce, HDFS, Hive, Sqoop, java. Duration: 2016 to 2017. Role: Hadoop Developer Rplus offers an quick, simple and powerful cloud based Solution, Demand Sense to accurately predict demand for your product in all your markets which Combines Enterprise and External Data to predict demand more accurately through Uses Social Conversation and Sentiments to derive demand and Identifies significant drivers of sale out of hordes of factors that Selects the best suited model out of multiple forecasting models for each product. Responsibilities: â¢ Involved in deploying the product for customers, gathering requirements and algorithm optimization at backend of the product. â¢ Load and transform Large Datasets of structured semi structured. â¢ Responsible to manage data coming from different sources and application â¢ Supported Map Reduce Programs those are running on the cluster â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.Education Details 

Hadoop Developer 

Hadoop Developer - Braindatawire
Skill Details 
APACHE HADOOP HDFS- Exprience - 49 months
APACHE HADOOP SQOOP- Exprience - 49 months
Hadoop- Exprience - 49 months
HADOOP- Exprience - 49 months
HADOOP DISTRIBUTED FILE SYSTEM- Exprience - 49 monthsCompany Details 
company - Braindatawire
description - Technical Skills:
â¢   Programming: Core Java, Map Reduce, Scala
â¢   Hadoop Tools: HDFS, Spark, Map Reduce, Sqoop, Hive, Hbase
â¢   Database: MySQL, Oracle
â¢   Scripting: Shell Scripting
â¢   IDE: Eclipse
â¢   Operating Systems: Linux (CentOS), Windows
â¢   Source Control: Git (Github)"
Hadoop,"â¢ Operating systems:-Linux- Ubuntu, Windows 2007/08 â¢ Other tools:- Tableau, SVN, Beyond Compare.Education Details 
January 2016 Bachelors of Engineering Engineering  Gujarat Technological University
Systems Engineer/Hadoop Developer 

Systems Engineer/Hadoop Developer - Tata Consultancy Services
Skill Details 
Hadoop,Spark,Sqoop,Hive,Flume,Pig- Exprience - 24 monthsCompany Details 
company - Tata Consultancy Services
description - Roles and responsibility:

Working for a American pharmaceutical company (one of the world's premier
biopharmaceutical) who develops and produces medicines and vaccines for a wide range of medical
disciplines, including immunology, oncology, cardiology, endocrinology, and neurology. To handle large
amount of United Healthcare data big data analytics is used. Data from all possible data sources like records of all Patients(Old and New), records of medicines, Treatment Pathways & Patient Journey for
Health Outcomes, Patient Finder (or Rare Disease Patient Finder), etc being gathered, stored and processed at one place.

â¢     Worked on cluster with specs as:
o    Cluster Architecture: Fully
Distributed Package Used:
CDH3
o    Cluster Capacity: 20 TB
o    No. of Nodes: 10 Data Nodes + 3 Masters + NFS Backup For NN

â¢     Developed proof of concepts for enterprise adoption of Hadoop.
â¢   Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera
distribution.
â¢   Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and trouble-shooting, manage and review data backups, and reviewing Hadoop log files.
â¢   Imported & exported large data sets of data into HDFS and vice-versa using sqoop.
â¢   Involved developing the Pig scripts and Hive Reports
â¢   Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive
partition.Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive.
â¢   Developed Spark scripts by using Scala shell commands as per the requirement and worked with both
Data frames/SQL/Data sets and RDD/MapReduce in Spark. Optimizing of existing algorithms in Hadoop
using SparkContext, Spark-SQL, Data Frames and RDD's.
â¢   Collaborated with infrastructure, network, database, application and BI to ensure data, quality and availability.
â¢   Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop.
â¢   Used ORC & Parquet file formats for serialization of data, and Snappy for the compression of the data.

Achievements

â¢   Appreciation for showing articulate leadership qualities in doing work with the team.
â¢   Completed the internal certification of TCS Certified Hadoop Developer.

Ongoing Learning
â¢   Preparing and scheduled the Cloudera Certified Spark Developer CCA 175."
Hadoop,"Areas of expertise â¢ Big Data Ecosystems: Hadoop-HDFS, MapReduce, Hive, Pig, Sqoop, HBase Oozie, Spark, Pyspark, HUE and having knowledge on cassandra â¢ Programming Languages: Python, Core Java and have an idea on Scala â¢ Databases: Oracle 10g, MySQL, Sqlserver NoSQL - HBase, Cassandra â¢ Tools: Eclipse, Toad, FTP, Tectia, Putty, Autosys, Anaconda, Jupyter notebool and Devops - RTC, RLM. â¢ Scripting Languages: JSP â¢ Platforms: Windows, UnixEducation Details 
 M.Tech (IT-DBS) B.Tech (CSE)  SRM University
Software Engineer 

Software Engineer - Larsen and Toubro
Skill Details 
Company Details 
company - Larsen and Toubro
description - Worked as a Software Engineer in Technosoft Corporation, Chennai from Aug 2015 to sep 2016.
company - Current Project
description - Duration: September 2016 to Till date
Vendor: Citi bank
Description:
Citibank's (Citi) Anti-Money Laundering (AML) Transaction Monitoring (TM) program is a future state solution and a rules-based system for transaction monitoring of ICG-Markets business.
Roles and Responesbilities:
â¢ Building and providing domain knowledge for Anti Money Laundering among team members.
â¢ The layered architecture has Data Warehouse and Workspace layers which are used by Business Analysts.
â¢ Actively involved in designing of star-schema model involving various Dimensions and Fact tables.
â¢ Designed SCD2 for maintaining history of the DIM data.
â¢ Developing Hive Queries for mapping data between different layers of architecture, and it's usage in Oozie Workflows.
â¢ Integration with Data Quality and Reconciliation Module.
â¢ Regression and Integration testing of solution for any issues in integration with other modules and effectively testing the data flow from layer-to-layer.
â¢ Transaction monitoring system development to generate Alerts for the suspicious and fraudulent transactions based on requirements provide by BAs.
â¢ Developing spark Jobs for various business rules.
â¢ Learning ""Machine Learning"", which will be used further in the project for developing an effective model for Fraud detection for Anti Money Laundering system.
â¢ Scheduling Jobs using Autosys tool.
â¢ Deployment and Code Management using RTC and RLM(Release Lifecycle Management)

Hadoop Developer
#  Current Project: PRTS - RAN
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Sqoop, HBase, Tez, Tableau, Sqlserver, Teradata
Cluster Size: 96 Node Cluster.
Distribution: Horton works - HDP2.3
company - Alcatel lucent
description - 1X) and  Ruckus Wireless
Description:
The scope of this project is to maintain and store the operational and parameters data collected from the multiple vendors networks by the mediation team into the OMS data store and make it available for RF engineers to boost the network performance.
Responsibilities:
â¢ Working with Hadoop Distributed File System.
â¢ Involved in importing data from MySQL to HDFS using SQOOP.
â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run  on top of  Tez execution Engine.
â¢ Involved in Preparing Test cases Document.
â¢ Involved in Integrating Hive and HBase to store the operational data.
â¢ Monitoring the Jobs through Oozie.
company - Current Project
description - Anti - Money laundering
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Oozie, Spark, Unix, Autosys, Python, RTC, RLM, ETL Framwe work
Cluster Size: 56 Node Cluster.
Distribution: Cloudera 5.9.14"
Hadoop,"Technical Skill Set: Programming Languages Apache Hadoop, Python, shell scripting, SQL Technologies Hive, Pig, Sqoop, Flume, Oozie, Impala, hdfs Tools Dataiku, Unravel, Cloudera, Putty, HUE, Cloudera Manager, Eclipse, Resource Manager Initial Learning Program: Tata Consultancy Services: June 2015 to August 2015 Description: This is a learning program conducted by TCS for the newly joined employees, to accomplish them to learn the working standard of the organization. During this period employee are groomed with various technical as well as ethical aspects. Education Details 
 B.E. Electronics & Communication Indore, Madhya Pradesh Medi-caps Institute of Technology & Management
Hadoop developer 

hadoop,hive,sqoop,flume,pig,mapreduce,python,impala,spark,scala,sql,unix.
Skill Details 
APACHE HADOOP SQOOP- Exprience - 31 months
Hadoop- Exprience - 31 months
HADOOP- Exprience - 31 months
Hive- Exprience - 31 months
SQOOP- Exprience - 31 months
python- Exprience - Less than 1 year months
hdfs- Exprience - Less than 1 year months
unix- Exprience - Less than 1 year months
impala- Exprience - Less than 1 year months
pig- Exprience - Less than 1 year months
unravel- Exprience - Less than 1 year months
mapreduce- Exprience - Less than 1 year months
dataiku- Exprience - Less than 1 year monthsCompany Details 
company - Tata Consultancy Services
description - Project Description
Data warehouse division has multiple products for injecting, storing, analysing and presenting data. The Data Lake program is started to provide multi-talent, secure data hub to store application's data on Hadoop platform with strong data governance, lineage, auditing and monitoring capabilities. The object of the project is to provide necessary engineering support to analytics and application teams so that they can focus on the business logic development. In this project, the major task is to set up the Hadoop cluster and govern all the activities which are required for the smooth functioning of various Hadoop ecosystems. As the day and day data increasing so to provide stability to the ecosystem and smooth working of it, Developing and automating the various requirement specific utilities.

Responsibility 1. Developed proactive Health Check utility for Data Lake. The utility proactively checks the smooth functioning of all Hadoop components on the cluster and sends the result to email in HTML format. The utility is being used for daily Health Checks as well as after upgrades.
2. Getting the data in different formats and processing the data in Hadoop ecosystem after filtering the data using the appropriate techniques.
3. Developed data pipeline utility to ingest data from RDBMS database to Hive external tables using Sqoop commands. The utility also offers the data quality check like row count validation.
4. Developed and automated various cluster health check, usage, capacity related reports using Unix shell scripting.
5. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
6. Creating flume agents to process the data to Hadoop ecosystem side.
7. Performed benchmark testing on the Hive Queries and impala queries.
8. Involved in setting up the cluster and its components like edge node and HA implementation of the services: Hive Server2, Impala, and HDFS.
9. Filtering the required data from available data using different technologies like pig, regex Serde etc.
10. Dataiku benchmark testing on top of impala and hive in compare to Greenplum database.
11. Moving the data from Greenplum database to Hadoop side with help of Sqoop pipeline, process the data to Hadoop side and storing the data into hive tables to do the performance testing.
12. Dealing with the Hadoop ecosystem related issues in order to provide stability to WM Hadoop ecosystem.
13. Rescheduling of job from autosys job hosting to TWS job hosting for better performance.

Declaration:
I hereby declare that the above mentioned information is authentic to the best of my knowledge
company - Tata Consultancy Services
description - Clients: 1. Barclays 2. Union bank of California (UBC) 3. Morgan Stanley (MS)

KEY PROJECTS HANDLED
Project Name ABSA- Reconciliations, UBC and WMDATALAKE COE
company - Tata Consultancy Services
description - Project Description
Migration of data from RDBMS database to Hive (Hadoop ecosystem) . Hadoop platform ability with strong data governance, lineage, auditing and monitoring capabilities. The objective of this project was to speed up the data processing so that the analysis and decision making become easy. Due to RDBMS limitations to process waste amount of data at once and produce the results at the earliest, Client wanted to move the data to Hadoop ecosystem so that they can over-come from those limitations and focus on business improvement only.

Responsibility 1. Optimising the SQL queries for those data which were not required to move from RDBMS to any other platform.
2. Writing the Hive queries and logic to move the data from RDBMS to Hadoop ecosystem.
3. Writing the hive queries to analyse the required data as per the business requirements.
4. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
5. Writing the sqoop commands and scripts to move the data from RDBMS to Hadoop side.
company - Tata Consultancy Services
description - Project Description
Create recs and migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.

Responsibility 1. Have worked on extracting business requirements, analyzing and implementing them in developing Recs 2. Worked on migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.
3. Done the back end work where most of the things were related to writing the sql queries and provide the data for the new recs.

Project Name   PSO"
Hadoop,"Technical Skills Programming Languages: C, C++, Java, .Net., J2EE, HTML5, CSS, MapReduce Scripting Languages: Javascript, Python Databases: Oracle (PL-SQL), MY-SQL, IBM DB2 Tools:IBM Rational Rose, R, Weka Operating Systems: Windows XP, Vista, UNIX, Windows 7, Red Hat 7Education Details 
January 2015 B.E  Pimpri Chinchwad, MAHARASHTRA, IN Pimpri Chinchwad College of Engineering
January 2012 Diploma MSBTE  Dnyanganaga Polytechnic
 S.S.C   New English School Takali
Hadoop/Big Data Developer 

Hadoop/Big Data Developer - British Telecom
Skill Details 
APACHE HADOOP MAPREDUCE- Exprience - 37 months
MapReduce- Exprience - 37 months
MAPREDUCE- Exprience - 37 months
JAVA- Exprience - 32 months
.NET- Exprience - 6 monthsCompany Details 
company - British Telecom
description - Project: British Telecom project (UK)
Responsibilities:
â¢ Working on HDFS, MapReduce, Hive, Spark, Scala, Sqoop, Kerberos etc. technologies
â¢ Implemented various data mining algorithms on Spark like K-means clustering, Random forest, NaÃ¯ve bayes etc.
â¢ A knowledge of installing, configuring, maintaining and securing Hadoop.
company - DXC technology
description - HPE legacy), Bangalore
â¢ Worked on Hadoop + Java programming
â¢ Worked on Azure and AWS (EMR) services.
â¢ Worked on HDInsight Hadoop cluster..
â¢ Design, develop, document and architect Hadoop applications
â¢ Develop MapReduce coding that works seamlessly on Hadoop clusters.
â¢ Analyzing and processing the large data sets on HDFS.
â¢ An analytical bent of mind and ability to learn-unlearn-relearn surely comes in handy."
Hadoop,"Technical Skill Set Big Data Ecosystems: Hadoop, HDFS, HBase, Map Reduce, Sqoop, Hive, Pig, Spark-Core, Flume. Other Language: Scala, Core-Java, SQL, PLSQL, Sell Scripting ETL Tools: Informatica Power Center8.x/9.6, Talend 5.6 Tools: Eclipse, Intellij Idea. Platforms: Windows Family, Linux /UNIX, Cloudera. Databases: MySQL, Oracle.10/11gEducation Details 
 M.C.A  Pune, MAHARASHTRA, IN Pune University
Hodoop Developer 

Hodoop Developer - PRGX India Private Limited Pune
Skill Details 
Company Details 
company - PRGX India Private Limited Pune
description - Team Size: 10+
Environment: Hive, Spark, Sqoop, Scala and Flume.

Project Description:
The bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. The customers spending ranges from online shopping, medical expenses in hospitals, cash transactions, and debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. The portal allows the customers to login and see their transactions which they make on a day to day basis .the analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. The portal used hadoop framework to analyes the data as per the rules and regulations placed by the regulators from the respective countries. The offers and the interest rates also complied with the regulations and all these processing was done using the hadoop framework as big data analytics system.

Role & Responsibilities:
â Import data from legacy system to hadoop using Sqoop, flume.
â Implement the business logic to analyses  the data
â Per-process data using spark.
â Create hive script and loading data into hive.
â Sourcing various attributes to the data processing logic to retrieve the correct results.

Project 2
company - PRGX India Private Limited Pune
description - 
company - PRGX India Private Limited Pune
description - Team Size: 11+
Environment: Hadoop, HDFS, Hive, Sqoop, MySQL, Map Reduce

Project Description:-
The Purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it.the solution was based on the open source s/w hadoop. The data will be stored in hadoop file system and processed using Map/Reduce jobs. Which in trun includes getting the raw html data from the micro websites, process the html to obtain product and user information, extract various reports out of the vistor tracking information and export the information for further processing

Role & Responsibilities:
â Move all crawl data flat files generated from various micro sites to HDFS for further processing.
â Sqoop implementation for interaction with database
â Write Map Reduce scripts to process the data file.
â Create hive tables to store the processed data in tabular formats.
â Reports creation from hive data.

Project 3
company - PRGX India Private Limited Pune
description - Team Size: 15+
Environment: Informatica 9.5, Oracle11g, UNIX

Project Description:
Pfizer Inc. is an American global pharmaceutical corporation headquartered in New York City. The main objective of the project is to build a Development Data Repository for Pfizer Inc. Because all the downstream application are like Etrack, TSP database, RTS, SADMS, GFS, GDO having their own sql request on the OLTP system directly due to which the performance of OLTP system goes slows down. For this we have created a Development Data Repository to replace the entire sql request directly on the OLTP system. DDR process extracts all clinical, pre-clinical, study, product, subject, sites related information from the upstream applications like EPECS, CDSS, RCM, PRC, E-CLINICAL, EDH and after applying some business logic put it into DDR core tables. From these snapshot and dimensional layer are created which are used for reporting application.

Role & Responsibilities:
â To understand & analyze the requirement documents and resolve the queries.
â To design Informatica mappings by using various basic transformations like Filter, Router, Source qualifier, Lookup etc and advance transformations like Aggregators, Joiner, Sorters and so on.
â Perform cross Unit and Integration testing for mappings developed within the team. Reporting bugs and bug fixing.
â Create workflow/batches and set the session dependencies.
â Implemented Change Data Capture using mapping parameters, SCD and SK generation.
â Developed Mapplet, reusable transformations to populate the data into data warehouse.
â Created Sessions & Worklets using workflow Manager to load the data into the Target Database.
â Involved in Unit Case Testing (UTC)
â Performing Unit Testing and UAT for SCD Type1/Type2, fact load and CDC implementation.

Personal Scan

Address: Jijayi Heights, Flat no 118, Narhe, (Police chowki) Pune- 411041"
Hadoop,"Education Details 

Hadoop Developer 

Hadoop Developer - INFOSYS
Skill Details 
Company Details 
company - INFOSYS
description - Project Description: The banking information had stored the data in different data ware house systems for each department but it becomes difficult for the organization to manage the data and to perform some analytics on the past data, so it is combined them into a single global repository in Hadoop for analysis.

Responsibilities:
â¢       Analyze the banking rates data set.
â¢       Create specification document.
â¢       Provide effort estimation.
â¢       Develop SPARK Scala, SPARK SQL Programs using Eclipse IDE on Windows/Linux environment.
â¢       Create KPI's test scenarios, test cases, test result document.
â¢       Test the Scala programs in Linux Spark Standalone mode.
â¢       setup multi cluster on AWS, deploy the Spark Scala programs
â¢       Provided solution using Hadoop ecosystem - HDFS, MapReduce, Pig, Hive, HBase, and Zookeeper.
â¢       Provided solution using large scale server-side systems with distributed processing algorithms.
â¢       Created reports for the BI team using Sqoop to export data into HDFS and Hive.
â¢       Provided solution in supporting and assisting in troubleshooting and optimization of MapReduce jobs and
Pig Latin scripts.
â¢       Deep understanding of Hadoop design principles, cluster connectivity, security and the factors that affect
system performance.
â¢       Worked on Importing and exporting data from different databases like Oracle, Teradata into HDFS and Hive
using Sqoop, TPT and Connect Direct.
â¢       Import and export the data from RDBMS to HDFS/HBASE
â¢       Wrote script and placed it in client side so that the data moved to HDFS will be stored in temporary file and then it will start loading it in hive tables.
â¢       Developed the Sqoop scripts in order to make the interaction between Pig and MySQL Database.
â¢       Involved in developing the Hive Reports, Partitions of Hive tables.
â¢       Created and maintained technical documentation for launching HADOOP Clusters and for executing HIVE
queries and PIG scripts.
â¢       Involved in running Hadoop jobs for processing millions of records of text data

Environment: Java, Hadoop, HDFS, Map-Reduce, Pig, Hive, Sqoop, Flume, Oozie, HBase, Spark, Scala,
Linux, NoSQL, Storm, Tomcat, Putty, SVN, GitHub, IBM WebSphere v8.5.

Project #1: TELECOMMUNICATIONS
Hadoop Developer

Description To identify customers who are likely to churn and 360-degree view of the customer is created from different heterogeneous data sources. The data is brought into data lake (HDFS) from different sources and analyzed using different Hadoop tools like pig and hive.

Responsibilities:
â¢       Installed and Configured Apache Hadoop tools like Hive, Pig, HBase and Sqoop for application development and unit testing.
â¢       Wrote MapReduce jobs to discover trends in data usage by users.
â¢       Involved in database connection using SQOOP.
â¢       Involved in creating Hive tables, loading data and writing hive queries Using the HiveQL.
â¢       Involved in partitioning and joining Hive tables for Hive query optimization.
â¢       Experienced in SQL DB Migration to HDFS.
â¢       Used NoSQL(HBase) for faster performance, which maintains the data in the De-Normalized way for OLTP.
â¢       The data is collected from distributed sources into Avro models. Applied transformations and standardizations and loaded into HBase for further data processing.
â¢       Experienced in defining job flows.
â¢       Used Oozie to orchestrate the workflow.
â¢       Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce
jobs given by the users.
â¢       Exported the analyzed data to the relational databases using HIVE for visualization and to generate reports for the BI team.

Environment: Hadoop, Hive, Linux, MapReduce, HDFS, Hive, Python, Pig, Sqoop, Cloudera, Shell Scripting,
Java (JDK 1.6), Java 6, Oracle 10g, PL/SQL, SQL*PLUS"
Hadoop,"Skill Set: Hadoop, Map Reduce, HDFS, Hive, Sqoop, java. Duration: 2016 to 2017. Role: Hadoop Developer Rplus offers an quick, simple and powerful cloud based Solution, Demand Sense to accurately predict demand for your product in all your markets which Combines Enterprise and External Data to predict demand more accurately through Uses Social Conversation and Sentiments to derive demand and Identifies significant drivers of sale out of hordes of factors that Selects the best suited model out of multiple forecasting models for each product. Responsibilities: â¢ Involved in deploying the product for customers, gathering requirements and algorithm optimization at backend of the product. â¢ Load and transform Large Datasets of structured semi structured. â¢ Responsible to manage data coming from different sources and application â¢ Supported Map Reduce Programs those are running on the cluster â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.Education Details 

Hadoop Developer 

Hadoop Developer - Braindatawire
Skill Details 
APACHE HADOOP HDFS- Exprience - 49 months
APACHE HADOOP SQOOP- Exprience - 49 months
Hadoop- Exprience - 49 months
HADOOP- Exprience - 49 months
HADOOP DISTRIBUTED FILE SYSTEM- Exprience - 49 monthsCompany Details 
company - Braindatawire
description - Technical Skills:
â¢   Programming: Core Java, Map Reduce, Scala
â¢   Hadoop Tools: HDFS, Spark, Map Reduce, Sqoop, Hive, Hbase
â¢   Database: MySQL, Oracle
â¢   Scripting: Shell Scripting
â¢   IDE: Eclipse
â¢   Operating Systems: Linux (CentOS), Windows
â¢   Source Control: Git (Github)"
Hadoop,"â¢ Operating systems:-Linux- Ubuntu, Windows 2007/08 â¢ Other tools:- Tableau, SVN, Beyond Compare.Education Details 
January 2016 Bachelors of Engineering Engineering  Gujarat Technological University
Systems Engineer/Hadoop Developer 

Systems Engineer/Hadoop Developer - Tata Consultancy Services
Skill Details 
Hadoop,Spark,Sqoop,Hive,Flume,Pig- Exprience - 24 monthsCompany Details 
company - Tata Consultancy Services
description - Roles and responsibility:

Working for a American pharmaceutical company (one of the world's premier
biopharmaceutical) who develops and produces medicines and vaccines for a wide range of medical
disciplines, including immunology, oncology, cardiology, endocrinology, and neurology. To handle large
amount of United Healthcare data big data analytics is used. Data from all possible data sources like records of all Patients(Old and New), records of medicines, Treatment Pathways & Patient Journey for
Health Outcomes, Patient Finder (or Rare Disease Patient Finder), etc being gathered, stored and processed at one place.

â¢     Worked on cluster with specs as:
o    Cluster Architecture: Fully
Distributed Package Used:
CDH3
o    Cluster Capacity: 20 TB
o    No. of Nodes: 10 Data Nodes + 3 Masters + NFS Backup For NN

â¢     Developed proof of concepts for enterprise adoption of Hadoop.
â¢   Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera
distribution.
â¢   Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and trouble-shooting, manage and review data backups, and reviewing Hadoop log files.
â¢   Imported & exported large data sets of data into HDFS and vice-versa using sqoop.
â¢   Involved developing the Pig scripts and Hive Reports
â¢   Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive
partition.Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive.
â¢   Developed Spark scripts by using Scala shell commands as per the requirement and worked with both
Data frames/SQL/Data sets and RDD/MapReduce in Spark. Optimizing of existing algorithms in Hadoop
using SparkContext, Spark-SQL, Data Frames and RDD's.
â¢   Collaborated with infrastructure, network, database, application and BI to ensure data, quality and availability.
â¢   Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop.
â¢   Used ORC & Parquet file formats for serialization of data, and Snappy for the compression of the data.

Achievements

â¢   Appreciation for showing articulate leadership qualities in doing work with the team.
â¢   Completed the internal certification of TCS Certified Hadoop Developer.

Ongoing Learning
â¢   Preparing and scheduled the Cloudera Certified Spark Developer CCA 175."
Hadoop,"Areas of expertise â¢ Big Data Ecosystems: Hadoop-HDFS, MapReduce, Hive, Pig, Sqoop, HBase Oozie, Spark, Pyspark, HUE and having knowledge on cassandra â¢ Programming Languages: Python, Core Java and have an idea on Scala â¢ Databases: Oracle 10g, MySQL, Sqlserver NoSQL - HBase, Cassandra â¢ Tools: Eclipse, Toad, FTP, Tectia, Putty, Autosys, Anaconda, Jupyter notebool and Devops - RTC, RLM. â¢ Scripting Languages: JSP â¢ Platforms: Windows, UnixEducation Details 
 M.Tech (IT-DBS) B.Tech (CSE)  SRM University
Software Engineer 

Software Engineer - Larsen and Toubro
Skill Details 
Company Details 
company - Larsen and Toubro
description - Worked as a Software Engineer in Technosoft Corporation, Chennai from Aug 2015 to sep 2016.
company - Current Project
description - Duration: September 2016 to Till date
Vendor: Citi bank
Description:
Citibank's (Citi) Anti-Money Laundering (AML) Transaction Monitoring (TM) program is a future state solution and a rules-based system for transaction monitoring of ICG-Markets business.
Roles and Responesbilities:
â¢ Building and providing domain knowledge for Anti Money Laundering among team members.
â¢ The layered architecture has Data Warehouse and Workspace layers which are used by Business Analysts.
â¢ Actively involved in designing of star-schema model involving various Dimensions and Fact tables.
â¢ Designed SCD2 for maintaining history of the DIM data.
â¢ Developing Hive Queries for mapping data between different layers of architecture, and it's usage in Oozie Workflows.
â¢ Integration with Data Quality and Reconciliation Module.
â¢ Regression and Integration testing of solution for any issues in integration with other modules and effectively testing the data flow from layer-to-layer.
â¢ Transaction monitoring system development to generate Alerts for the suspicious and fraudulent transactions based on requirements provide by BAs.
â¢ Developing spark Jobs for various business rules.
â¢ Learning ""Machine Learning"", which will be used further in the project for developing an effective model for Fraud detection for Anti Money Laundering system.
â¢ Scheduling Jobs using Autosys tool.
â¢ Deployment and Code Management using RTC and RLM(Release Lifecycle Management)

Hadoop Developer
#  Current Project: PRTS - RAN
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Sqoop, HBase, Tez, Tableau, Sqlserver, Teradata
Cluster Size: 96 Node Cluster.
Distribution: Horton works - HDP2.3
company - Alcatel lucent
description - 1X) and  Ruckus Wireless
Description:
The scope of this project is to maintain and store the operational and parameters data collected from the multiple vendors networks by the mediation team into the OMS data store and make it available for RF engineers to boost the network performance.
Responsibilities:
â¢ Working with Hadoop Distributed File System.
â¢ Involved in importing data from MySQL to HDFS using SQOOP.
â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run  on top of  Tez execution Engine.
â¢ Involved in Preparing Test cases Document.
â¢ Involved in Integrating Hive and HBase to store the operational data.
â¢ Monitoring the Jobs through Oozie.
company - Current Project
description - Anti - Money laundering
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Oozie, Spark, Unix, Autosys, Python, RTC, RLM, ETL Framwe work
Cluster Size: 56 Node Cluster.
Distribution: Cloudera 5.9.14"
Hadoop,"Technical Skill Set: Programming Languages Apache Hadoop, Python, shell scripting, SQL Technologies Hive, Pig, Sqoop, Flume, Oozie, Impala, hdfs Tools Dataiku, Unravel, Cloudera, Putty, HUE, Cloudera Manager, Eclipse, Resource Manager Initial Learning Program: Tata Consultancy Services: June 2015 to August 2015 Description: This is a learning program conducted by TCS for the newly joined employees, to accomplish them to learn the working standard of the organization. During this period employee are groomed with various technical as well as ethical aspects. Education Details 
 B.E. Electronics & Communication Indore, Madhya Pradesh Medi-caps Institute of Technology & Management
Hadoop developer 

hadoop,hive,sqoop,flume,pig,mapreduce,python,impala,spark,scala,sql,unix.
Skill Details 
APACHE HADOOP SQOOP- Exprience - 31 months
Hadoop- Exprience - 31 months
HADOOP- Exprience - 31 months
Hive- Exprience - 31 months
SQOOP- Exprience - 31 months
python- Exprience - Less than 1 year months
hdfs- Exprience - Less than 1 year months
unix- Exprience - Less than 1 year months
impala- Exprience - Less than 1 year months
pig- Exprience - Less than 1 year months
unravel- Exprience - Less than 1 year months
mapreduce- Exprience - Less than 1 year months
dataiku- Exprience - Less than 1 year monthsCompany Details 
company - Tata Consultancy Services
description - Project Description
Data warehouse division has multiple products for injecting, storing, analysing and presenting data. The Data Lake program is started to provide multi-talent, secure data hub to store application's data on Hadoop platform with strong data governance, lineage, auditing and monitoring capabilities. The object of the project is to provide necessary engineering support to analytics and application teams so that they can focus on the business logic development. In this project, the major task is to set up the Hadoop cluster and govern all the activities which are required for the smooth functioning of various Hadoop ecosystems. As the day and day data increasing so to provide stability to the ecosystem and smooth working of it, Developing and automating the various requirement specific utilities.

Responsibility 1. Developed proactive Health Check utility for Data Lake. The utility proactively checks the smooth functioning of all Hadoop components on the cluster and sends the result to email in HTML format. The utility is being used for daily Health Checks as well as after upgrades.
2. Getting the data in different formats and processing the data in Hadoop ecosystem after filtering the data using the appropriate techniques.
3. Developed data pipeline utility to ingest data from RDBMS database to Hive external tables using Sqoop commands. The utility also offers the data quality check like row count validation.
4. Developed and automated various cluster health check, usage, capacity related reports using Unix shell scripting.
5. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
6. Creating flume agents to process the data to Hadoop ecosystem side.
7. Performed benchmark testing on the Hive Queries and impala queries.
8. Involved in setting up the cluster and its components like edge node and HA implementation of the services: Hive Server2, Impala, and HDFS.
9. Filtering the required data from available data using different technologies like pig, regex Serde etc.
10. Dataiku benchmark testing on top of impala and hive in compare to Greenplum database.
11. Moving the data from Greenplum database to Hadoop side with help of Sqoop pipeline, process the data to Hadoop side and storing the data into hive tables to do the performance testing.
12. Dealing with the Hadoop ecosystem related issues in order to provide stability to WM Hadoop ecosystem.
13. Rescheduling of job from autosys job hosting to TWS job hosting for better performance.

Declaration:
I hereby declare that the above mentioned information is authentic to the best of my knowledge
company - Tata Consultancy Services
description - Clients: 1. Barclays 2. Union bank of California (UBC) 3. Morgan Stanley (MS)

KEY PROJECTS HANDLED
Project Name ABSA- Reconciliations, UBC and WMDATALAKE COE
company - Tata Consultancy Services
description - Project Description
Migration of data from RDBMS database to Hive (Hadoop ecosystem) . Hadoop platform ability with strong data governance, lineage, auditing and monitoring capabilities. The objective of this project was to speed up the data processing so that the analysis and decision making become easy. Due to RDBMS limitations to process waste amount of data at once and produce the results at the earliest, Client wanted to move the data to Hadoop ecosystem so that they can over-come from those limitations and focus on business improvement only.

Responsibility 1. Optimising the SQL queries for those data which were not required to move from RDBMS to any other platform.
2. Writing the Hive queries and logic to move the data from RDBMS to Hadoop ecosystem.
3. Writing the hive queries to analyse the required data as per the business requirements.
4. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
5. Writing the sqoop commands and scripts to move the data from RDBMS to Hadoop side.
company - Tata Consultancy Services
description - Project Description
Create recs and migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.

Responsibility 1. Have worked on extracting business requirements, analyzing and implementing them in developing Recs 2. Worked on migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.
3. Done the back end work where most of the things were related to writing the sql queries and provide the data for the new recs.

Project Name   PSO"
Hadoop,"Technical Skills Programming Languages: C, C++, Java, .Net., J2EE, HTML5, CSS, MapReduce Scripting Languages: Javascript, Python Databases: Oracle (PL-SQL), MY-SQL, IBM DB2 Tools:IBM Rational Rose, R, Weka Operating Systems: Windows XP, Vista, UNIX, Windows 7, Red Hat 7Education Details 
January 2015 B.E  Pimpri Chinchwad, MAHARASHTRA, IN Pimpri Chinchwad College of Engineering
January 2012 Diploma MSBTE  Dnyanganaga Polytechnic
 S.S.C   New English School Takali
Hadoop/Big Data Developer 

Hadoop/Big Data Developer - British Telecom
Skill Details 
APACHE HADOOP MAPREDUCE- Exprience - 37 months
MapReduce- Exprience - 37 months
MAPREDUCE- Exprience - 37 months
JAVA- Exprience - 32 months
.NET- Exprience - 6 monthsCompany Details 
company - British Telecom
description - Project: British Telecom project (UK)
Responsibilities:
â¢ Working on HDFS, MapReduce, Hive, Spark, Scala, Sqoop, Kerberos etc. technologies
â¢ Implemented various data mining algorithms on Spark like K-means clustering, Random forest, NaÃ¯ve bayes etc.
â¢ A knowledge of installing, configuring, maintaining and securing Hadoop.
company - DXC technology
description - HPE legacy), Bangalore
â¢ Worked on Hadoop + Java programming
â¢ Worked on Azure and AWS (EMR) services.
â¢ Worked on HDInsight Hadoop cluster..
â¢ Design, develop, document and architect Hadoop applications
â¢ Develop MapReduce coding that works seamlessly on Hadoop clusters.
â¢ Analyzing and processing the large data sets on HDFS.
â¢ An analytical bent of mind and ability to learn-unlearn-relearn surely comes in handy."
Hadoop,"Technical Skill Set Big Data Ecosystems: Hadoop, HDFS, HBase, Map Reduce, Sqoop, Hive, Pig, Spark-Core, Flume. Other Language: Scala, Core-Java, SQL, PLSQL, Sell Scripting ETL Tools: Informatica Power Center8.x/9.6, Talend 5.6 Tools: Eclipse, Intellij Idea. Platforms: Windows Family, Linux /UNIX, Cloudera. Databases: MySQL, Oracle.10/11gEducation Details 
 M.C.A  Pune, MAHARASHTRA, IN Pune University
Hodoop Developer 

Hodoop Developer - PRGX India Private Limited Pune
Skill Details 
Company Details 
company - PRGX India Private Limited Pune
description - Team Size: 10+
Environment: Hive, Spark, Sqoop, Scala and Flume.

Project Description:
The bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. The customers spending ranges from online shopping, medical expenses in hospitals, cash transactions, and debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. The portal allows the customers to login and see their transactions which they make on a day to day basis .the analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. The portal used hadoop framework to analyes the data as per the rules and regulations placed by the regulators from the respective countries. The offers and the interest rates also complied with the regulations and all these processing was done using the hadoop framework as big data analytics system.

Role & Responsibilities:
â Import data from legacy system to hadoop using Sqoop, flume.
â Implement the business logic to analyses  the data
â Per-process data using spark.
â Create hive script and loading data into hive.
â Sourcing various attributes to the data processing logic to retrieve the correct results.

Project 2
company - PRGX India Private Limited Pune
description - 
company - PRGX India Private Limited Pune
description - Team Size: 11+
Environment: Hadoop, HDFS, Hive, Sqoop, MySQL, Map Reduce

Project Description:-
The Purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it.the solution was based on the open source s/w hadoop. The data will be stored in hadoop file system and processed using Map/Reduce jobs. Which in trun includes getting the raw html data from the micro websites, process the html to obtain product and user information, extract various reports out of the vistor tracking information and export the information for further processing

Role & Responsibilities:
â Move all crawl data flat files generated from various micro sites to HDFS for further processing.
â Sqoop implementation for interaction with database
â Write Map Reduce scripts to process the data file.
â Create hive tables to store the processed data in tabular formats.
â Reports creation from hive data.

Project 3
company - PRGX India Private Limited Pune
description - Team Size: 15+
Environment: Informatica 9.5, Oracle11g, UNIX

Project Description:
Pfizer Inc. is an American global pharmaceutical corporation headquartered in New York City. The main objective of the project is to build a Development Data Repository for Pfizer Inc. Because all the downstream application are like Etrack, TSP database, RTS, SADMS, GFS, GDO having their own sql request on the OLTP system directly due to which the performance of OLTP system goes slows down. For this we have created a Development Data Repository to replace the entire sql request directly on the OLTP system. DDR process extracts all clinical, pre-clinical, study, product, subject, sites related information from the upstream applications like EPECS, CDSS, RCM, PRC, E-CLINICAL, EDH and after applying some business logic put it into DDR core tables. From these snapshot and dimensional layer are created which are used for reporting application.

Role & Responsibilities:
â To understand & analyze the requirement documents and resolve the queries.
â To design Informatica mappings by using various basic transformations like Filter, Router, Source qualifier, Lookup etc and advance transformations like Aggregators, Joiner, Sorters and so on.
â Perform cross Unit and Integration testing for mappings developed within the team. Reporting bugs and bug fixing.
â Create workflow/batches and set the session dependencies.
â Implemented Change Data Capture using mapping parameters, SCD and SK generation.
â Developed Mapplet, reusable transformations to populate the data into data warehouse.
â Created Sessions & Worklets using workflow Manager to load the data into the Target Database.
â Involved in Unit Case Testing (UTC)
â Performing Unit Testing and UAT for SCD Type1/Type2, fact load and CDC implementation.

Personal Scan

Address: Jijayi Heights, Flat no 118, Narhe, (Police chowki) Pune- 411041"
Hadoop,"Education Details 

Hadoop Developer 

Hadoop Developer - INFOSYS
Skill Details 
Company Details 
company - INFOSYS
description - Project Description: The banking information had stored the data in different data ware house systems for each department but it becomes difficult for the organization to manage the data and to perform some analytics on the past data, so it is combined them into a single global repository in Hadoop for analysis.

Responsibilities:
â¢       Analyze the banking rates data set.
â¢       Create specification document.
â¢       Provide effort estimation.
â¢       Develop SPARK Scala, SPARK SQL Programs using Eclipse IDE on Windows/Linux environment.
â¢       Create KPI's test scenarios, test cases, test result document.
â¢       Test the Scala programs in Linux Spark Standalone mode.
â¢       setup multi cluster on AWS, deploy the Spark Scala programs
â¢       Provided solution using Hadoop ecosystem - HDFS, MapReduce, Pig, Hive, HBase, and Zookeeper.
â¢       Provided solution using large scale server-side systems with distributed processing algorithms.
â¢       Created reports for the BI team using Sqoop to export data into HDFS and Hive.
â¢       Provided solution in supporting and assisting in troubleshooting and optimization of MapReduce jobs and
Pig Latin scripts.
â¢       Deep understanding of Hadoop design principles, cluster connectivity, security and the factors that affect
system performance.
â¢       Worked on Importing and exporting data from different databases like Oracle, Teradata into HDFS and Hive
using Sqoop, TPT and Connect Direct.
â¢       Import and export the data from RDBMS to HDFS/HBASE
â¢       Wrote script and placed it in client side so that the data moved to HDFS will be stored in temporary file and then it will start loading it in hive tables.
â¢       Developed the Sqoop scripts in order to make the interaction between Pig and MySQL Database.
â¢       Involved in developing the Hive Reports, Partitions of Hive tables.
â¢       Created and maintained technical documentation for launching HADOOP Clusters and for executing HIVE
queries and PIG scripts.
â¢       Involved in running Hadoop jobs for processing millions of records of text data

Environment: Java, Hadoop, HDFS, Map-Reduce, Pig, Hive, Sqoop, Flume, Oozie, HBase, Spark, Scala,
Linux, NoSQL, Storm, Tomcat, Putty, SVN, GitHub, IBM WebSphere v8.5.

Project #1: TELECOMMUNICATIONS
Hadoop Developer

Description To identify customers who are likely to churn and 360-degree view of the customer is created from different heterogeneous data sources. The data is brought into data lake (HDFS) from different sources and analyzed using different Hadoop tools like pig and hive.

Responsibilities:
â¢       Installed and Configured Apache Hadoop tools like Hive, Pig, HBase and Sqoop for application development and unit testing.
â¢       Wrote MapReduce jobs to discover trends in data usage by users.
â¢       Involved in database connection using SQOOP.
â¢       Involved in creating Hive tables, loading data and writing hive queries Using the HiveQL.
â¢       Involved in partitioning and joining Hive tables for Hive query optimization.
â¢       Experienced in SQL DB Migration to HDFS.
â¢       Used NoSQL(HBase) for faster performance, which maintains the data in the De-Normalized way for OLTP.
â¢       The data is collected from distributed sources into Avro models. Applied transformations and standardizations and loaded into HBase for further data processing.
â¢       Experienced in defining job flows.
â¢       Used Oozie to orchestrate the workflow.
â¢       Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce
jobs given by the users.
â¢       Exported the analyzed data to the relational databases using HIVE for visualization and to generate reports for the BI team.

Environment: Hadoop, Hive, Linux, MapReduce, HDFS, Hive, Python, Pig, Sqoop, Cloudera, Shell Scripting,
Java (JDK 1.6), Java 6, Oracle 10g, PL/SQL, SQL*PLUS"
Hadoop,"Skill Set: Hadoop, Map Reduce, HDFS, Hive, Sqoop, java. Duration: 2016 to 2017. Role: Hadoop Developer Rplus offers an quick, simple and powerful cloud based Solution, Demand Sense to accurately predict demand for your product in all your markets which Combines Enterprise and External Data to predict demand more accurately through Uses Social Conversation and Sentiments to derive demand and Identifies significant drivers of sale out of hordes of factors that Selects the best suited model out of multiple forecasting models for each product. Responsibilities: â¢ Involved in deploying the product for customers, gathering requirements and algorithm optimization at backend of the product. â¢ Load and transform Large Datasets of structured semi structured. â¢ Responsible to manage data coming from different sources and application â¢ Supported Map Reduce Programs those are running on the cluster â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.Education Details 

Hadoop Developer 

Hadoop Developer - Braindatawire
Skill Details 
APACHE HADOOP HDFS- Exprience - 49 months
APACHE HADOOP SQOOP- Exprience - 49 months
Hadoop- Exprience - 49 months
HADOOP- Exprience - 49 months
HADOOP DISTRIBUTED FILE SYSTEM- Exprience - 49 monthsCompany Details 
company - Braindatawire
description - Technical Skills:
â¢   Programming: Core Java, Map Reduce, Scala
â¢   Hadoop Tools: HDFS, Spark, Map Reduce, Sqoop, Hive, Hbase
â¢   Database: MySQL, Oracle
â¢   Scripting: Shell Scripting
â¢   IDE: Eclipse
â¢   Operating Systems: Linux (CentOS), Windows
â¢   Source Control: Git (Github)"
Hadoop,"â¢ Operating systems:-Linux- Ubuntu, Windows 2007/08 â¢ Other tools:- Tableau, SVN, Beyond Compare.Education Details 
January 2016 Bachelors of Engineering Engineering  Gujarat Technological University
Systems Engineer/Hadoop Developer 

Systems Engineer/Hadoop Developer - Tata Consultancy Services
Skill Details 
Hadoop,Spark,Sqoop,Hive,Flume,Pig- Exprience - 24 monthsCompany Details 
company - Tata Consultancy Services
description - Roles and responsibility:

Working for a American pharmaceutical company (one of the world's premier
biopharmaceutical) who develops and produces medicines and vaccines for a wide range of medical
disciplines, including immunology, oncology, cardiology, endocrinology, and neurology. To handle large
amount of United Healthcare data big data analytics is used. Data from all possible data sources like records of all Patients(Old and New), records of medicines, Treatment Pathways & Patient Journey for
Health Outcomes, Patient Finder (or Rare Disease Patient Finder), etc being gathered, stored and processed at one place.

â¢     Worked on cluster with specs as:
o    Cluster Architecture: Fully
Distributed Package Used:
CDH3
o    Cluster Capacity: 20 TB
o    No. of Nodes: 10 Data Nodes + 3 Masters + NFS Backup For NN

â¢     Developed proof of concepts for enterprise adoption of Hadoop.
â¢   Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera
distribution.
â¢   Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and trouble-shooting, manage and review data backups, and reviewing Hadoop log files.
â¢   Imported & exported large data sets of data into HDFS and vice-versa using sqoop.
â¢   Involved developing the Pig scripts and Hive Reports
â¢   Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive
partition.Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive.
â¢   Developed Spark scripts by using Scala shell commands as per the requirement and worked with both
Data frames/SQL/Data sets and RDD/MapReduce in Spark. Optimizing of existing algorithms in Hadoop
using SparkContext, Spark-SQL, Data Frames and RDD's.
â¢   Collaborated with infrastructure, network, database, application and BI to ensure data, quality and availability.
â¢   Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop.
â¢   Used ORC & Parquet file formats for serialization of data, and Snappy for the compression of the data.

Achievements

â¢   Appreciation for showing articulate leadership qualities in doing work with the team.
â¢   Completed the internal certification of TCS Certified Hadoop Developer.

Ongoing Learning
â¢   Preparing and scheduled the Cloudera Certified Spark Developer CCA 175."
Hadoop,"Areas of expertise â¢ Big Data Ecosystems: Hadoop-HDFS, MapReduce, Hive, Pig, Sqoop, HBase Oozie, Spark, Pyspark, HUE and having knowledge on cassandra â¢ Programming Languages: Python, Core Java and have an idea on Scala â¢ Databases: Oracle 10g, MySQL, Sqlserver NoSQL - HBase, Cassandra â¢ Tools: Eclipse, Toad, FTP, Tectia, Putty, Autosys, Anaconda, Jupyter notebool and Devops - RTC, RLM. â¢ Scripting Languages: JSP â¢ Platforms: Windows, UnixEducation Details 
 M.Tech (IT-DBS) B.Tech (CSE)  SRM University
Software Engineer 

Software Engineer - Larsen and Toubro
Skill Details 
Company Details 
company - Larsen and Toubro
description - Worked as a Software Engineer in Technosoft Corporation, Chennai from Aug 2015 to sep 2016.
company - Current Project
description - Duration: September 2016 to Till date
Vendor: Citi bank
Description:
Citibank's (Citi) Anti-Money Laundering (AML) Transaction Monitoring (TM) program is a future state solution and a rules-based system for transaction monitoring of ICG-Markets business.
Roles and Responesbilities:
â¢ Building and providing domain knowledge for Anti Money Laundering among team members.
â¢ The layered architecture has Data Warehouse and Workspace layers which are used by Business Analysts.
â¢ Actively involved in designing of star-schema model involving various Dimensions and Fact tables.
â¢ Designed SCD2 for maintaining history of the DIM data.
â¢ Developing Hive Queries for mapping data between different layers of architecture, and it's usage in Oozie Workflows.
â¢ Integration with Data Quality and Reconciliation Module.
â¢ Regression and Integration testing of solution for any issues in integration with other modules and effectively testing the data flow from layer-to-layer.
â¢ Transaction monitoring system development to generate Alerts for the suspicious and fraudulent transactions based on requirements provide by BAs.
â¢ Developing spark Jobs for various business rules.
â¢ Learning ""Machine Learning"", which will be used further in the project for developing an effective model for Fraud detection for Anti Money Laundering system.
â¢ Scheduling Jobs using Autosys tool.
â¢ Deployment and Code Management using RTC and RLM(Release Lifecycle Management)

Hadoop Developer
#  Current Project: PRTS - RAN
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Sqoop, HBase, Tez, Tableau, Sqlserver, Teradata
Cluster Size: 96 Node Cluster.
Distribution: Horton works - HDP2.3
company - Alcatel lucent
description - 1X) and  Ruckus Wireless
Description:
The scope of this project is to maintain and store the operational and parameters data collected from the multiple vendors networks by the mediation team into the OMS data store and make it available for RF engineers to boost the network performance.
Responsibilities:
â¢ Working with Hadoop Distributed File System.
â¢ Involved in importing data from MySQL to HDFS using SQOOP.
â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run  on top of  Tez execution Engine.
â¢ Involved in Preparing Test cases Document.
â¢ Involved in Integrating Hive and HBase to store the operational data.
â¢ Monitoring the Jobs through Oozie.
company - Current Project
description - Anti - Money laundering
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Oozie, Spark, Unix, Autosys, Python, RTC, RLM, ETL Framwe work
Cluster Size: 56 Node Cluster.
Distribution: Cloudera 5.9.14"
Hadoop,"Technical Skill Set: Programming Languages Apache Hadoop, Python, shell scripting, SQL Technologies Hive, Pig, Sqoop, Flume, Oozie, Impala, hdfs Tools Dataiku, Unravel, Cloudera, Putty, HUE, Cloudera Manager, Eclipse, Resource Manager Initial Learning Program: Tata Consultancy Services: June 2015 to August 2015 Description: This is a learning program conducted by TCS for the newly joined employees, to accomplish them to learn the working standard of the organization. During this period employee are groomed with various technical as well as ethical aspects. Education Details 
 B.E. Electronics & Communication Indore, Madhya Pradesh Medi-caps Institute of Technology & Management
Hadoop developer 

hadoop,hive,sqoop,flume,pig,mapreduce,python,impala,spark,scala,sql,unix.
Skill Details 
APACHE HADOOP SQOOP- Exprience - 31 months
Hadoop- Exprience - 31 months
HADOOP- Exprience - 31 months
Hive- Exprience - 31 months
SQOOP- Exprience - 31 months
python- Exprience - Less than 1 year months
hdfs- Exprience - Less than 1 year months
unix- Exprience - Less than 1 year months
impala- Exprience - Less than 1 year months
pig- Exprience - Less than 1 year months
unravel- Exprience - Less than 1 year months
mapreduce- Exprience - Less than 1 year months
dataiku- Exprience - Less than 1 year monthsCompany Details 
company - Tata Consultancy Services
description - Project Description
Data warehouse division has multiple products for injecting, storing, analysing and presenting data. The Data Lake program is started to provide multi-talent, secure data hub to store application's data on Hadoop platform with strong data governance, lineage, auditing and monitoring capabilities. The object of the project is to provide necessary engineering support to analytics and application teams so that they can focus on the business logic development. In this project, the major task is to set up the Hadoop cluster and govern all the activities which are required for the smooth functioning of various Hadoop ecosystems. As the day and day data increasing so to provide stability to the ecosystem and smooth working of it, Developing and automating the various requirement specific utilities.

Responsibility 1. Developed proactive Health Check utility for Data Lake. The utility proactively checks the smooth functioning of all Hadoop components on the cluster and sends the result to email in HTML format. The utility is being used for daily Health Checks as well as after upgrades.
2. Getting the data in different formats and processing the data in Hadoop ecosystem after filtering the data using the appropriate techniques.
3. Developed data pipeline utility to ingest data from RDBMS database to Hive external tables using Sqoop commands. The utility also offers the data quality check like row count validation.
4. Developed and automated various cluster health check, usage, capacity related reports using Unix shell scripting.
5. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
6. Creating flume agents to process the data to Hadoop ecosystem side.
7. Performed benchmark testing on the Hive Queries and impala queries.
8. Involved in setting up the cluster and its components like edge node and HA implementation of the services: Hive Server2, Impala, and HDFS.
9. Filtering the required data from available data using different technologies like pig, regex Serde etc.
10. Dataiku benchmark testing on top of impala and hive in compare to Greenplum database.
11. Moving the data from Greenplum database to Hadoop side with help of Sqoop pipeline, process the data to Hadoop side and storing the data into hive tables to do the performance testing.
12. Dealing with the Hadoop ecosystem related issues in order to provide stability to WM Hadoop ecosystem.
13. Rescheduling of job from autosys job hosting to TWS job hosting for better performance.

Declaration:
I hereby declare that the above mentioned information is authentic to the best of my knowledge
company - Tata Consultancy Services
description - Clients: 1. Barclays 2. Union bank of California (UBC) 3. Morgan Stanley (MS)

KEY PROJECTS HANDLED
Project Name ABSA- Reconciliations, UBC and WMDATALAKE COE
company - Tata Consultancy Services
description - Project Description
Migration of data from RDBMS database to Hive (Hadoop ecosystem) . Hadoop platform ability with strong data governance, lineage, auditing and monitoring capabilities. The objective of this project was to speed up the data processing so that the analysis and decision making become easy. Due to RDBMS limitations to process waste amount of data at once and produce the results at the earliest, Client wanted to move the data to Hadoop ecosystem so that they can over-come from those limitations and focus on business improvement only.

Responsibility 1. Optimising the SQL queries for those data which were not required to move from RDBMS to any other platform.
2. Writing the Hive queries and logic to move the data from RDBMS to Hadoop ecosystem.
3. Writing the hive queries to analyse the required data as per the business requirements.
4. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
5. Writing the sqoop commands and scripts to move the data from RDBMS to Hadoop side.
company - Tata Consultancy Services
description - Project Description
Create recs and migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.

Responsibility 1. Have worked on extracting business requirements, analyzing and implementing them in developing Recs 2. Worked on migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.
3. Done the back end work where most of the things were related to writing the sql queries and provide the data for the new recs.

Project Name   PSO"
Hadoop,"Technical Skills Programming Languages: C, C++, Java, .Net., J2EE, HTML5, CSS, MapReduce Scripting Languages: Javascript, Python Databases: Oracle (PL-SQL), MY-SQL, IBM DB2 Tools:IBM Rational Rose, R, Weka Operating Systems: Windows XP, Vista, UNIX, Windows 7, Red Hat 7Education Details 
January 2015 B.E  Pimpri Chinchwad, MAHARASHTRA, IN Pimpri Chinchwad College of Engineering
January 2012 Diploma MSBTE  Dnyanganaga Polytechnic
 S.S.C   New English School Takali
Hadoop/Big Data Developer 

Hadoop/Big Data Developer - British Telecom
Skill Details 
APACHE HADOOP MAPREDUCE- Exprience - 37 months
MapReduce- Exprience - 37 months
MAPREDUCE- Exprience - 37 months
JAVA- Exprience - 32 months
.NET- Exprience - 6 monthsCompany Details 
company - British Telecom
description - Project: British Telecom project (UK)
Responsibilities:
â¢ Working on HDFS, MapReduce, Hive, Spark, Scala, Sqoop, Kerberos etc. technologies
â¢ Implemented various data mining algorithms on Spark like K-means clustering, Random forest, NaÃ¯ve bayes etc.
â¢ A knowledge of installing, configuring, maintaining and securing Hadoop.
company - DXC technology
description - HPE legacy), Bangalore
â¢ Worked on Hadoop + Java programming
â¢ Worked on Azure and AWS (EMR) services.
â¢ Worked on HDInsight Hadoop cluster..
â¢ Design, develop, document and architect Hadoop applications
â¢ Develop MapReduce coding that works seamlessly on Hadoop clusters.
â¢ Analyzing and processing the large data sets on HDFS.
â¢ An analytical bent of mind and ability to learn-unlearn-relearn surely comes in handy."
Hadoop,"Technical Skill Set Big Data Ecosystems: Hadoop, HDFS, HBase, Map Reduce, Sqoop, Hive, Pig, Spark-Core, Flume. Other Language: Scala, Core-Java, SQL, PLSQL, Sell Scripting ETL Tools: Informatica Power Center8.x/9.6, Talend 5.6 Tools: Eclipse, Intellij Idea. Platforms: Windows Family, Linux /UNIX, Cloudera. Databases: MySQL, Oracle.10/11gEducation Details 
 M.C.A  Pune, MAHARASHTRA, IN Pune University
Hodoop Developer 

Hodoop Developer - PRGX India Private Limited Pune
Skill Details 
Company Details 
company - PRGX India Private Limited Pune
description - Team Size: 10+
Environment: Hive, Spark, Sqoop, Scala and Flume.

Project Description:
The bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. The customers spending ranges from online shopping, medical expenses in hospitals, cash transactions, and debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. The portal allows the customers to login and see their transactions which they make on a day to day basis .the analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. The portal used hadoop framework to analyes the data as per the rules and regulations placed by the regulators from the respective countries. The offers and the interest rates also complied with the regulations and all these processing was done using the hadoop framework as big data analytics system.

Role & Responsibilities:
â Import data from legacy system to hadoop using Sqoop, flume.
â Implement the business logic to analyses  the data
â Per-process data using spark.
â Create hive script and loading data into hive.
â Sourcing various attributes to the data processing logic to retrieve the correct results.

Project 2
company - PRGX India Private Limited Pune
description - 
company - PRGX India Private Limited Pune
description - Team Size: 11+
Environment: Hadoop, HDFS, Hive, Sqoop, MySQL, Map Reduce

Project Description:-
The Purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it.the solution was based on the open source s/w hadoop. The data will be stored in hadoop file system and processed using Map/Reduce jobs. Which in trun includes getting the raw html data from the micro websites, process the html to obtain product and user information, extract various reports out of the vistor tracking information and export the information for further processing

Role & Responsibilities:
â Move all crawl data flat files generated from various micro sites to HDFS for further processing.
â Sqoop implementation for interaction with database
â Write Map Reduce scripts to process the data file.
â Create hive tables to store the processed data in tabular formats.
â Reports creation from hive data.

Project 3
company - PRGX India Private Limited Pune
description - Team Size: 15+
Environment: Informatica 9.5, Oracle11g, UNIX

Project Description:
Pfizer Inc. is an American global pharmaceutical corporation headquartered in New York City. The main objective of the project is to build a Development Data Repository for Pfizer Inc. Because all the downstream application are like Etrack, TSP database, RTS, SADMS, GFS, GDO having their own sql request on the OLTP system directly due to which the performance of OLTP system goes slows down. For this we have created a Development Data Repository to replace the entire sql request directly on the OLTP system. DDR process extracts all clinical, pre-clinical, study, product, subject, sites related information from the upstream applications like EPECS, CDSS, RCM, PRC, E-CLINICAL, EDH and after applying some business logic put it into DDR core tables. From these snapshot and dimensional layer are created which are used for reporting application.

Role & Responsibilities:
â To understand & analyze the requirement documents and resolve the queries.
â To design Informatica mappings by using various basic transformations like Filter, Router, Source qualifier, Lookup etc and advance transformations like Aggregators, Joiner, Sorters and so on.
â Perform cross Unit and Integration testing for mappings developed within the team. Reporting bugs and bug fixing.
â Create workflow/batches and set the session dependencies.
â Implemented Change Data Capture using mapping parameters, SCD and SK generation.
â Developed Mapplet, reusable transformations to populate the data into data warehouse.
â Created Sessions & Worklets using workflow Manager to load the data into the Target Database.
â Involved in Unit Case Testing (UTC)
â Performing Unit Testing and UAT for SCD Type1/Type2, fact load and CDC implementation.

Personal Scan

Address: Jijayi Heights, Flat no 118, Narhe, (Police chowki) Pune- 411041"
Hadoop,"Education Details 

Hadoop Developer 

Hadoop Developer - INFOSYS
Skill Details 
Company Details 
company - INFOSYS
description - Project Description: The banking information had stored the data in different data ware house systems for each department but it becomes difficult for the organization to manage the data and to perform some analytics on the past data, so it is combined them into a single global repository in Hadoop for analysis.

Responsibilities:
â¢       Analyze the banking rates data set.
â¢       Create specification document.
â¢       Provide effort estimation.
â¢       Develop SPARK Scala, SPARK SQL Programs using Eclipse IDE on Windows/Linux environment.
â¢       Create KPI's test scenarios, test cases, test result document.
â¢       Test the Scala programs in Linux Spark Standalone mode.
â¢       setup multi cluster on AWS, deploy the Spark Scala programs
â¢       Provided solution using Hadoop ecosystem - HDFS, MapReduce, Pig, Hive, HBase, and Zookeeper.
â¢       Provided solution using large scale server-side systems with distributed processing algorithms.
â¢       Created reports for the BI team using Sqoop to export data into HDFS and Hive.
â¢       Provided solution in supporting and assisting in troubleshooting and optimization of MapReduce jobs and
Pig Latin scripts.
â¢       Deep understanding of Hadoop design principles, cluster connectivity, security and the factors that affect
system performance.
â¢       Worked on Importing and exporting data from different databases like Oracle, Teradata into HDFS and Hive
using Sqoop, TPT and Connect Direct.
â¢       Import and export the data from RDBMS to HDFS/HBASE
â¢       Wrote script and placed it in client side so that the data moved to HDFS will be stored in temporary file and then it will start loading it in hive tables.
â¢       Developed the Sqoop scripts in order to make the interaction between Pig and MySQL Database.
â¢       Involved in developing the Hive Reports, Partitions of Hive tables.
â¢       Created and maintained technical documentation for launching HADOOP Clusters and for executing HIVE
queries and PIG scripts.
â¢       Involved in running Hadoop jobs for processing millions of records of text data

Environment: Java, Hadoop, HDFS, Map-Reduce, Pig, Hive, Sqoop, Flume, Oozie, HBase, Spark, Scala,
Linux, NoSQL, Storm, Tomcat, Putty, SVN, GitHub, IBM WebSphere v8.5.

Project #1: TELECOMMUNICATIONS
Hadoop Developer

Description To identify customers who are likely to churn and 360-degree view of the customer is created from different heterogeneous data sources. The data is brought into data lake (HDFS) from different sources and analyzed using different Hadoop tools like pig and hive.

Responsibilities:
â¢       Installed and Configured Apache Hadoop tools like Hive, Pig, HBase and Sqoop for application development and unit testing.
â¢       Wrote MapReduce jobs to discover trends in data usage by users.
â¢       Involved in database connection using SQOOP.
â¢       Involved in creating Hive tables, loading data and writing hive queries Using the HiveQL.
â¢       Involved in partitioning and joining Hive tables for Hive query optimization.
â¢       Experienced in SQL DB Migration to HDFS.
â¢       Used NoSQL(HBase) for faster performance, which maintains the data in the De-Normalized way for OLTP.
â¢       The data is collected from distributed sources into Avro models. Applied transformations and standardizations and loaded into HBase for further data processing.
â¢       Experienced in defining job flows.
â¢       Used Oozie to orchestrate the workflow.
â¢       Implemented Fair schedulers on the Job tracker to share the resources of the Cluster for the Map Reduce
jobs given by the users.
â¢       Exported the analyzed data to the relational databases using HIVE for visualization and to generate reports for the BI team.

Environment: Hadoop, Hive, Linux, MapReduce, HDFS, Hive, Python, Pig, Sqoop, Cloudera, Shell Scripting,
Java (JDK 1.6), Java 6, Oracle 10g, PL/SQL, SQL*PLUS"
Hadoop,"Skill Set: Hadoop, Map Reduce, HDFS, Hive, Sqoop, java. Duration: 2016 to 2017. Role: Hadoop Developer Rplus offers an quick, simple and powerful cloud based Solution, Demand Sense to accurately predict demand for your product in all your markets which Combines Enterprise and External Data to predict demand more accurately through Uses Social Conversation and Sentiments to derive demand and Identifies significant drivers of sale out of hordes of factors that Selects the best suited model out of multiple forecasting models for each product. Responsibilities: â¢ Involved in deploying the product for customers, gathering requirements and algorithm optimization at backend of the product. â¢ Load and transform Large Datasets of structured semi structured. â¢ Responsible to manage data coming from different sources and application â¢ Supported Map Reduce Programs those are running on the cluster â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run internally in map reduce way.Education Details 

Hadoop Developer 

Hadoop Developer - Braindatawire
Skill Details 
APACHE HADOOP HDFS- Exprience - 49 months
APACHE HADOOP SQOOP- Exprience - 49 months
Hadoop- Exprience - 49 months
HADOOP- Exprience - 49 months
HADOOP DISTRIBUTED FILE SYSTEM- Exprience - 49 monthsCompany Details 
company - Braindatawire
description - Technical Skills:
â¢   Programming: Core Java, Map Reduce, Scala
â¢   Hadoop Tools: HDFS, Spark, Map Reduce, Sqoop, Hive, Hbase
â¢   Database: MySQL, Oracle
â¢   Scripting: Shell Scripting
â¢   IDE: Eclipse
â¢   Operating Systems: Linux (CentOS), Windows
â¢   Source Control: Git (Github)"
Hadoop,"â¢ Operating systems:-Linux- Ubuntu, Windows 2007/08 â¢ Other tools:- Tableau, SVN, Beyond Compare.Education Details 
January 2016 Bachelors of Engineering Engineering  Gujarat Technological University
Systems Engineer/Hadoop Developer 

Systems Engineer/Hadoop Developer - Tata Consultancy Services
Skill Details 
Hadoop,Spark,Sqoop,Hive,Flume,Pig- Exprience - 24 monthsCompany Details 
company - Tata Consultancy Services
description - Roles and responsibility:

Working for a American pharmaceutical company (one of the world's premier
biopharmaceutical) who develops and produces medicines and vaccines for a wide range of medical
disciplines, including immunology, oncology, cardiology, endocrinology, and neurology. To handle large
amount of United Healthcare data big data analytics is used. Data from all possible data sources like records of all Patients(Old and New), records of medicines, Treatment Pathways & Patient Journey for
Health Outcomes, Patient Finder (or Rare Disease Patient Finder), etc being gathered, stored and processed at one place.

â¢     Worked on cluster with specs as:
o    Cluster Architecture: Fully
Distributed Package Used:
CDH3
o    Cluster Capacity: 20 TB
o    No. of Nodes: 10 Data Nodes + 3 Masters + NFS Backup For NN

â¢     Developed proof of concepts for enterprise adoption of Hadoop.
â¢   Used SparkAPI over Cloudera Hadoop YARN to perform analytics on the Healthcare data in Cloudera
distribution.
â¢   Responsible for cluster maintenance, adding and removing cluster nodes, cluster monitoring and trouble-shooting, manage and review data backups, and reviewing Hadoop log files.
â¢   Imported & exported large data sets of data into HDFS and vice-versa using sqoop.
â¢   Involved developing the Pig scripts and Hive Reports
â¢   Worked on Hive partition and bucketing concepts and created hive external and Internal tables with Hive
partition.Monitoring Hadoop scripts which take the input from HDFS and load the data into Hive.
â¢   Developed Spark scripts by using Scala shell commands as per the requirement and worked with both
Data frames/SQL/Data sets and RDD/MapReduce in Spark. Optimizing of existing algorithms in Hadoop
using SparkContext, Spark-SQL, Data Frames and RDD's.
â¢   Collaborated with infrastructure, network, database, application and BI to ensure data, quality and availability.
â¢   Developed reports using TABLEAU and exported data to HDFS and hive using Sqoop.
â¢   Used ORC & Parquet file formats for serialization of data, and Snappy for the compression of the data.

Achievements

â¢   Appreciation for showing articulate leadership qualities in doing work with the team.
â¢   Completed the internal certification of TCS Certified Hadoop Developer.

Ongoing Learning
â¢   Preparing and scheduled the Cloudera Certified Spark Developer CCA 175."
Hadoop,"Areas of expertise â¢ Big Data Ecosystems: Hadoop-HDFS, MapReduce, Hive, Pig, Sqoop, HBase Oozie, Spark, Pyspark, HUE and having knowledge on cassandra â¢ Programming Languages: Python, Core Java and have an idea on Scala â¢ Databases: Oracle 10g, MySQL, Sqlserver NoSQL - HBase, Cassandra â¢ Tools: Eclipse, Toad, FTP, Tectia, Putty, Autosys, Anaconda, Jupyter notebool and Devops - RTC, RLM. â¢ Scripting Languages: JSP â¢ Platforms: Windows, UnixEducation Details 
 M.Tech (IT-DBS) B.Tech (CSE)  SRM University
Software Engineer 

Software Engineer - Larsen and Toubro
Skill Details 
Company Details 
company - Larsen and Toubro
description - Worked as a Software Engineer in Technosoft Corporation, Chennai from Aug 2015 to sep 2016.
company - Current Project
description - Duration: September 2016 to Till date
Vendor: Citi bank
Description:
Citibank's (Citi) Anti-Money Laundering (AML) Transaction Monitoring (TM) program is a future state solution and a rules-based system for transaction monitoring of ICG-Markets business.
Roles and Responesbilities:
â¢ Building and providing domain knowledge for Anti Money Laundering among team members.
â¢ The layered architecture has Data Warehouse and Workspace layers which are used by Business Analysts.
â¢ Actively involved in designing of star-schema model involving various Dimensions and Fact tables.
â¢ Designed SCD2 for maintaining history of the DIM data.
â¢ Developing Hive Queries for mapping data between different layers of architecture, and it's usage in Oozie Workflows.
â¢ Integration with Data Quality and Reconciliation Module.
â¢ Regression and Integration testing of solution for any issues in integration with other modules and effectively testing the data flow from layer-to-layer.
â¢ Transaction monitoring system development to generate Alerts for the suspicious and fraudulent transactions based on requirements provide by BAs.
â¢ Developing spark Jobs for various business rules.
â¢ Learning ""Machine Learning"", which will be used further in the project for developing an effective model for Fraud detection for Anti Money Laundering system.
â¢ Scheduling Jobs using Autosys tool.
â¢ Deployment and Code Management using RTC and RLM(Release Lifecycle Management)

Hadoop Developer
#  Current Project: PRTS - RAN
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Sqoop, HBase, Tez, Tableau, Sqlserver, Teradata
Cluster Size: 96 Node Cluster.
Distribution: Horton works - HDP2.3
company - Alcatel lucent
description - 1X) and  Ruckus Wireless
Description:
The scope of this project is to maintain and store the operational and parameters data collected from the multiple vendors networks by the mediation team into the OMS data store and make it available for RF engineers to boost the network performance.
Responsibilities:
â¢ Working with Hadoop Distributed File System.
â¢ Involved in importing data from MySQL to HDFS using SQOOP.
â¢ Involved in creating Hive tables, loading with data and writing hive queries which will run  on top of  Tez execution Engine.
â¢ Involved in Preparing Test cases Document.
â¢ Involved in Integrating Hive and HBase to store the operational data.
â¢ Monitoring the Jobs through Oozie.
company - Current Project
description - Anti - Money laundering
Environment: Hadoop 2.x, HDFS, Yarn, Hive, Oozie, Spark, Unix, Autosys, Python, RTC, RLM, ETL Framwe work
Cluster Size: 56 Node Cluster.
Distribution: Cloudera 5.9.14"
Hadoop,"Technical Skill Set: Programming Languages Apache Hadoop, Python, shell scripting, SQL Technologies Hive, Pig, Sqoop, Flume, Oozie, Impala, hdfs Tools Dataiku, Unravel, Cloudera, Putty, HUE, Cloudera Manager, Eclipse, Resource Manager Initial Learning Program: Tata Consultancy Services: June 2015 to August 2015 Description: This is a learning program conducted by TCS for the newly joined employees, to accomplish them to learn the working standard of the organization. During this period employee are groomed with various technical as well as ethical aspects. Education Details 
 B.E. Electronics & Communication Indore, Madhya Pradesh Medi-caps Institute of Technology & Management
Hadoop developer 

hadoop,hive,sqoop,flume,pig,mapreduce,python,impala,spark,scala,sql,unix.
Skill Details 
APACHE HADOOP SQOOP- Exprience - 31 months
Hadoop- Exprience - 31 months
HADOOP- Exprience - 31 months
Hive- Exprience - 31 months
SQOOP- Exprience - 31 months
python- Exprience - Less than 1 year months
hdfs- Exprience - Less than 1 year months
unix- Exprience - Less than 1 year months
impala- Exprience - Less than 1 year months
pig- Exprience - Less than 1 year months
unravel- Exprience - Less than 1 year months
mapreduce- Exprience - Less than 1 year months
dataiku- Exprience - Less than 1 year monthsCompany Details 
company - Tata Consultancy Services
description - Project Description
Data warehouse division has multiple products for injecting, storing, analysing and presenting data. The Data Lake program is started to provide multi-talent, secure data hub to store application's data on Hadoop platform with strong data governance, lineage, auditing and monitoring capabilities. The object of the project is to provide necessary engineering support to analytics and application teams so that they can focus on the business logic development. In this project, the major task is to set up the Hadoop cluster and govern all the activities which are required for the smooth functioning of various Hadoop ecosystems. As the day and day data increasing so to provide stability to the ecosystem and smooth working of it, Developing and automating the various requirement specific utilities.

Responsibility 1. Developed proactive Health Check utility for Data Lake. The utility proactively checks the smooth functioning of all Hadoop components on the cluster and sends the result to email in HTML format. The utility is being used for daily Health Checks as well as after upgrades.
2. Getting the data in different formats and processing the data in Hadoop ecosystem after filtering the data using the appropriate techniques.
3. Developed data pipeline utility to ingest data from RDBMS database to Hive external tables using Sqoop commands. The utility also offers the data quality check like row count validation.
4. Developed and automated various cluster health check, usage, capacity related reports using Unix shell scripting.
5. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
6. Creating flume agents to process the data to Hadoop ecosystem side.
7. Performed benchmark testing on the Hive Queries and impala queries.
8. Involved in setting up the cluster and its components like edge node and HA implementation of the services: Hive Server2, Impala, and HDFS.
9. Filtering the required data from available data using different technologies like pig, regex Serde etc.
10. Dataiku benchmark testing on top of impala and hive in compare to Greenplum database.
11. Moving the data from Greenplum database to Hadoop side with help of Sqoop pipeline, process the data to Hadoop side and storing the data into hive tables to do the performance testing.
12. Dealing with the Hadoop ecosystem related issues in order to provide stability to WM Hadoop ecosystem.
13. Rescheduling of job from autosys job hosting to TWS job hosting for better performance.

Declaration:
I hereby declare that the above mentioned information is authentic to the best of my knowledge
company - Tata Consultancy Services
description - Clients: 1. Barclays 2. Union bank of California (UBC) 3. Morgan Stanley (MS)

KEY PROJECTS HANDLED
Project Name ABSA- Reconciliations, UBC and WMDATALAKE COE
company - Tata Consultancy Services
description - Project Description
Migration of data from RDBMS database to Hive (Hadoop ecosystem) . Hadoop platform ability with strong data governance, lineage, auditing and monitoring capabilities. The objective of this project was to speed up the data processing so that the analysis and decision making become easy. Due to RDBMS limitations to process waste amount of data at once and produce the results at the earliest, Client wanted to move the data to Hadoop ecosystem so that they can over-come from those limitations and focus on business improvement only.

Responsibility 1. Optimising the SQL queries for those data which were not required to move from RDBMS to any other platform.
2. Writing the Hive queries and logic to move the data from RDBMS to Hadoop ecosystem.
3. Writing the hive queries to analyse the required data as per the business requirements.
4. Optimization of hive queries in order to increase the performance and minimize the Hadoop resource utilizations.
5. Writing the sqoop commands and scripts to move the data from RDBMS to Hadoop side.
company - Tata Consultancy Services
description - Project Description
Create recs and migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.

Responsibility 1. Have worked on extracting business requirements, analyzing and implementing them in developing Recs 2. Worked on migrating static setup of reconciliations from 8.1 version to 9.1 version of the environment Intellimatch.
3. Done the back end work where most of the things were related to writing the sql queries and provide the data for the new recs.

Project Name   PSO"
Hadoop,"Technical Skills Programming Languages: C, C++, Java, .Net., J2EE, HTML5, CSS, MapReduce Scripting Languages: Javascript, Python Databases: Oracle (PL-SQL), MY-SQL, IBM DB2 Tools:IBM Rational Rose, R, Weka Operating Systems: Windows XP, Vista, UNIX, Windows 7, Red Hat 7Education Details 
January 2015 B.E  Pimpri Chinchwad, MAHARASHTRA, IN Pimpri Chinchwad College of Engineering
January 2012 Diploma MSBTE  Dnyanganaga Polytechnic
 S.S.C   New English School Takali
Hadoop/Big Data Developer 

Hadoop/Big Data Developer - British Telecom
Skill Details 
APACHE HADOOP MAPREDUCE- Exprience - 37 months
MapReduce- Exprience - 37 months
MAPREDUCE- Exprience - 37 months
JAVA- Exprience - 32 months
.NET- Exprience - 6 monthsCompany Details 
company - British Telecom
description - Project: British Telecom project (UK)
Responsibilities:
â¢ Working on HDFS, MapReduce, Hive, Spark, Scala, Sqoop, Kerberos etc. technologies
â¢ Implemented various data mining algorithms on Spark like K-means clustering, Random forest, NaÃ¯ve bayes etc.
â¢ A knowledge of installing, configuring, maintaining and securing Hadoop.
company - DXC technology
description - HPE legacy), Bangalore
â¢ Worked on Hadoop + Java programming
â¢ Worked on Azure and AWS (EMR) services.
â¢ Worked on HDInsight Hadoop cluster..
â¢ Design, develop, document and architect Hadoop applications
â¢ Develop MapReduce coding that works seamlessly on Hadoop clusters.
â¢ Analyzing and processing the large data sets on HDFS.
â¢ An analytical bent of mind and ability to learn-unlearn-relearn surely comes in handy."
Hadoop,"Technical Skill Set Big Data Ecosystems: Hadoop, HDFS, HBase, Map Reduce, Sqoop, Hive, Pig, Spark-Core, Flume. Other Language: Scala, Core-Java, SQL, PLSQL, Sell Scripting ETL Tools: Informatica Power Center8.x/9.6, Talend 5.6 Tools: Eclipse, Intellij Idea. Platforms: Windows Family, Linux /UNIX, Cloudera. Databases: MySQL, Oracle.10/11gEducation Details 
 M.C.A  Pune, MAHARASHTRA, IN Pune University
Hodoop Developer 

Hodoop Developer - PRGX India Private Limited Pune
Skill Details 
Company Details 
company - PRGX India Private Limited Pune
description - Team Size: 10+
Environment: Hive, Spark, Sqoop, Scala and Flume.

Project Description:
The bank wanted to help its customers to avail different products of the bank through analyzing their expenditure behavior. The customers spending ranges from online shopping, medical expenses in hospitals, cash transactions, and debit card usage etc. the behavior allows the bank to create an analytical report and based on which the bank used to display the product offers on the customer portal which was built using java. The portal allows the customers to login and see their transactions which they make on a day to day basis .the analytics also help the customers plan their budgets through the budget watch and my financial forecast applications embedded into the portal. The portal used hadoop framework to analyes the data as per the rules and regulations placed by the regulators from the respective countries. The offers and the interest rates also complied with the regulations and all these processing was done using the hadoop framework as big data analytics system.

Role & Responsibilities:
â Import data from legacy system to hadoop using Sqoop, flume.
â Implement the business logic to analyses  the data
â Per-process data using spark.
â Create hive script and loading data into hive.
â Sourcing various attributes to the data processing logic to retrieve the correct results.

Project 2
company - PRGX India Private Limited Pune
description - 
company - PRGX India Private Limited Pune
description - Team Size: 11+
Environment: Hadoop, HDFS, Hive, Sqoop, MySQL, Map Reduce

Project Description:-
The Purpose of this project is to store terabytes of information from the web application and extract meaningful information out of it.the solution was based on the open source s/w hadoop. The data will be stored in hadoop file system and processed using Map/Reduce jobs. Which in trun includes getting the raw html data from the micro websites, process the html to obtain product and user information, extract various reports out of the vistor tracking information and export the information for further processing

Role & Responsibilities:
â Move all crawl data flat files generated from various micro sites to HDFS for further processing.
â Sqoop implementation for interaction with database
â Write Map Reduce scripts to process the data file.
â Create hive tables to store the processed data in tabular formats.
â Reports creation from hive data.

Project 3
company - PRGX India Private Limited Pune
description - Team Size: 15+
Environment: Informatica 9.5, Oracle11g, UNIX

Project Description:
Pfizer Inc. is an American global pharmaceutical corporation headquartered in New York City. The main objective of the project is to build a Development Data Repository for Pfizer Inc. Because all the downstream application are like Etrack, TSP database, RTS, SADMS, GFS, GDO having their own sql request on the OLTP system directly due to which the performance of OLTP system goes slows down. For this we have created a Development Data Repository to replace the entire sql request directly on the OLTP system. DDR process extracts all clinical, pre-clinical, study, product, subject, sites related information from the upstream applications like EPECS, CDSS, RCM, PRC, E-CLINICAL, EDH and after applying some business logic put it into DDR core tables. From these snapshot and dimensional layer are created which are used for reporting application.

Role & Responsibilities:
â To understand & analyze the requirement documents and resolve the queries.
â To design Informatica mappings by using various basic transformations like Filter, Router, Source qualifier, Lookup etc and advance transformations like Aggregators, Joiner, Sorters and so on.
â Perform cross Unit and Integration testing for mappings developed within the team. Reporting bugs and bug fixing.
â Create workflow/batches and set the session dependencies.
â Implemented Change Data Capture using mapping parameters, SCD and SK generation.
â Developed Mapplet, reusable transformations to populate the data into data warehouse.
â Created Sessions & Worklets using workflow Manager to load the data into the Target Database.
â Involved in Unit Case Testing (UTC)
â Performing Unit Testing and UAT for SCD Type1/Type2, fact load and CDC implementation.

Personal Scan

Address: Jijayi Heights, Flat no 118, Narhe, (Police chowki) Pune- 411041"
